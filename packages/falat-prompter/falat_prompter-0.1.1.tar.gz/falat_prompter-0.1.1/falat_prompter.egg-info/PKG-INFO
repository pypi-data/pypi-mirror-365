Metadata-Version: 2.4
Name: falat-prompter
Version: 0.1.1
Summary: Extensible LLM provider system
Author: falatform
License: MIT
Project-URL: Homepage, https://github.com/falatform/prompter
Project-URL: Repository, https://github.com/falatform/prompter
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Dynamic: license-file




# Prompter: Extensible LLM Provider System

**[→ Community Packs: Domain-Specific Templates & Schemas](PACKS.md)**  
**[→ Contributor Guide](CONTRIBUTING.md)**

## Installation

Install from PyPI:

```bash
pip install falat-prompter
```

Prompter is a flexible, plugin-style Python framework for working with multiple Large Language Model (LLM) providers. It supports dynamic provider loading, robust error handling, and unified response parsing, making it easy to integrate, extend, and use in any open source or production environment.

## Purpose
Prompter aims to make prompt engineering and LLM integration easy, robust, and provider-agnostic. It lets you:
- Build prompts using templates or code
- Swap LLM providers with a single line change
- Get structured, type-safe responses from any provider
- Focus on your application logic, not SDK quirks

## Features
- **Plugin-style LLM providers**: Easily add or swap providers (OpenAI, Cohere, Anthropic, local, etc.)
- **Optional SDKs**: Only install what you need; clear errors if a provider's SDK is missing
- **Unified response handling**: Consistent, type-safe output from all providers
- **Prompting flexibility**: Use prompt templates or build prompts programmatically
- **Extensible and testable**: Add new providers or prompt strategies with minimal code

## How to Build Prompts

### 1. Using Template Files
Define prompts in external files (e.g., plain text, Jinja2) and load them at runtime. This enables easy prompt management and reuse.

**Example:**

_File: `prompter/templates/qa/qa.prompt`_
```
Background:
{{context}}
Question: {{question}}
Answer:
```

_Python usage:_
```python
from prompter.prompt_template_processor import PromptTemplateProcessor

template = PromptTemplateProcessor("qa/qa.prompt")
prompt = template.render(context="France is a country in Europe.", question="What is the capital of France?")
```

### 2. Programmatic Prompt Building
Build prompts dynamically in code using f-strings or other logic. Useful for advanced or highly dynamic use cases.

**Example:**
```python
context = "France is a country in Europe."
question = "What is the capital of France?"
prompt = f"Background:\n{context}\nQuestion: {question}\nAnswer:"
```

## How to Run and Choose a Provider

Prompter makes it easy to switch between LLM providers. All providers have a unified interface:

```python
from prompter.providers import OpenAIService, LocalLLMService

# Choose a provider (just change the class to switch)
service = OpenAIService(api_key="sk-...", model="gpt-4")
# or
# service = LocalLLMService(endpoint_url="http://localhost:8000/generate", model="llama-3")

response = service.generate(prompt)
print(response)
```


## Defining Output Python Objects (Structured Output)

You can ask any provider to return a structured response matching a Python dataclass or Pydantic model. Just pass the `result_object` parameter:

**With a dataclass:**
```python
from dataclasses import dataclass

@dataclass
class Answer:
    answer: str
    confidence: float

response = service.generate(prompt, result_object=Answer)
print(response.answer, response.confidence)
```

**With a Pydantic model:**
```python
from pydantic import BaseModel

class Answer(BaseModel):
    answer: str
    confidence: float

response = service.generate(prompt, result_object=Answer)
print(response.answer, response.confidence)
```

## Extending
- Add new providers by subclassing and implementing the `generate` method.
- Add new prompt strategies by creating new template loaders or programmatic builders.

## License
MIT

## Contributing
Pull requests and issues are welcome! Please see CONTRIBUTING.md for guidelines.
