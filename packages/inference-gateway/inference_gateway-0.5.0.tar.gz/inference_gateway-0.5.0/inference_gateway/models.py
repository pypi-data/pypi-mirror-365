# generated by datamodel-codegen:
#   filename:  openapi.yaml
#   timestamp: 2025-07-26T15:10:49+00:00

from __future__ import annotations

from collections.abc import Mapping, Sequence
from typing import Annotated, Any, Literal, Optional

from pydantic import BaseModel, ConfigDict, Field, RootModel


class Provider(
    RootModel[
        Literal[
            "ollama", "groq", "openai", "cloudflare", "cohere", "anthropic", "deepseek", "google"
        ]
    ]
):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal[
        "ollama", "groq", "openai", "cloudflare", "cohere", "anthropic", "deepseek", "google"
    ]


class ProviderSpecificResponse(BaseModel):
    """
    Provider-specific response format. Examples:

    OpenAI GET /v1/models?provider=openai response:
    ```json
    {
      "provider": "openai",
      "object": "list",
      "data": [
        {
          "id": "gpt-4",
          "object": "model",
          "created": 1687882410,
          "owned_by": "openai",
          "served_by": "openai"
        }
      ]
    }
    ```

    Anthropic GET /v1/models?provider=anthropic response:
    ```json
    {
      "provider": "anthropic",
      "object": "list",
      "data": [
        {
          "id": "gpt-4",
          "object": "model",
          "created": 1687882410,
          "owned_by": "openai",
          "served_by": "openai"
        }
      ]
    }
    ```

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )


class ProviderAuthType(RootModel[Literal["bearer", "xheader", "query", "none"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["bearer", "xheader", "query", "none"]
    """
    Authentication type for providers
    """


class SSEvent(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    event: Optional[
        Literal[
            "message-start",
            "stream-start",
            "content-start",
            "content-delta",
            "content-end",
            "message-end",
            "stream-end",
        ]
    ] = None
    data: Optional[str] = None
    retry: Optional[int] = None


class Endpoints(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    models: str
    chat: str


class Error(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    error: Optional[str] = None


class MessageRole(RootModel[Literal["system", "user", "assistant", "tool"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["system", "user", "assistant", "tool"]
    """
    Role of the message sender
    """


class Model(BaseModel):
    """
    Common model information
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    object: str
    created: int
    owned_by: str
    served_by: Provider


class ListModelsResponse(BaseModel):
    """
    Response structure for listing models
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    provider: Optional[Provider] = None
    object: str
    data: Sequence[Model] = []


class A2AAgentCard(BaseModel):
    """
    An AgentCard conveys key information:
    - Overall details (version, name, description, uses)
    - Skills: A set of capabilities the agent can perform
    - Default modalities/content types supported by the agent.
    - Authentication requirements
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    capabilities: Any
    """
    Optional capabilities supported by the agent.
    """
    default_input_modes: Annotated[Sequence[str], Field(alias="defaultInputModes")]
    """
    The set of interaction modes that the agent supports across all skills. This can be overridden per-skill.
    Supported media types for input.
    """
    default_output_modes: Annotated[Sequence[str], Field(alias="defaultOutputModes")]
    """
    Supported media types for output.
    """
    description: str
    """
    A human-readable description of the agent. Used to assist users and
    other agents in understanding what the agent can do.
    """
    documentation_url: Annotated[Optional[str], Field(alias="documentationUrl")] = None
    """
    A URL to documentation for the agent.
    """
    icon_url: Annotated[Optional[str], Field(alias="iconUrl")] = None
    """
    A URL to an icon for the agent.
    """
    id: str
    """
    Unique identifier for the agent (base64-encoded SHA256 hash of the agent URL).
    """
    name: str
    """
    Human readable name of the agent.
    """
    provider: Optional[Any] = None
    """
    The service provider of the agent
    """
    security: Optional[Sequence[Mapping[str, Any]]] = None
    """
    Security requirements for contacting the agent.
    """
    security_schemes: Annotated[Optional[Mapping[str, Any]], Field(alias="securitySchemes")] = None
    """
    Security scheme details used for authenticating with this agent.
    """
    skills: Sequence[Any]
    """
    Skills are a unit of capability that an agent can perform.
    """
    supports_authenticated_extended_card: Annotated[
        Optional[bool], Field(alias="supportsAuthenticatedExtendedCard")
    ] = None
    """
    true if the agent supports providing an extended agent card when the user is authenticated.
    Defaults to false if not specified.
    """
    url: str
    """
    A URL to the address the agent is hosted at.
    """
    version: str
    """
    The version of the agent - format is up to the provider.
    """


class MCPTool(BaseModel):
    """
    An MCP tool definition
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    name: Annotated[str, Field(examples=["read_file"])]
    """
    The name of the tool
    """
    description: Annotated[str, Field(examples=["Read content from a file"])]
    """
    A description of what the tool does
    """
    server: Annotated[str, Field(examples=["http://mcp-filesystem-server:8083/mcp"])]
    """
    The MCP server that provides this tool
    """
    input_schema: Annotated[
        Optional[Mapping[str, Any]],
        Field(
            examples=[
                {
                    "type": "object",
                    "properties": {
                        "file_path": {"type": "string", "description": "Path to the file to read"}
                    },
                    "required": ["file_path"],
                }
            ]
        ),
    ] = None
    """
    JSON schema for the tool's input parameters
    """


class FunctionParameters(BaseModel):
    """
    The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
    Omitting `parameters` defines a function with an empty parameter list.
    """

    model_config = ConfigDict(
        extra="allow",
        populate_by_name=True,
    )


class ChatCompletionToolType(RootModel[Literal["function"]]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["function"] = "function"
    """
    The type of the tool. Currently, only `function` is supported.
    """


class CompletionUsage(BaseModel):
    """
    Usage statistics for the completion request.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    completion_tokens: int = 0
    """
    Number of tokens in the generated completion.
    """
    prompt_tokens: int = 0
    """
    Number of tokens in the prompt.
    """
    total_tokens: int = 0
    """
    Total number of tokens used in the request (prompt + completion).
    """


class ChatCompletionStreamOptions(BaseModel):
    """
    Options for streaming response. Only set this when you set `stream: true`.

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    include_usage: bool
    """
    If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.

    """


class ChatCompletionMessageToolCallFunction(BaseModel):
    """
    The function that the model called.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    name: str
    """
    The name of the function to call.
    """
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
    """


class ChatCompletionMessageToolCall(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    The ID of the tool call.
    """
    type: ChatCompletionToolType
    function: ChatCompletionMessageToolCallFunction


class ChatCompletionMessageToolCallChunk(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    index: int
    id: Optional[str] = None
    """
    The ID of the tool call.
    """
    type: Optional[str] = None
    """
    The type of the tool. Currently, only `function` is supported.
    """
    function: Optional[ChatCompletionMessageToolCallFunction] = None


class TopLogprob(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    token: str
    """
    The token.
    """
    logprob: float
    """
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    bytes: Sequence[int]
    """
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    """


class ChatCompletionTokenLogprob(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    token: str
    """
    The token.
    """
    logprob: float
    """
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    """
    bytes: Sequence[int]
    """
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    """
    top_logprobs: Sequence[TopLogprob]
    """
    List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
    """


class FinishReason(
    RootModel[Literal["stop", "length", "tool_calls", "content_filter", "function_call"]]
):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """
    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool.

    """


class Config(RootModel[Any]):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    root: Any


class Message(BaseModel):
    """
    Message structure for provider requests
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    role: MessageRole
    content: str
    tool_calls: Optional[Sequence[ChatCompletionMessageToolCall]] = None
    tool_call_id: Optional[str] = None
    reasoning_content: Optional[str] = None
    """
    The reasoning content of the chunk message.
    """
    reasoning: Optional[str] = None
    """
    The reasoning of the chunk message. Same as reasoning_content.
    """


class ListToolsResponse(BaseModel):
    """
    Response structure for listing MCP tools
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    object: Annotated[str, Field(examples=["list"])]
    """
    Always "list"
    """
    data: Sequence[MCPTool] = []
    """
    Array of available MCP tools
    """


class ListAgentsResponse(BaseModel):
    """
    Response structure for listing A2A agents
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    object: Annotated[str, Field(examples=["list"])]
    """
    Always "list"
    """
    data: Sequence[A2AAgentCard] = []
    """
    Array of available A2A agents
    """


class FunctionObject(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    description: Optional[str] = None
    """
    A description of what the function does, used by the model to choose when and how to call the function.
    """
    name: str
    """
    The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    """
    parameters: Optional[FunctionParameters] = None
    strict: bool = False
    """
    Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).
    """


class ChatCompletionTool(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    type: ChatCompletionToolType
    function: FunctionObject


class CreateChatCompletionRequest(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    model: str
    """
    Model ID to use
    """
    messages: Annotated[Sequence[Message], Field(min_length=1)]
    """
    A list of messages comprising the conversation so far.

    """
    max_tokens: Optional[int] = None
    """
    An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.

    """
    stream: bool = False
    """
    If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).

    """
    stream_options: Optional[ChatCompletionStreamOptions] = None
    tools: Optional[Sequence[ChatCompletionTool]] = None
    """
    A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

    """
    reasoning_format: Optional[str] = None
    """
    The format of the reasoning content. Can be `raw` or `parsed`.
    When specified as raw some reasoning models will output <think /> tags. When specified as parsed the model will output the reasoning under  `reasoning` or `reasoning_content` attribute.

    """


class ChatCompletionChoice(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    finish_reason: Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    """
    The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
    `length` if the maximum number of tokens specified in the request was reached,
    `content_filter` if content was omitted due to a flag from our content filters,
    `tool_calls` if the model called a tool.

    """
    index: int
    """
    The index of the choice in the list of choices.
    """
    message: Message


class Logprobs(BaseModel):
    """
    Log probability information for the choice.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    content: Sequence[ChatCompletionTokenLogprob]
    """
    A list of message content tokens with log probability information.
    """
    refusal: Sequence[ChatCompletionTokenLogprob]
    """
    A list of message refusal tokens with log probability information.
    """


class CreateChatCompletionResponse(BaseModel):
    """
    Represents a chat completion response returned by model, based on the provided input.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    A unique identifier for the chat completion.
    """
    choices: Sequence[ChatCompletionChoice]
    """
    A list of chat completion choices. Can be more than one if `n` is greater than 1.
    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created.
    """
    model: str
    """
    The model used for the chat completion.
    """
    object: str
    """
    The object type, which is always `chat.completion`.
    """
    usage: Optional[CompletionUsage] = None


class ChatCompletionStreamResponseDelta(BaseModel):
    """
    A chat completion delta generated by streamed model responses.
    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    content: str
    """
    The contents of the chunk message.
    """
    reasoning_content: Optional[str] = None
    """
    The reasoning content of the chunk message.
    """
    reasoning: Optional[str] = None
    """
    The reasoning of the chunk message. Same as reasoning_content.
    """
    tool_calls: Optional[Sequence[ChatCompletionMessageToolCallChunk]] = None
    role: MessageRole
    refusal: Optional[str] = None
    """
    The refusal message generated by the model.
    """


class ChatCompletionStreamChoice(BaseModel):
    model_config = ConfigDict(
        populate_by_name=True,
    )
    delta: ChatCompletionStreamResponseDelta
    logprobs: Optional[Logprobs] = None
    """
    Log probability information for the choice.
    """
    finish_reason: FinishReason
    index: int
    """
    The index of the choice in the list of choices.
    """


class CreateChatCompletionStreamResponse(BaseModel):
    """
    Represents a streamed chunk of a chat completion response returned
    by the model, based on the provided input.

    """

    model_config = ConfigDict(
        populate_by_name=True,
    )
    id: str
    """
    A unique identifier for the chat completion. Each chunk has the same ID.
    """
    choices: Sequence[ChatCompletionStreamChoice]
    """
    A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
    last chunk if you set `stream_options: {"include_usage": true}`.

    """
    created: int
    """
    The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
    """
    model: str
    """
    The model to generate the completion.
    """
    system_fingerprint: Optional[str] = None
    """
    This fingerprint represents the backend configuration that the model runs with.
    Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

    """
    object: str
    """
    The object type, which is always `chat.completion.chunk`.
    """
    usage: Optional[CompletionUsage] = None
    reasoning_format: Optional[str] = None
    """
    The format of the reasoning content. Can be `raw` or `parsed`.
    When specified as raw some reasoning models will output <think /> tags. When specified as parsed the model will output the reasoning under reasoning_content.

    """
