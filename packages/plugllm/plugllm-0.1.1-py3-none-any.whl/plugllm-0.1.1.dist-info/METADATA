Metadata-Version: 2.4
Name: plugllm
Version: 0.1.1
Summary: Unified LLM API interface for OpenAI, Gemini, Mistral, Groq etc.
Home-page: https://github.com/firoziya/plugllm
Author: Yash Kumar Firoziya
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Dynamic: author
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary


# ğŸ”Œ plugllm

**plugllm** is a unified and provider-agnostic Python package that lets you interact with multiple LLM APIs (like OpenAI, Gemini, Mistral, Groq, etc.) using a single, consistent interface â€” without needing to learn each providerâ€™s SDK.

> Created by **Yash Kumar Firoziya**

---

## ğŸŒŸ Features

- ğŸ”Œ **Unified API** â€” One interface for all providers  
- ğŸ“¡ **Supports multiple providers** â€” OpenAI, Gemini, Mistral, Groq (more coming)  
- ğŸ§  **Same request structure** â€” Compatible message format across providers  
- ğŸ” **Secure & simple config** â€” Use environment variables or inline setup  
- ğŸš« **No SDKs required** â€” Only uses Python `requests` library  
- ğŸ“œ **Role-based prompt support** â€” For both single and multi-turn chat  
- ğŸ”„ **Extensible** â€” Add custom providers easily

---

## ğŸ“¦ Installation

```bash
pip install plugllm
```

---

## âš™ï¸ Configuration

You can configure directly in your code:

```python
from plugllm import config

config(
    provider="openai",       # "gemini", "mistral", "groq" also supported
    api_key="your-api-key",
    model="gpt-4",           # model name based on provider
    base_url=None            # optional: custom or local API endpoint
)
```

Or use environment variables for better security:

```bash
export LLM_PROVIDER=openai
export LLM_API_KEY=your-api-key
export LLM_MODEL=gpt-4
```

---

## ğŸ’¬ Basic Usage

```python
from plugllm import generate

response = generate("What is quantum entanglement?")
print(response)
```

### ğŸ§µ Multi-turn Chat

```python
generate([
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What are black holes?"}
])
```

---

## ğŸ“¡ Supported Providers

* OpenAI (ChatGPT, GPT-4, GPT-3.5)
* Google Gemini
* Mistral AI
* GroqCloud (Mixtral)
* **Coming soon:** Cohere, Anthropic Claude, Ollama, LM Studio

---

## ğŸ—‚ï¸ Project Structure

```
plugllm/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core.py
â”œâ”€â”€ config.py
â”œâ”€â”€ prompts.py
â””â”€â”€ providers/
    â”œâ”€â”€ base.py
    â”œâ”€â”€ openai.py
    â”œâ”€â”€ gemini.py
    â”œâ”€â”€ mistral.py
    â””â”€â”€ groq.py
```

---

## ğŸ¤ Contributing

Pull requests are welcome! If you want to add support for a new provider, just create a new module in `providers/` based on `base.py`.

---

## ğŸªª License

This project is licensed under the **MIT License**.

---

## âœ¨ Author

**Yash Kumar Firoziya**

---

