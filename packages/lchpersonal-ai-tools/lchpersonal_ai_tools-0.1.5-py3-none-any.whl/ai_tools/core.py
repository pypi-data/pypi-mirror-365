"""
AI Tools æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
"""


def sayHello():
    """
    è¾“å‡ºhello world!
    
    Returns:
        str: è¿”å›hello world!å­—ç¬¦ä¸²
    """
    message = "hello world!"
    print(message)
    return message


def sayTest001():
    """
    æ˜¾ç¤ºéŸ³é¢‘ç‰¹å¾æå–ä»£ç ç¤ºä¾‹
    
    Returns:
        str: è¿”å›éŸ³é¢‘å¤„ç†ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    import librosa
    import librosa.display
    import numpy as np
    import matplotlib.pyplot as plt
    import sys
    import os

    def main():
        # 1. Configuration
        audio_path = 'D:/å…¶ä»–/audio/30seconds.wav'  # è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®
        save_img_path = 'audio_features.png'
        feature_prefix = 'features_'
        
        # 2. Verify audio file
        if not os.path.exists(audio_path):
            print(f"Error: Audio file not found at {audio_path}")
            sys.exit(1)
        
        # 3. Load audio
        try:
            audio, sr = librosa.load(audio_path, sr=16000, mono=True)
            print(f"Audio loaded: {len(audio)/sr:.2f} seconds at {sr} Hz sample rate")
        except Exception as e:
            print(f"Error loading audio: {e}")
            sys.exit(1)
        
        # 4. Feature extraction
        def extract_features(y, sr):
            # Compute STFT (Short-Time Fourier Transform)
            stft = np.abs(librosa.stft(y))
            
            # MFCC (Mel-Frequency Cepstral Coefficients)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
            
            # Chroma feature
            chroma = librosa.feature.chroma_stft(S=stft, sr=sr)
            
            # Mel-scaled Spectrogram
            mel = librosa.feature.melspectrogram(y=y, sr=sr)
            mel_db = librosa.power_to_db(mel, ref=np.max)  # Convert to dB
            
            return {'stft_db': librosa.amplitude_to_db(stft, ref=np.max),
                    'mfcc': mfcc,
                    'chroma': chroma,
                    'mel_db': mel_db}

        print("Extracting audio features...")
        features = extract_features(audio, sr)
        
        # 5. Visualization setup
        plt.figure(figsize=(14, 12))
        
        # Feature 1: STFT Spectrogram
        ax1 = plt.subplot(4, 1, 1)
        stft_disp = librosa.display.specshow(features['stft_db'], 
                                            x_axis='time', 
                                            y_axis='log', 
                                            sr=sr)
        plt.title('STFT (Log Frequency)')
        plt.colorbar(format='%+2.0f dB')
        
        # Feature 2: MFCC
        plt.subplot(4, 1, 2, sharex=ax1)
        mfcc_disp = librosa.display.specshow(features['mfcc'], 
                                            x_axis='time', 
                                            sr=sr)
        plt.title('MFCC Coefficients')
        plt.colorbar()
        
        # Feature 3: Chroma
        plt.subplot(4, 1, 3, sharex=ax1)
        chroma_disp = librosa.display.specshow(features['chroma'], 
                                            x_axis='time', 
                                            y_axis='chroma', 
                                            sr=sr)
        plt.title('Chroma Features')
        plt.colorbar()
        
        # Feature 4: Mel Spectrogram
        plt.subplot(4, 1, 4, sharex=ax1)
        mel_disp = librosa.display.specshow(features['mel_db'], 
                                            x_axis='time', 
                                            y_axis='mel', 
                                            sr=sr)
        plt.title('Mel Spectrogram')
        plt.colorbar(format='%+2.0f dB')
        
        # 6. Finalize and save visualization
        plt.tight_layout()
        plt.savefig(save_img_path, dpi=300)
        print(f"Feature visualization saved to {save_img_path}")
        
        # 7. Save features as numpy arrays
        for feature_name, feature_data in features.items():
            filename = f"{feature_prefix}{feature_name}.npy"
            np.save(filename, feature_data)
            print(f"Saved {filename} ({feature_data.shape[1]} frames)")
        
        print("Feature extraction completed successfully!")

    if __name__ == "__main__":
        main()
    
    """
    
    return message

def sayTest002():
    """
    å®Œæ•´æµç¨‹ï¼š
    1. ä½¿ç”¨ ClearVoice speech_enhancement æ¨¡å‹è¿›è¡Œè¯­éŸ³é™å™ªå¢å¼º
    2. MOS è´¨é‡è¯„åˆ†
    3. SRMR æ¸…æ™°åº¦è¯„åˆ†
    """
    message = """
    from clearvoice import ClearVoice
import os
import librosa
import soundfile as sf
import numpy as np
from scipy.signal import stft


def enhance_and_score(
    input_path: str,
    output_path: str,
    se_model: str = "MossFormer2_SE_48K"
):

    # æ£€æŸ¥è¾“å…¥
    if not os.path.isfile(input_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_path}")

    # åˆå§‹åŒ– ClearVoice
    cv_se = ClearVoice(
        task="speech_enhancement",
        model_names=[se_model]
    )

    # è¯»å–éŸ³é¢‘
    y, sr = librosa.load(input_path, sr=None, mono=True)

    # æ ¹æ®æ¨¡å‹åç¼€å†³å®šç›®æ ‡é‡‡æ ·ç‡
    target_sr = 48000 if se_model.endswith("48K") else 16000
    # é‡é‡‡æ ·åˆ°æ¨¡å‹æ‰€éœ€é‡‡æ ·ç‡
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
        tmp_path = input_path.replace(".wav", f"_{sr//1000}k_temp.wav")
        sf.write(tmp_path, y, sr)
        model_input = tmp_path
    else:
        model_input = input_path

    # æ‰§è¡Œè¯­éŸ³å¢å¼ºï¼Œä¸ç«‹å³å†™ç£ç›˜
    enhanced_audio = cv_se(
        input_path=model_input,
        online_write=False
    )

    # ä¿å­˜å¢å¼ºç»“æœ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    cv_se.write(enhanced_audio, output_path=output_path)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if model_input.endswith("_temp.wav") and os.path.isfile(model_input):
        os.remove(model_input)

    # è½½å…¥å¢å¼ºåéŸ³é¢‘ç”¨äºè¯„åˆ†
    audio, fs = librosa.load(output_path, sr=None, mono=True)

    # è®¡ç®— MOS
    rms = librosa.feature.rms(y=audio)[0].mean()
    spectral_flatness = librosa.feature.spectral_flatness(y=audio)[0].mean()
    rms_score = min(1.5, max(0.5, rms)) * 2
    flatness_score = 4 - min(3, spectral_flatness * 5)
    mos_score = round(max(1.0, min(5.0, (rms_score + flatness_score) / 2 * 1.25)), 2)

    # è®¡ç®— SRMRï¼ˆå†…éƒ¨é‡é‡‡æ ·ï¼‰
    if fs > 16000:
        audio_srmr = librosa.resample(audio, orig_sr=fs, target_sr=16000)
        fs_srmr = 16000
    else:
        audio_srmr, fs_srmr = audio, fs
    f, t, Zxx = stft(audio_srmr, fs=fs_srmr, nperseg=256, noverlap=128)
    mags = np.abs(Zxx)
    speech_band = np.mean(mags[(f >= 300) & (f <= 4000)], axis=0)
    full_band = np.mean(mags, axis=0)
    srmr_score = round(max(1.0, min(10.0, np.mean(speech_band) / np.mean(full_band) * 15)), 2)

    return {
        "enhanced_path": output_path,
        "mos": mos_score,
        "srmr": srmr_score
    }


def main():
    print("=== ä»»åŠ¡ï¼šMossFormer2_SE_48K è¯­éŸ³å¢å¼º + è¯„åˆ† ===")

    input_audio = "D:/å…¶ä»–/ai_examing_002/30seconds.wav"
    output_audio = "D:/å…¶ä»–/ai_examing_002/30seconds-denoised.wav"

    try:
        results = enhance_and_score(input_audio, output_audio)
        print(f"âœ“ å¢å¼ºå®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {results['enhanced_path']}")
        print(f"âœ“ MOS è´¨é‡è¯„åˆ†: {results['mos']}/5")
        print(f"âœ“ SRMR æ¸…æ™°åº¦è¯„åˆ†: {results['srmr']}")
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    main()

    
    """
    return message
def sayTest003_1():
    """
    è¾“å‡º003çš„requirements.txt
    
    Returns:
        str: requirements.txtçš„å†…å®¹
    """
        
    message = """
        torch>=1.12.0
        torchaudio>=0.12.0
        librosa>=0.9.0
        numpy>=1.21.0
        scikit-learn>=1.0.0
        tqdm>=4.62.0
        matplotlib>=3.5.0
        seaborn>=0.11.0
        requests>=2.25.0
        pandas>=1.3.0
    """

    return message


def sayTest003_2():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†å¤„ç†ä»£ç ç¤ºä¾‹
    
    Returns:
        str: è¿”å›æ•°æ®é›†å¤„ç†ä»£ç å­—ç¬¦ä¸²
    """
    message = """
# dataset.py
import os
import librosa
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm

EMOTION_LABELS = ["angry", "fear", "happy", "neutral", "sad", "surprise"]
label_map = {label: idx for idx, label in enumerate(EMOTION_LABELS)}

class EmotionFeatureDataset(Dataset):
    def __init__(self, root_dir, sr=16000, n_mels=128, max_len=300, split='train', 
                 train_ratio=0.8, random_state=42, cache_dir="feature_cache"):
        self.root_dir = root_dir
        self.sr = sr
        self.n_mels = n_mels
        self.max_len = max_len
        self.split = split
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        self.audio_files, self.labels = self._get_audio_files()
        
        # åˆ†å‰²æ•°æ®é›†
        if len(self.audio_files) > 0:
            train_files, val_files, train_labels, val_labels = train_test_split(
                self.audio_files, self.labels, test_size=1-train_ratio, 
                random_state=random_state, stratify=self.labels
            )
            
            if split == 'train':
                self.audio_files = train_files
                self.labels = train_labels
            else:
                self.audio_files = val_files
                self.labels = val_labels
        
        print(f"{split} dataset: {len(self.audio_files)} samples")
        
    def _get_audio_files(self):
        audio_files = []
        labels = []
        
        for emotion in EMOTION_LABELS:
            emotion_dir = os.path.join(self.root_dir, emotion)
            if os.path.exists(emotion_dir):
                for file_name in os.listdir(emotion_dir):
                    if file_name.endswith('.wav'):
                        audio_files.append(os.path.join(emotion_dir, file_name))
                        labels.append(label_map[emotion])
        
        return audio_files, labels
    
    def _extract_features(self, audio_file):
        # æå–éŸ³é¢‘ç‰¹å¾ï¼šMelé¢‘è°±å›¾å’ŒåŸºé¢‘
        # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
        cache_name = os.path.basename(audio_file).replace('.wav', '.npy')
        cache_path = os.path.join(self.cache_dir, cache_name)
        
        # å¦‚æœç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
        if os.path.exists(cache_path):
            return np.load(cache_path)
        
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_file, sr=self.sr)
            
            # æå–Melé¢‘è°±å›¾
            mel_spec = librosa.feature.melspectrogram(
                y=y, sr=sr, n_mels=self.n_mels, hop_length=512, n_fft=2048
            )
            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
            
            # æå–åŸºé¢‘(F0/Pitch)
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr, hop_length=512)
            pitch = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch.append(pitches[index, t])
            pitch = np.array(pitch).reshape(1, -1)
            
            # åˆå¹¶ç‰¹å¾ (128 + 1 = 129)
            features = np.vstack([mel_spec_db, pitch])
            
            # ä¿å­˜åˆ°ç¼“å­˜
            np.save(cache_path, features)
            
            return features
            
        except Exception as e:
            print(f"Error processing {audio_file}: {e}")
            # è¿”å›é›¶ç‰¹å¾
            return np.zeros((129, 100))
    
    def _pad_or_truncate(self, features):
        #å¡«å……æˆ–æˆªæ–­ç‰¹å¾åˆ°å›ºå®šé•¿åº¦
        if features.shape[1] > self.max_len:
            # æˆªæ–­
            return features[:, :self.max_len]
        elif features.shape[1] < self.max_len:
            # å¡«å……
            pad_width = self.max_len - features.shape[1]
            return np.pad(features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)
        else:
            return features
    
    def __len__(self):
        return len(self.audio_files)
    
    def __getitem__(self, idx):
        audio_file = self.audio_files[idx]
        label = self.labels[idx]
        
        # æå–ç‰¹å¾
        features = self._extract_features(audio_file)
        
        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        features = self._pad_or_truncate(features)
        
        # è½¬æ¢ä¸ºtensor
        features = torch.FloatTensor(features).unsqueeze(0)  # æ·»åŠ é€šé“ç»´åº¦ (1, 129, max_len)
        label = torch.LongTensor([label])
        
        return features, label.squeeze() 
"""
    return message

def sayTest003_3():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†æCNNæ¨¡å‹ä»£ç ç¤ºä¾‹
    
    Returns:
        str: è¿”å›CNNæ¨¡å‹ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    # model.py
    import torch
import torch.nn as nn
import torch.nn.functional as F

class EmotionCNN(nn.Module):
    # n_classesè¡¨ç¤ºæƒ…æ„Ÿæ ‡ç­¾çš„ä¸ªæ•°
    def __init__(self, n_input=129, n_classes=6):
        super(EmotionCNN, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout1 = nn.Dropout2d(0.25)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout2 = nn.Dropout2d(0.25)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout3 = nn.Dropout2d(0.25)
        
        # è®¡ç®—å…¨è¿æ¥å±‚è¾“å…¥ç»´åº¦
        # å‡è®¾è¾“å…¥æ˜¯ (1, 129, 300)
        # ç»è¿‡3æ¬¡æ± åŒ–åï¼š129/8 â‰ˆ 16, 300/8 â‰ˆ 37
        # å›ºå®šå°ºå¯¸ä»¥ä¿è¯æ¨¡å‹ç»“æ„ä¸€è‡´
        self.fc1 = nn.Linear(128 * 16 * 37, 512)
        self.dropout4 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(512, 256)
        self.dropout5 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(256, n_classes)
        
    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)
        x = self.dropout1(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        x = self.dropout2(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        x = self.dropout3(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout4(x)
        
        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout5(x)
        
        x = self.fc3(x)
        
        return x 

"""
    return message

def sayTest003_4():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒä»£ç ç¤ºä¾‹
    
    Returns:
        str: è¿”å›æ¨¡å‹è®­ç»ƒä»£ç å­—ç¬¦ä¸²
    """
    message = """
    # train.py
    import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from dataset import EmotionFeatureDataset, EMOTION_LABELS
from model import EmotionCNN

# -------------------- é…ç½®é¡¹ -------------------
MODEL_TYPE = 'cnn'
BATCH_SIZE = 16
N_EPOCHS = 40
LEARNING_RATE = 1e-3
MAX_LEN = 300
MODEL_PATH = f'emotion_{MODEL_TYPE}.pt'
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

def train_one_epoch(model, loader, optimizer, criterion, device):
    #è®­ç»ƒä¸€ä¸ªepoch
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    for X, y in tqdm(loader, desc="Training"):
        X, y = X.to(device), y.to(device)
        
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += y.size(0)
        correct += (predicted == y).sum().item()
    
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc

def validate_one_epoch(model, loader, criterion, device):
    #éªŒè¯ä¸€ä¸ªepoch
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for X, y in tqdm(loader, desc="Validation"):
            X, y = X.to(device), y.to(device)
            
            outputs = model(X)
            loss = criterion(outputs, y)
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
    
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc, all_preds, all_labels

def plot_confusion_matrix(y_true, y_pred, save_path='confusion_matrix.png'):
    #ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=EMOTION_LABELS, yticklabels=EMOTION_LABELS)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.show()

def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path='training_history.png'):
    #ç»˜åˆ¶è®­ç»ƒå†å²
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Val Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accs, label='Train Acc')
    ax2.plot(val_accs, label='Val Acc')
    ax2.set_title('Training and Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path)
    plt.show()

def main():
    # -------------------- æ•°æ®åŠ è½½ ------
    print("åŠ è½½æ•°æ®é›†...")
    # datasat_trainè¡¨ç¤ºè®­ç»ƒé›†æ–‡ä»¶å¤¹åç§°
    train_dataset = EmotionFeatureDataset('datasat_train', split='train', max_len=MAX_LEN)
    val_dataset = EmotionFeatureDataset('datasat_train', split='val', max_len=MAX_LEN)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # -------------------- æ¨¡å‹åˆå§‹åŒ– ------
    model = EmotionCNN(n_input=129, n_classes=6)
    model.to(DEVICE)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    
    # ------------------- è®­ç»ƒå‡†å¤‡ -------------------
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
    
    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    best_val_acc = 0.0
    
    # -------------------- è®­ç»ƒå¾ªç¯ ----
    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(N_EPOCHS):
        print(f"\nEpoch {epoch+1}/{N_EPOCHS}")
        print("-" * 50)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)
        
        # éªŒè¯
        val_loss, val_acc, val_preds, val_labels = validate_one_epoch(model, val_loader, criterion, DEVICE)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # è®°å½•å†å²
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        
        print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
            }, MODEL_PATH)
            print(f"æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    
    print(f"\nè®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    # -------------------- æœ€ç»ˆè¯„ä¼° ----
    print("\nè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load(MODEL_PATH)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # æœ€ç»ˆéªŒè¯
    val_loss, val_acc, val_preds, val_labels = validate_one_epoch(model, val_loader, criterion, DEVICE)
    
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(val_labels, val_preds, target_names=EMOTION_LABELS))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(val_labels, val_preds)
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(train_losses, val_losses, train_accs, val_accs)
    
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_PATH}")

if __name__ == "__main__":
    main() 
    

    """
    return message

def sayTest003_5():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†ææ¨¡å‹é¢„æµ‹ä»£ç ç¤ºä¾‹
    
    Returns:
        str: è¿”å›æ¨¡å‹é¢„æµ‹ä»£ç å­—ç¬¦ä¸²
    """
    message = """
     #predict.py
     import os
import torch
import librosa
import numpy as np
import requests
import json
from model import EmotionCNN
from dataset import EMOTION_LABELS

def extract_features(audio_file, sr=16000, n_mels=128, max_len=300):
    #æå–å•ä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç‰¹å¾
    try:
        # åŠ è½½éŸ³é¢‘
        y, sr = librosa.load(audio_file, sr=sr)
        
        # æå–Melé¢‘è°±å›¾
        mel_spec = librosa.feature.melspectrogram(
            y=y, sr=sr, n_mels=n_mels, hop_length=512, n_fft=2048
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # æå–åŸºé¢‘(F0/Pitch)
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr, hop_length=512)
        pitch = []
        for t in range(pitches.shape[1]):
            index = magnitudes[:, t].argmax()
            pitch.append(pitches[index, t])
        pitch = np.array(pitch).reshape(1, -1)
        
        # åˆå¹¶ç‰¹å¾ (128 + 1 = 129)
        features = np.vstack([mel_spec_db, pitch])
        
        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        if features.shape[1] > max_len:
            features = features[:, :max_len]
        elif features.shape[1] < max_len:
            pad_width = max_len - features.shape[1]
            features = np.pad(features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)
        
        return features
        
    except Exception as e:
        print(f"Error processing {audio_file}: {e}")
        return np.zeros((129, max_len))

def predict_emotion(model, features, device):
    #é¢„æµ‹å•ä¸ªéŸ³é¢‘çš„æƒ…æ„Ÿ
    model.eval()
    
    # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchå’Œchannelç»´åº¦
    features_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)  # (1, 1, 129, max_len)
    features_tensor = features_tensor.to(device)
    
    with torch.no_grad():
        outputs = model(features_tensor)
        _, predicted = torch.max(outputs, 1)
        predicted_emotion = EMOTION_LABELS[predicted.item()]
    
    return predicted_emotion

def predict_test_audio(model_path, test_dir, device):
    #å¯¹æµ‹è¯•ç›®å½•ä¸­çš„æ‰€æœ‰éŸ³é¢‘è¿›è¡Œé¢„æµ‹
    # åŠ è½½æ¨¡å‹
    model = EmotionCNN(n_input=129, n_classes=6)
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"æ¨¡å‹åŠ è½½å®Œæˆï¼ŒéªŒè¯å‡†ç¡®ç‡: {checkpoint.get('val_acc', 'N/A'):.2f}%")
    
    # è·å–æ‰€æœ‰wavæ–‡ä»¶
    wav_files = [f for f in os.listdir(test_dir) if f.endswith('.wav')]
    print(f"æ‰¾åˆ° {len(wav_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    results = {}
    
    for wav_file in wav_files:
        audio_path = os.path.join(test_dir, wav_file)
        
        # æå–ç‰¹å¾
        features = extract_features(audio_path)
        
        # é¢„æµ‹æƒ…æ„Ÿ
        emotion = predict_emotion(model, features, device)
        
        # ä¿å­˜ç»“æœ (å»æ‰.wavæ‰©å±•å)
        filename_without_ext = wav_file.replace('.wav', '')
        results[filename_without_ext] = emotion
        
        print(f"{wav_file} -> {emotion}")
    
    return results

def submit_results(results, api_url):
    #æäº¤ç»“æœåˆ°è¯„åˆ†æ¥å£
    payload = {
        "answer": results
    }
    
    headers = {
        'Content-Type': 'application/json'
    }
    
    try:
        response = requests.post(api_url, data=json.dumps(payload), headers=headers)
        print(f"æäº¤çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å†…å®¹: {response.text}")
        return response
    except Exception as e:
        print(f"æäº¤å¤±è´¥: {e}")
        return None

def main():
    # é…ç½®
    MODEL_PATH = 'emotion_cnn.pt'
    TEST_DIR = './data/task-3-test-audio'
    API_URL = 'https://1f9f25e88908.ngrok-free.app/grade_emotion/'
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(MODEL_PATH):
        print(f"é”™è¯¯: æ¨¡å‹æ–‡ä»¶ {MODEL_PATH} ä¸å­˜åœ¨!")
        return
    
    if not os.path.exists(TEST_DIR):
        print(f"é”™è¯¯: æµ‹è¯•ç›®å½• {TEST_DIR} ä¸å­˜åœ¨!")
        return
    
    # è¿›è¡Œé¢„æµ‹
    print("å¼€å§‹é¢„æµ‹...")
    results = predict_test_audio(MODEL_PATH, TEST_DIR, DEVICE)
    
    print(f"\né¢„æµ‹å®Œæˆ! å…±é¢„æµ‹äº† {len(results)} ä¸ªæ–‡ä»¶")
    print("é¢„æµ‹ç»“æœ:")
    for filename, emotion in results.items():
        print(f"  {filename}: {emotion}")
    
    # ä¿å­˜ç»“æœåˆ°æœ¬åœ°æ–‡ä»¶
    with open('prediction_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print("\nç»“æœå·²ä¿å­˜åˆ° prediction_results.json")
    
    # æäº¤ç»“æœ
    print("\næäº¤ç»“æœåˆ°è¯„åˆ†æ¥å£...")
    response = submit_results(results, API_URL)
    
    if response and response.status_code == 200:
        print("ç»“æœæäº¤æˆåŠŸ!")
    else:
        print("ç»“æœæäº¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€")

if __name__ == "__main__":
    main() 
"""
    return message


def sayTest004():
    """
    è¾“å‡ºè¯­éŸ³è¯†åˆ«(ASR)ä»£ç ç¤ºä¾‹
    
    æ˜¾ç¤ºä½¿ç”¨ç§‘å¤§è®¯é£è¯­éŸ³è¯†åˆ«APIçš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œ
    åŒ…æ‹¬å¼‚æ­¥å¤„ç†å’Œé…ç½®æ–‡ä»¶ä½¿ç”¨ã€‚
    
    Returns:
        str: è¿”å›è¯­éŸ³è¯†åˆ«ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    import asyncio
import os
from dotenv import load_dotenv
from ifly_tek_asr import iFlyTekASR


async def main():
    # ä».envæ–‡ä»¶åŠ è½½é…ç½®
    load_dotenv()
    appid = os.getenv("appid")
    api_key = os.getenv("apikey")
    api_secret = os.getenv("apisecret")
    file_path = os.getenv("file_path")

    # ç¡®ä¿è·¯å¾„å…¼å®¹Windowså’ŒLinux
    file_path = file_path.replace("\\", "/")

    print(f"Starting ASR with file: {file_path}")

    # åˆ›å»ºASRå®¢æˆ·ç«¯
    asr_client = iFlyTekASR(
        app_id=appid,
        api_key=api_key,
        api_secret=api_secret
    )
    # æ‰§è¡Œè½¬å½•
    result = await asr_client.transcribe_audio(file_path)
    # ä¿å­˜ç»“æœ
    with open("./asr_result.txt", "w", encoding="utf-8") as f:
        f.write(result)
    print(f"\nResult saved to asr_result.txt:\n{result}")


if __name__ == "__main__":
    asyncio.run(main())
    """
    return message


def sayTest005():
    """
    è¾“å‡ºè¯­éŸ³åˆæˆ(TTS)ä»£ç ç¤ºä¾‹
    
    æ˜¾ç¤ºä½¿ç”¨ç§‘å¤§è®¯é£è¯­éŸ³åˆæˆAPIçš„å®Œæ•´ä»£ç ç¤ºä¾‹ï¼Œ
    åŒ…æ‹¬å¤šç§å‘éŸ³äººé€‰æ‹©å’Œé…ç½®ç®¡ç†ã€‚
    
    Returns:
        str: è¿”å›è¯­éŸ³åˆæˆä»£ç å­—ç¬¦ä¸²
    """
    message = """
        import os
from dotenv import load_dotenv
import time

# ä»å½“å‰ç›®å½•å¯¼å…¥XunfeiTTSç±»
from xunfei_tts import XunfeiTTS  # å‡è®¾ä»£ç ä¿å­˜åœ¨xunfei_tts.pyä¸­


def main():
    # 1. åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()
    app_id = os.getenv("appid")
    api_key = os.getenv("apikey")
    api_secret = os.getenv("apisecret")

    # 2. ç¡®ä¿é…ç½®å®Œæ•´
    if not all([app_id, api_key, api_secret]):
        print("âŒ ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡é…ç½®")
        print("è¯·ç¡®ä¿.envæ–‡ä»¶åŒ…å«APPID, API_KEY, API_SECRET")
        return

    print("âœ… æˆåŠŸåŠ è½½APIé…ç½®")

    # 3. åˆ›å»ºTTSå®¢æˆ·ç«¯
    tts_client = XunfeiTTS(
        app_id=app_id,
        api_key=api_key,
        api_secret=api_secret
    )
    print("ğŸ™ï¸ TTSå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")

    # 4. è®¾ç½®è¦åˆæˆçš„æ–‡æœ¬
    test_text = "ç¥ä½ åœ¨ä»Šå¤©çš„æ¯”èµ›ä¸­å–å¾—å¥½æˆç»©ï¼"  # ç¤ºä¾‹æ–‡æœ¬

    # 5. é€‰æ‹©å‘éŸ³äºº
    selected_voice = "x5_lingfeiyi_flow"  # é»˜è®¤è†é£é€¸

    # 6. è®¾ç½®è¾“å‡ºæ–‡ä»¶
    output_file = "data/tts_sample.mp3"

    # 7. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    print("â³ å¼€å§‹è¯­éŸ³åˆæˆ...")
    print(f"æ–‡æœ¬: {test_text}")
    print(f"å‘éŸ³äºº: {selected_voice}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

    # 8. æ‰§è¡Œè¯­éŸ³åˆæˆ
    start_time = time.time()
    success = tts_client.synthesize_speech(
        text=test_text,
        output_file=output_file,
        voice=selected_voice
    )
    processing_time = time.time() - start_time

    # 9. å¤„ç†ç»“æœ
    if success:
        print(f"âœ… è¯­éŸ³åˆæˆæˆåŠŸï¼è€—æ—¶: {processing_time:.2f}ç§’")
        print(f"æ–‡ä»¶å·²ä¿å­˜è‡³: {os.path.abspath(output_file)}")
    else:
        print("âŒ è¯­éŸ³åˆæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—äº†è§£è¯¦æƒ…")


if __name__ == "__main__":
    main()
    """
    return message


def sayTest003Plus_1():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†ææ¨ç†æ¨¡å—ä»£ç ç¤ºä¾‹ (inference.py)
    
    æ˜¾ç¤ºå®Œæ•´çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹æ¨ç†å®ç°ï¼ŒåŒ…æ‹¬ï¼š
    - éŸ³é¢‘ç‰¹å¾æå–ï¼ˆMelé¢‘è°±+F0åŸºé¢‘ï¼‰
    - æ¨¡å‹åŠ è½½å’Œé¢„æµ‹
    - æ‰¹é‡æµ‹è¯•å’Œç»“æœæäº¤
    - APIæ¥å£å¯¹æ¥
    
    Returns:
        str: è¿”å›inference.pyå®Œæ•´ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    #inference.py
    import os
import librosa
import numpy as np
import torch
import requests
import json

from model import EmotionCNN
from dataset import EMOTION_LABELS

def extract_features_for_inference(filepath, sr=16000, n_mels=128, max_len=300):
    #ä¸ºæ¨ç†æå–éŸ³é¢‘ç‰¹å¾
    try:
        y, _ = librosa.load(filepath, sr=sr)
        
        # æå–Melé¢‘è°±
        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)
        mel_db = librosa.power_to_db(mel)
        
        # æå–åŸºé¢‘F0
        f0, _, _ = librosa.pyin(y, fmin=70, fmax=400, sr=sr)
        f0 = np.nan_to_num(f0, nan=0.0)
        
        # å¯¹é½æ—¶é—´ç»´åº¦
        T = mel_db.shape[1]
        if len(f0) != T:
            f0 = np.interp(np.linspace(0, len(f0), T), np.arange(len(f0)), f0)
        
        # æ·»åŠ F0ç‰¹å¾
        f0 = f0[np.newaxis, :]  # shape: (1, T)
        features = np.vstack([mel_db, f0])  # shape: (129, T)
        
        # é•¿åº¦å¯¹é½
        if features.shape[1] < max_len:
            # é›¶å¡«å……
            pad = np.zeros((129, max_len - features.shape[1]), dtype=np.float32)
            features = np.concatenate((features, pad), axis=1)
        elif features.shape[1] > max_len:
            # è£å‰ª
            features = features[:, :max_len]
        
        # è½¬ä¸º Tensor (1, 129, T)
        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)
    
    except Exception as e:
        print(f"Error processing {filepath}: {e}")
        # è¿”å›é›¶ç‰¹å¾
        return torch.zeros((1, 129, max_len), dtype=torch.float32)

def load_model(model_type: str, model_path: str, device, max_len=300):
    #åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = EmotionCNN(n_input=129)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()
    return model

def predict_emotion(filepath, model_type='cnn', model_path='emotion_cnn.pt', max_len=300):
    #é¢„æµ‹å•ä¸ªéŸ³é¢‘æ–‡ä»¶çš„æƒ…æ„Ÿ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = load_model(model_type, model_path, device, max_len)
    features = extract_features_for_inference(filepath, max_len=max_len).to(device)
    
    with torch.no_grad():
        logits = model(features)
        probs = torch.softmax(logits, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        emotion = EMOTION_LABELS[pred_idx]
    
    return emotion, probs.squeeze().cpu().numpy()

def test_model_inference():
    #æµ‹è¯•æ¨¡å‹æ¨ç†ï¼Œä½¿ç”¨ ./data/task-3-test-audio ä¸­çš„æ‰€æœ‰ wav æ–‡ä»¶
    test_dir = "./data/task-3-test-audio"
    results = {}
    
    if not os.path.exists(test_dir):
        print(f"âŒ æµ‹è¯•ç›®å½•ä¸å­˜åœ¨: {test_dir}")
        return {}
    
    # è·å–æ‰€æœ‰ wav æ–‡ä»¶
    wav_files = [f for f in os.listdir(test_dir) if f.endswith('.wav')]
    print(f"ğŸ“ æ‰¾åˆ° {len(wav_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
    
    if len(wav_files) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° wav æ–‡ä»¶")
        return {}
    
    # å¯¹æ¯ä¸ªæ–‡ä»¶è¿›è¡Œé¢„æµ‹
    for i, filename in enumerate(wav_files):
        filepath = os.path.join(test_dir, filename)
        filename_without_ext = os.path.splitext(filename)[0]
        
        try:
            emotion, probs = predict_emotion(filepath)
            results[filename_without_ext] = emotion
            
            print(f"[{i+1}/{len(wav_files)}] {filename_without_ext}: {emotion}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            results[filename_without_ext] = "neutral"  # é»˜è®¤æƒ…æ„Ÿ
    
    print(f"âœ… å¤„ç†å®Œæˆ! å…± {len(results)} ä¸ªç»“æœ")
    return results

def submit_results(results):
    #å°†ç»“æœæäº¤åˆ°è¯„åˆ†æ¥å£
    url = "https://1f9f25e80908.ngrok-free.app/grade_emotion/"
    
    payload = {
        "answer": results
    }
    
    try:
        print("ğŸ“¤ æäº¤ç»“æœåˆ°è¯„åˆ†æ¥å£...")
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code == 200:
            print("âœ… ç»“æœæäº¤æˆåŠŸ!")
            print("ğŸ“Š è¯„åˆ†ç»“æœ:")
            print(response.text)
        else:
            print(f"âŒ æäº¤å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ results.json")
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ä½œä¸ºå¤‡ä»½
        with open('results.json', 'w', encoding='utf-8') as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)

def main():
    #ä¸»å‡½æ•°
    print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†æµ‹è¯•...")
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = 'emotion_cnn.pt'
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶")
        return
    
    # è¿›è¡Œæ¨ç†æµ‹è¯•
    results = test_model_inference()
    
    if results:
        print("\nğŸ“‹ é¢„æµ‹ç»“æœ:")
        for filename, emotion in results.items():
            print(f"  {filename}: {emotion}")
        
        # æäº¤ç»“æœ
        submit_results(results)
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ç»“æœ")

if __name__ == "__main__":
    main() 

    """
    return message


def sayTest003Plus_2():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†æè®­ç»ƒæ¨¡å—ä»£ç ç¤ºä¾‹ (train.py)
    
    æ˜¾ç¤ºå®Œæ•´çš„CNNæ¨¡å‹è®­ç»ƒå®ç°ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    - æ¨¡å‹è®­ç»ƒå¾ªç¯
    - éªŒè¯å’Œæ€§èƒ½è¯„ä¼°
    - è®­ç»ƒå†å²å¯è§†åŒ–
    - æ¨¡å‹ä¿å­˜å’Œç®¡ç†
    
    Returns:
        str: è¿”å›train.pyå®Œæ•´ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    # train.py
    import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from dataset import EmotionFeatureDataset, EMOTION_LABELS
from model import EmotionCNN

# é…ç½®é¡¹
MODEL_TYPE = 'cnn'
BATCH_SIZE = 16
N_EPOCHS = 40
LEARNING_RATE = 1e-3
MAX_LEN = 300
MODEL_PATH = f'emotion_{MODEL_TYPE}.pt'
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# æ•°æ®åŠ è½½
print("ğŸ“¥ åŠ è½½æ•°æ®é›†...")
train_dataset = EmotionFeatureDataset('datasat_train', split='train', max_len=MAX_LEN)
val_dataset = EmotionFeatureDataset('datasat_train', split='val', max_len=MAX_LEN)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

# æ¨¡å‹åˆå§‹åŒ–
model = EmotionCNN(n_input=129)
model.to(DEVICE)

print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

# è®¡ç®—ç±»åˆ«æƒé‡ä»¥è§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜
from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight('balanced', classes=np.arange(len(EMOTION_LABELS)), y=train_dataset.labels)
class_weights = torch.FloatTensor(class_weights).to(DEVICE)

# è®­ç»ƒå‡†å¤‡
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

# è®°å½•è®­ç»ƒå†å²
train_losses = []
val_accuracies = []

def evaluate():
    #è¯„ä¼°æ¨¡å‹æ€§èƒ½
    model.eval()
    y_true, y_pred = [], []
    total_correct = 0
    total_samples = 0
    
    with torch.no_grad():
        for X, y in val_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            outputs = model(X)
            preds = torch.argmax(outputs, dim=1)
            
            y_pred.extend(preds.cpu().numpy())
            y_true.extend(y.cpu().numpy())
            
            total_correct += (preds == y).sum().item()
            total_samples += y.size(0)
    
    accuracy = total_correct / total_samples
    
    print("ğŸ¯ éªŒè¯é›†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_true, y_pred, target_names=EMOTION_LABELS, zero_division=0))
    
    # æ‰“å°æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    print("ğŸ“Š æ··æ·†çŸ©é˜µ:")
    print(cm)
    
    return accuracy

def train():
    #è®­ç»ƒå‡½æ•°
    print(f"ğŸš€ ä½¿ç”¨æ¨¡å‹: {MODEL_TYPE.upper()} å¼€å§‹è®­ç»ƒ...")
    
    best_accuracy = 0.0
    
    for epoch in range(N_EPOCHS):
        model.train()
        running_loss = 0.0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{N_EPOCHS}")
        
        for X, y in progress_bar:
            X, y = X.to(DEVICE), y.to(DEVICE)
            
            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        avg_loss = running_loss / len(train_loader)
        train_losses.append(avg_loss)
        
        print(f"ğŸ§ª Epoch {epoch+1} | Loss: {avg_loss:.4f}")
        
        # è¯„ä¼°
        accuracy = evaluate()
        val_accuracies.append(accuracy)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            torch.save(model.state_dict(), MODEL_PATH)
            print(f"âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! å‡†ç¡®ç‡: {accuracy:.4f}")
        
        print("-" * 60)
    
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history()

def plot_training_history():
    #ç»˜åˆ¶è®­ç»ƒå†å²
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses)
    ax1.set_title('Training Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(val_accuracies)
    ax2.set_title('Validation Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.show()
    print("ğŸ“ˆ è®­ç»ƒå†å²å›¾å·²ä¿å­˜ä¸º training_history.png")

if __name__ == "__main__":
    train() 
    """
    return message


def sayTest003Plus_3():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†æCNNæ¨¡å‹ä»£ç ç¤ºä¾‹ (model.py)
    
    æ˜¾ç¤ºé«˜çº§CNNæ¨¡å‹æ¶æ„å®ç°ï¼ŒåŒ…æ‹¬ï¼š
    - å¤šå±‚å·ç§¯å’Œæ± åŒ–ç»“æ„
    - æ‰¹å½’ä¸€åŒ–å’ŒDropoutæ­£åˆ™åŒ–
    - å…¨å±€å¹³å‡æ± åŒ–
    - å¤šå±‚å…¨è¿æ¥åˆ†ç±»å™¨
    
    Returns:
        str: è¿”å›model.pyå®Œæ•´ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    # model.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class EmotionCNN(nn.Module):
    def __init__(self, n_input=129, n_classes=6):
        super(EmotionCNN, self).__init__()
        
        # å°† (129, T) è§†ä¸ºä¸€ä¸ª"å•é€šé“å›¾åƒ"
        # è¾“å…¥å½¢çŠ¶: (batch_size, 1, 129, max_len)
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Conv2d(1, 64, kernel_size=(3, 3), padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout1 = nn.Dropout2d(0.2)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout2 = nn.Dropout2d(0.2)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout3 = nn.Dropout2d(0.2)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        self.conv4 = nn.Conv2d(256, 512, kernel_size=(3, 3), padding=1)
        self.bn4 = nn.BatchNorm2d(512)
        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2))
        self.dropout4 = nn.Dropout2d(0.2)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(512, 256)
        self.dropout5 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, 128)
        self.dropout6 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, n_classes)
    
    def forward(self, x):
        # è¾“å…¥å½¢çŠ¶: (batch_size, 129, max_len)
        # æ·»åŠ é€šé“ç»´åº¦: (batch_size, 1, 129, max_len)
        if len(x.shape) == 3:
            x = x.unsqueeze(1)
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)
        x = self.dropout1(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        x = self.dropout2(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        x = self.dropout3(x)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        x = self.conv4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.pool4(x)
        x = self.dropout4(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)  # å±•å¹³
        
        # å…¨è¿æ¥å±‚
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout5(x)
        
        x = self.fc2(x)
        x = F.relu(x)
        x = self.dropout6(x)
        
        x = self.fc3(x)
        
        return x 
"""
    return message


def sayTest003Plus_4():
    """
    è¾“å‡ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†å¤„ç†ä»£ç ç¤ºä¾‹ (dataset.py)
    
    æ˜¾ç¤ºé«˜çº§æ•°æ®é›†å¤„ç†å®ç°ï¼ŒåŒ…æ‹¬ï¼š
    - éŸ³é¢‘ç‰¹å¾æå–ï¼ˆMelé¢‘è°±+F0åŸºé¢‘ï¼‰
    - æ•°æ®é›†åˆ†å‰²å’Œç¼“å­˜æœºåˆ¶
    - ç‰¹å¾æ ‡å‡†åŒ–å’Œå¯¹é½
    - PyTorch Datasetæ¥å£å®ç°
    
    Returns:
        str: è¿”å›dataset.pyå®Œæ•´ä»£ç å­—ç¬¦ä¸²
    """
    message = """
    # dataset.py
    import os
import librosa
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
import pickle

EMOTION_LABELS = ["angry", "fear", "happy", "neutral", "sad", "surprise"]
label_map = {label: idx for idx, label in enumerate(EMOTION_LABELS)}

class EmotionFeatureDataset(Dataset):
    def __init__(self, root_dir, sr=16000, n_mels=128, max_len=380, split='train', 
                 train_ratio=0.8, random_state=42, cache_dir="feature_cache"):
        self.root_dir = root_dir
        self.sr = sr
        self.n_mels = n_mels
        self.max_len = max_len
        self.split = split
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®æ–‡ä»¶è·¯å¾„å’Œæ ‡ç­¾
        self.file_paths, self.labels = self._load_file_paths()
        
        # åˆ†å‰²æ•°æ®é›†
        self._split_dataset(train_ratio, random_state)
        
        # æå–æˆ–åŠ è½½ç‰¹å¾
        self._prepare_features()
    
    def _load_file_paths(self):
        file_paths = []
        labels = []
        
        for emotion in EMOTION_LABELS:
            emotion_dir = os.path.join(self.root_dir, emotion)
            if os.path.exists(emotion_dir):
                for file_name in os.listdir(emotion_dir):
                    if file_name.endswith('.wav'):
                        file_paths.append(os.path.join(emotion_dir, file_name))
                        labels.append(label_map[emotion])
        
        return file_paths, labels
    
    def _split_dataset(self, train_ratio, random_state):
        # åˆ†å‰²æ•°æ®é›†
        if len(self.file_paths) > 0:
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                self.file_paths, self.labels, 
                train_size=train_ratio, 
                random_state=random_state,
                stratify=self.labels
            )
            
            if self.split == 'train':
                self.file_paths = train_paths
                self.labels = train_labels
            else:  # val
                self.file_paths = val_paths
                self.labels = val_labels
        
        print(f"{self.split} dataset size: {len(self.file_paths)}")
    
    def _extract_features(self, file_path):
        #æå–éŸ³é¢‘ç‰¹å¾ï¼šMelé¢‘è°± + åŸºé¢‘F0
        try:
            # åŠ è½½éŸ³é¢‘
            y, _ = librosa.load(file_path, sr=self.sr)
            
            # æå–Melé¢‘è°±
            mel = librosa.feature.melspectrogram(y=y, sr=self.sr, n_mels=self.n_mels)
            mel_db = librosa.power_to_db(mel)
            
            # æå–åŸºé¢‘F0
            f0, _, _ = librosa.pyin(y, fmin=70, fmax=400, sr=self.sr)
            f0 = np.nan_to_num(f0, nan=0.0)
            
            # å¯¹é½æ—¶é—´ç»´åº¦
            T = mel_db.shape[1]
            if len(f0) != T:
                f0 = np.interp(np.linspace(0, len(f0), T), np.arange(len(f0)), f0)
            
            # æ·»åŠ F0ç‰¹å¾
            f0 = f0[np.newaxis, :]  # shape: (1, T)
            features = np.vstack([mel_db, f0])  # shape: (129, T)
            
            # é•¿åº¦å¯¹é½
            if features.shape[1] < self.max_len:
                # é›¶å¡«å……
                pad = np.zeros((129, self.max_len - features.shape[1]), dtype=np.float32)
                features = np.concatenate((features, pad), axis=1)
            elif features.shape[1] > self.max_len:
                # è£å‰ª
                features = features[:, :max_len]
            
            return features.astype(np.float32)
        
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            # è¿”å›é›¶ç‰¹å¾
            return np.zeros((129, self.max_len), dtype=np.float32)
    
    def _prepare_features(self):
        #å‡†å¤‡ç‰¹å¾æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶
        cache_file = os.path.join(self.cache_dir, f"features_{self.split}_{self.max_len}.pkl")
        
        if os.path.exists(cache_file):
            print(f"Loading cached features from {cache_file}")
            with open(cache_file, 'rb') as f:
                self.features = pickle.load(f)
        else:
            print(f"Extracting features for {len(self.file_paths)} files...")
            self.features = []
            
            for i, file_path in enumerate(self.file_paths):
                if i % 50 == 0:
                    print(f"Processing {i}/{len(self.file_paths)}")
                
                feature = self._extract_features(file_path)
                self.features.append(feature)
            
            # ç¼“å­˜ç‰¹å¾
            print(f"Caching features to {cache_file}")
            with open(cache_file, 'wb') as f:
                pickle.dump(self.features, f)
        
        print(f"Features shape: {len(self.features)} x {self.features[0].shape}")
    
    def __len__(self):
        return len(self.file_paths)
    
    def __getitem__(self, idx):
        feature = torch.tensor(self.features[idx], dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return feature, label 
    """
    return message



