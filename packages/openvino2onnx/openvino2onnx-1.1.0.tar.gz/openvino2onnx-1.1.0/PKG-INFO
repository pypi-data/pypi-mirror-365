Metadata-Version: 2.4
Name: openvino2onnx
Version: 1.1.0
Summary: An Open Neural Network Exchange (ONNX) Optimization and Transformation Tool.
Author-email: Wenyi Tang <wenyitang@outlook.com>
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development
License-File: LICENSE
Requires-Dist: onnx>=1.17.0
Requires-Dist: networkx
Requires-Dist: numpy<2.2.0
Requires-Dist: tabulate
Requires-Dist: colorlog>=6.9.0
Requires-Dist: termcolor>=3.1.0
Requires-Dist: onnxoptimizer ; extra == "test"
Requires-Dist: onnxruntime ; extra == "test"
Requires-Dist: onnxsim ; extra == "test"
Requires-Dist: openvino>=2025.0.0 ; extra == "test"
Requires-Dist: pre-commit ; extra == "test"
Requires-Dist: pytest ; extra == "test"
Requires-Dist: pytest-cov ; extra == "test"
Requires-Dist: pooch ; extra == "test"
Project-URL: Docs, https://loseall.github.io/openvino2onnx
Project-URL: Home, https://github.com/LoSealL/openvino2onnx
Provides-Extra: test

# OPENVINO2ONNX
A simple tool to convert your IR XML to ONNX file.

[![Checked with pyright](https://microsoft.github.io/pyright/img/pyright_badge.svg)](https://microsoft.github.io/pyright/)

Supported OpenVINO IR Version

- IRv10: openvino>=2020,<2023
- IRv11: openvino>=2023

## Usage

1. Install from PyPI
```shell
pip install openvino2onnx
```

2. Convert IR using CLI
```shell
openvino2onnx model.xml
```

```
usage: openvino2onnx input_model.xml [output_model.onnx]

openvino2onnx command-line api

options:
  -h, --help            show this help message and exit
  -a [ACTIVATE ...], --activate [ACTIVATE ...]
                        select passes to be activated, activate L1, L2 and L3 passes if not set.
  -r [REMOVE ...], --remove [REMOVE ...]
                        specify passes to be removed from activated passes.
  -n, --no-passes       do not run any optimizing passes, just convert the model
  --print [PRINT]       print the name of all optimizing passes
  --format {protobuf,textproto,json,onnxtxt}
                        onnx file format
  -s, --infer-shapes    infer model shapes
  -c CONFIG_FILE, --config-file CONFIG_FILE
                        specify a json-format config file for passes
  -u, --uncheck         no checking output model
  --check               check optimized model with random inputs
  --checker-backend {onnx,openvino,onnxruntime}
                        backend for accuracy checking, defaults to openvino
  -v OPSET_VERSION, --opset-version OPSET_VERSION
                        target opset version, defaults to 19
  -vv [{DEBUG,INFO,WARNING,ERROR,CRITICAL}], --log-level [{DEBUG,INFO,WARNING,ERROR,CRITICAL}]
                        specify the level of log messages to be printed, defaults to INFO
```

To print pass information:

```
openvino2onnx --print all
openvino2onnx --print fuse_swish
openvino2onnx --print l1
```

