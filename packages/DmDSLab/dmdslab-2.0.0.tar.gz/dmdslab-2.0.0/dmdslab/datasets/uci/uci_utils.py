"""
–£—Ç–∏–ª–∏—Ç—ã –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è UCI Dataset Loader.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã, –≤–∫–ª—é—á–∞—è:
- Progress bar —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏
- –§—É–Ω–∫—Ü–∏–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–≥–µ—Ä–∞
- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
"""

import functools
import logging
import time
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union

try:
    from tqdm.auto import tqdm
except ImportError:
    from tqdm import tqdm

import numpy as np
import pandas as pd

from dmdslab.datasets.uci.uci_types import DatasetID, Domain, LogLevel, TaskType

# Type variable –¥–ª—è –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–≤
F = TypeVar("F", bound=Callable[..., Any])


# ============================================================================
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# ============================================================================


def _log_handler(arg0, numeric_level, formatter, logger):
    arg0.setLevel(numeric_level)
    arg0.setFormatter(formatter)
    logger.addHandler(arg0)


def setup_logger(
    name: str = "uci",
    level: Union[str, LogLevel] = LogLevel.INFO,
    log_file: Optional[Path] = None,
    format_string: Optional[str] = None,
) -> logging.Logger:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è UCI Dataset Loader.

    Args:
        name: –ò–º—è –ª–æ–≥–≥–µ—Ä–∞
        level: –£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ LogLevel enum)
        log_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        format_string: –§–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç –ª–æ–≥–≥–µ—Ä–∞
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if isinstance(level, LogLevel):
        numeric_level = level.numeric_level
    else:
        numeric_level = getattr(logging, level.upper())

    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä
    logger = logging.getLogger(name)
    logger.setLevel(numeric_level)

    # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
    logger.handlers = []

    # –§–æ—Ä–º–∞—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    if format_string is None:
        format_string = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    formatter = logging.Formatter(format_string)

    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    console_handler = logging.StreamHandler()
    _log_handler(console_handler, numeric_level, formatter, logger)
    # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
    if log_file is not None:
        file_handler = logging.FileHandler(log_file, encoding="utf-8")
        _log_handler(file_handler, numeric_level, formatter, logger)
    return logger


def log_execution_time(logger: Optional[logging.Logger] = None) -> Callable[[F], F]:
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏.

    Args:
        logger: –õ–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)

    Returns:
        –î–µ–∫–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            nonlocal logger
            if logger is None:
                logger = logging.getLogger(func.__module__)

            start_time = time.time()
            logger.debug(f"–ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è {func.__name__}")

            try:
                result = func(*args, **kwargs)
                elapsed_time = time.time() - start_time
                logger.debug(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ {func.__name__} –∑–∞ {elapsed_time:.2f} —Å–µ–∫")
                return result
            except Exception as e:
                elapsed_time = time.time() - start_time
                logger.error(
                    f"–û—à–∏–±–∫–∞ –≤ {func.__name__} –ø–æ—Å–ª–µ {elapsed_time:.2f} —Å–µ–∫: {e}"
                )
                raise

        return wrapper

    return decorator


# ============================================================================
# Progress Bar
# ============================================================================


def create_progress_bar(
    total: Optional[int] = None,
    desc: str = "–ü—Ä–æ–≥—Ä–µ—Å—Å",
    unit: str = "it",
    leave: bool = True,
    disable: bool = False,
    **kwargs,
) -> tqdm:
    """–°–æ–∑–¥–∞–Ω–∏–µ progress bar —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.

    Args:
        total: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
        desc: –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        unit: –ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
        leave: –û—Å—Ç–∞–≤–∏—Ç—å progress bar –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        disable: –û—Ç–∫–ª—é—á–∏—Ç—å progress bar
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è tqdm

    Returns:
        –û–±—ä–µ–∫—Ç tqdm progress bar
    """
    default_kwargs = {
        "total": total,
        "desc": desc,
        "unit": unit,
        "leave": leave,
        "disable": disable,
        "ncols": 100,
        "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]",
    }
    default_kwargs.update(kwargs)

    return tqdm(**default_kwargs)


@contextmanager
def progress_context(
    desc: str = "–û–±—Ä–∞–±–æ—Ç–∫–∞", total: Optional[int] = None, disable: bool = False
):
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è progress bar.

    Args:
        desc: –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞
        total: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
        disable: –û—Ç–∫–ª—é—á–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ

    Yields:
        –û–±—ä–µ–∫—Ç progress bar
    """
    pbar = create_progress_bar(total=total, desc=desc, disable=disable)
    try:
        yield pbar
    finally:
        pbar.close()


def download_with_progress(
    download_func: Callable,
    total_size: Optional[int] = None,
    desc: str = "–ó–∞–≥—Ä—É–∑–∫–∞",
    chunk_size: int = 8192,
) -> Any:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.

    Args:
        download_func: –§—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
        total_size: –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö (–µ—Å–ª–∏ –∏–∑–≤–µ—Å—Ç–µ–Ω)
        desc: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏
    """
    with progress_context(desc=desc, total=total_size) as pbar:

        def update_callback(chunk_size: int):
            pbar.update(chunk_size)

        return download_func(progress_callback=update_callback)


# ============================================================================
# –í–∞–ª–∏–¥–∞—Ü–∏—è
# ============================================================================


def validate_dataset_id(dataset_id: Any) -> DatasetID:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è ID –¥–∞—Ç–∞—Å–µ—Ç–∞.

    Args:
        dataset_id: ID –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        –í–∞–ª–∏–¥–Ω—ã–π ID –¥–∞—Ç–∞—Å–µ—Ç–∞

    Raises:
        ValueError: –ï—Å–ª–∏ ID –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
    """
    if isinstance(dataset_id, (int, str)):
        if isinstance(dataset_id, str):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è
            if not dataset_id.strip():
                raise ValueError("ID –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π")
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ int, –µ—Å–ª–∏ —ç—Ç–æ —á–∏—Å–ª–æ
            try:
                return int(dataset_id)
            except ValueError:
                return dataset_id
        elif isinstance(dataset_id, int):
            if dataset_id <= 0:
                raise ValueError("ID –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º")
            return dataset_id

    raise ValueError(
        f"ID –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å int –∏–ª–∏ str, –ø–æ–ª—É—á–µ–Ω {type(dataset_id).__name__}"
    )


# ============================================================================
# –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
# ============================================================================


def format_dataset_info(
    dataset_id: DatasetID,
    name: str,
    task_type: TaskType,
    n_instances: int,
    n_features: int,
    domain: Optional[Domain] = None,
    has_missing: bool = False,
    cached: Optional[bool] = False,
    cache_size: Optional[float] = None,
) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –≤—ã–≤–æ–¥–∞.

    Args:
        dataset_id: ID –¥–∞—Ç–∞—Å–µ—Ç–∞
        name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏
        n_instances: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        n_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        domain: –î–æ–º–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç–∞
        has_missing: –ï—Å—Ç—å –ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        cached: –ù–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –≤ –∫–µ—à–µ (None –µ—Å–ª–∏ –∫–µ—à –æ—Ç–∫–ª—é—á–µ–Ω)
        cache_size: –†–∞–∑–º–µ—Ä –≤ –∫–µ—à–µ (–≤ –ú–ë)

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    lines = [
        f"{'='*60}",
        f"–î–∞—Ç–∞—Å–µ—Ç: {name} (ID: {dataset_id})",
        f"{'='*60}",
        f"–¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type.value}",
        f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {n_instances} √ó {n_features}",
    ]

    if domain:
        lines.append(f"–î–æ–º–µ–Ω: {domain.value}")

    if has_missing:
        lines.append("‚ö†Ô∏è  –°–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")

    if cached:
        cache_info = "‚úÖ –í –∫–µ—à–µ"
        if cache_size is not None:
            cache_info += f" ({cache_size:.2f} –ú–ë)"
        lines.append(cache_info)
    elif cached is None:
        lines.append("‚ö™ –ö–µ—à –æ—Ç–∫–ª—é—á–µ–Ω")
    else:
        lines.append("‚ùå –ù–µ –≤ –∫–µ—à–µ")

    lines.append(f"{'='*60}")

    return "\n".join(lines)


def print_dataset_summary(
    features: Union[np.ndarray, pd.DataFrame],
    target: Optional[Union[np.ndarray, pd.Series]] = None,
    feature_names: Optional[List[str]] = None,
    categorical_indices: Optional[List[int]] = None,
    max_features: int = 10,
) -> None:
    """–í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.

    Args:
        features: –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        target: –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        feature_names: –ò–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        categorical_indices: –ò–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        max_features: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—ã–≤–æ–¥–∞
    """
    print("\nüìä –°–≤–æ–¥–∫–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É:")
    print("-" * 40)

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    if isinstance(features, pd.DataFrame):
        n_samples, n_features = features.shape
        print("–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: pandas.DataFrame")
    else:
        n_samples, n_features = (
            features.shape if features.ndim > 1 else (features.shape[0], 1)
        )
        print("–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: numpy.ndarray")

    print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {n_samples} √ó {n_features}")

    # –ò–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if feature_names:
        print(
            f"\n–ü—Ä–∏–∑–Ω–∞–∫–∏ ({min(max_features, len(feature_names))} –∏–∑ {len(feature_names)}):"
        )
        for i, name in enumerate(feature_names[:max_features]):
            cat_marker = (
                " [–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π]"
                if categorical_indices and i in categorical_indices
                else ""
            )
            print(f"  {i+1}. {name}{cat_marker}")
        if len(feature_names) > max_features:
            print(f"  ... –∏ –µ—â–µ {len(feature_names) - max_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    if target is not None:
        print("\n–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è:")
        if isinstance(target, (pd.Series, np.ndarray)):
            unique_values = np.unique(target)
            if len(unique_values) <= 10:
                print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {unique_values}")
            else:
                print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {len(unique_values)}")
                print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{np.min(target)}, {np.max(target)}]")

    print("-" * 40)


def format_cache_size(size_bytes: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤ –±–∞–π—Ç–∞—Ö –≤ —á–∏—Ç–∞–µ–º—ã–π –≤–∏–¥.

    Args:
        size_bytes: –†–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
    """
    for unit in ["–ë", "–ö–ë", "–ú–ë", "–ì–ë", "–¢–ë"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} –ü–ë"


def estimate_download_size(
    n_instances: int, n_features: int, dtype_size: int = 8
) -> Tuple[int, str]:
    """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.

    Args:
        n_instances: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        n_features: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        dtype_size: –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –±–∞–π—Ç–∞—Ö

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Ä–∞–∑–º–µ—Ä –≤ –±–∞–π—Ç–∞—Ö, –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞)
    """
    # –û—Ü–µ–Ω–∫–∞: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ + –≤–µ–∫—Ç–æ—Ä —Ü–µ–ª–µ–π + –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    estimated_bytes = (
        n_instances * n_features * dtype_size  # –ü—Ä–∏–∑–Ω–∞–∫–∏
        + n_instances * dtype_size  # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        + n_features * 100  # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∏–º–µ–Ω–∞ –∏ —Ç.–¥.)
    )

    # –î–æ–±–∞–≤–ª—è–µ–º 20% –Ω–∞ –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã
    estimated_bytes = int(estimated_bytes * 1.2)

    return estimated_bytes, format_cache_size(estimated_bytes)


def get_timestamp() -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–∏ –≤ —á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.

    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
    """
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def safe_dict_update(
    base_dict: Dict[str, Any], update_dict: Dict[str, Any], overwrite: bool = False
) -> Dict[str, Any]:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è.

    Args:
        base_dict: –ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å
        update_dict: –°–ª–æ–≤–∞—Ä—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
        overwrite: –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏

    Returns:
        –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
    """
    result = base_dict.copy()

    for key, value in update_dict.items():
        if overwrite or key not in result:
            result[key] = value

    return result


def get_popular_datasets() -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ UCI.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö
    """
    return [
        {
            "id": 17,
            "name": "Breast Cancer Wisconsin (Diagnostic)",
            "task_type": "classification",
            "instances": 569,
            "features": 30,
        },
        {
            "id": 53,
            "name": "Iris",
            "task_type": "classification",
            "instances": 150,
            "features": 4,
        },
        {
            "id": 14,
            "name": "Breast Cancer",
            "task_type": "classification",
            "instances": 286,
            "features": 9,
        },
        {
            "id": 15,
            "name": "Car Evaluation",
            "task_type": "classification",
            "instances": 1728,
            "features": 6,
        },
        {
            "id": 19,
            "name": "Wine",
            "task_type": "classification",
            "instances": 178,
            "features": 13,
        },
        {
            "id": 45,
            "name": "Heart Disease",
            "task_type": "classification",
            "instances": 303,
            "features": 13,
        },
        {
            "id": 80,
            "name": "Optical Recognition of Handwritten Digits",
            "task_type": "classification",
            "instances": 5620,
            "features": 64,
        },
        {
            "id": 697,
            "name": "Dry Bean Dataset",
            "task_type": "classification",
            "instances": 13611,
            "features": 16,
        },
    ]


def create_download_report(
    dataset_ids: List[DatasetID],
    results: Dict[DatasetID, Union[str, Exception]],
    start_time: float,
    cache_dir: Path,
) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.

    Args:
        dataset_ids: –°–ø–∏—Å–æ–∫ ID –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏ (—É—Å–ø–µ—Ö –∏–ª–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ)
        start_time: –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∫–∏
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
    """
    elapsed_time = time.time() - start_time
    successful = sum(not isinstance(r, Exception) for r in results.values())
    failed = len(results) - successful

    lines = [
        "\n" + "=" * 60,
        "üìã –û–¢–ß–ï–¢ –û –ó–ê–ì–†–£–ó–ö–ï –î–ê–¢–ê–°–ï–¢–û–í",
        "=" * 60,
        f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}",
        f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time:.2f} —Å–µ–∫",
        f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–µ—à–∞: {cache_dir}",
        "",
        f"–í—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {len(dataset_ids)}",
        f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {successful}",
        f"‚ùå –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {failed}",
        "",
        "–î–ï–¢–ê–õ–ò:",
        "-" * 60,
    ]

    # –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
    for dataset_id in dataset_ids:
        result = results.get(dataset_id)
        if isinstance(result, Exception):
            status = f"‚ùå –û—à–∏–±–∫–∞: {type(result).__name__}: {str(result)}"
        else:
            status = "‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω"
        lines.append(f"ID {dataset_id}: {status}")

    lines.extend(
        [
            "-" * 60,
            f"–†–∞–∑–º–µ—Ä –∫–µ—à–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {_get_cache_size(cache_dir)}",
            "=" * 60,
        ]
    )

    return "\n".join(lines)


def _get_cache_size(cache_dir: Path) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–µ—à–∞.

    Args:
        cache_dir: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–µ—à–∞

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
    """
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–ª—É—á–∞–π –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–≥–æ –∫–µ—à–∞
    if cache_dir.name == "no_cache":
        return "–ö–µ—à –æ—Ç–∫–ª—é—á–µ–Ω"

    if not cache_dir.exists():
        return "0 –ë"

    total_size = sum(f.stat().st_size for f in cache_dir.rglob("*") if f.is_file())

    return format_cache_size(total_size)
