# PyMCPEvals

> **âš ï¸ Still Under Development** - This project is actively being developed. APIs may change and features are being added. Please use with caution in production environments.

**Server-focused evaluation framework for MCP (Model Context Protocol) servers.**

ğŸš€ **Help MCP server developers test their tools by evaluating server capabilities, not LLM conversation patterns.**

## Features

- ğŸ¯ **Server-Focused Evaluation**: Judges MCP server capabilities, not LLM conversation style
- âœ… **Programmatic Tool Validation**: Instantly fail tests when expected tools aren't called
- ğŸ”§ **Tool Execution Tracking**: Monitor tool success/failure, timing, and error handling
- ğŸ”„ **Multi-turn Trajectories**: Test tool chaining and state management across conversation turns
- âš¡ **Fast Fail Validation**: Deterministic checks before expensive LLM evaluation
- ğŸ› ï¸ **FastMCP Integration**: Seamless connection to MCP servers via stdio or HTTP
- ğŸ“‹ **Multiple Output Formats**: Table, detailed, JSON, and JUnit XML for CI/CD

## Quick Start

```bash
# Install
pip install pymcpevals

# Create a template configuration
pymcpevals init

# Edit evals.yaml with your server and test cases
# Run evaluations
pymcpevals run evals.yaml
```

## Simple Example

Create `evals.yaml`:

```yaml
model:
  provider: openai
  name: gpt-4

server:
  command: ["python", "my_server.py"]

evaluations:
  - name: "weather_planning"
    description: "Can users plan their day with weather info?"
    prompt: "What should I wear tomorrow in San Francisco?"
    expected_result: "Should provide weather forecast and clothing suggestions"
    expected_tools: ["get_weather"]  # âœ… Validates these tools are called
    threshold: 3.5
    
  - name: "data_insights" 
    description: "Can users get insights from their database?"
    prompt: "Show me my best performing products this month"
    expected_result: "Should query database and provide ranked product list"
    expected_tools: ["query_database", "analyze_data"]  # âœ… Must call these exact tools
    threshold: 4.0

  - name: "multi_step_weather"
    description: "Test multi-step weather analysis"
    turns:
      - role: "user"
        content: "What's the weather like in London?"
        expected_tools: ["get_weather"]  # âœ… Per-turn tool validation
      - role: "user"
        content: "And how about Paris?"
        expected_tools: ["get_weather"]
    expected_result: "Should provide weather for both cities"
    threshold: 4.0
```

Run evaluations:

```bash
pymcpevals run evals.yaml
```

You'll get output showing:
- âœ…/âŒ Pass/fail status with scores (1-5 scale) 
- ğŸ”§ **Tool validation**: Instant feedback if expected tools weren't called
- ğŸ“Š **Server scores**: Tool accuracy, availability, error handling, result formatting
- â±ï¸ **Performance metrics**: Tool execution times and success rates
- ğŸ’­ **Server-focused feedback**: Comments about tool capabilities, not conversation style

## How It Works

PyMCPEvals focuses on **server capabilities** you can control as a developer:

1. **ğŸ”— Connect** to your MCP server using FastMCP
2. **ğŸ” Discover** available tools from the server  
3. **âš¡ Execute** user prompts and track tool calls
4. **âœ… Validate** expected tools are called (instant programmatic check)
5. **ğŸ¯ Evaluate** server tool performance (ignores LLM conversation style)
6. **ğŸ“‹ Report** tool execution results and server capabilities

## Core Problem Solved

**"Are my MCP server's tools working correctly and being used as expected?"**

PyMCPEvals separates what you **can control** (server) from what you **cannot** (LLM behavior):

### âœ… **What Server Developers Control (We Test This)**
- Tool implementation correctness
- Tool parameter validation
- Error handling and recovery
- Tool result formatting
- Multi-turn state management

### âŒ **What Server Developers Cannot Control (We Ignore This)**
- LLM conversation patterns
- How LLMs choose to use tools
- LLM response formatting
- Whether LLMs provide intermediate responses

## Evaluation Types

### 1. Single-Prompt Evaluations

Test individual prompts to verify basic functionality:

```yaml
evaluations:
  - name: "basic_weather"
    prompt: "What's the weather in Boston?"
    expected_result: "Should call weather API and return current conditions"
    expected_tools: ["get_weather"]  # Programmatically validates tool usage
    threshold: 3.0
```

**Programmatic Tool Validation**: When `expected_tools` is specified, the test will instantly fail if:
- âŒ Expected tools are not attempted (even if they error)
- âŒ Unexpected tools are called  
- âŒ No tools are called when some were expected

**Server-Focused LLM Evaluation**: The LLM judge focuses only on server capabilities:
- âœ… Were tool results accurate and well-formatted?
- âœ… Did the server provide the necessary tools to complete the task?
- âœ… Did tools execute successfully or handle errors appropriately?
- âŒ Ignores empty content during tool calls (normal behavior)
- âŒ Ignores LLM conversation style and patterns

## Why Server-Focused Evaluation?

Traditional evaluation judges **LLM conversation patterns**, but MCP server developers can't control that. PyMCPEvals focuses on what you **can** control:

```
âŒ Old Approach: "LLM didn't provide intermediate responses"
âœ… New Approach: "Server tools returned correct results in proper format"

âŒ Old Approach: "Conversation flow was awkward" 
âœ… New Approach: "Tools chained successfully across turns"

âŒ Old Approach: "Response formatting was poor"
âœ… New Approach: "Tool error handling worked correctly"
```

**Key Insight**: Empty content during tool calls is **normal** in MCP. PyMCPEvals understands this and evaluates the **server's tool capabilities**, not the LLM's conversation style.

### 2. Multi-Turn Trajectories

Test tool chaining and state management across conversation turns:

```yaml
evaluations:
  - name: "multi_step_calculation"
    description: "Test tool chaining across turns"
    turns:
      - role: "user"
        content: "What is 10 + 5?"
        expected_tools: ["add"]
      - role: "user"  
        content: "Now multiply that result by 2"
        expected_tools: ["multiply"]
    expected_result: "Should chain tools to calculate (10+5)*2 = 30"
    threshold: 4.0
```

**Trajectory Focus**: Tests server capabilities across multiple turns:
- âœ… Can tools be chained together successfully?
- âœ… Does the server maintain state between turns?
- âœ… Do tools provide results in formats that enable chaining?
- âœ… Can the server handle errors and continue the conversation?

## Installation

```bash
pip install pymcpevals
```

## Usage

### CLI Interface

```bash
# Create template config
pymcpevals init evals.yaml

# Run evaluations
pymcpevals run evals.yaml

# Override server for quick testing
pymcpevals run evals.yaml --server "node server.js"

# Override model 
pymcpevals run evals.yaml --model claude-3-opus-20240229 --provider anthropic

# Parallel execution
pymcpevals run evals.yaml --parallel

# Different output formats
pymcpevals run evals.yaml --output table     # Simple table view
pymcpevals run evals.yaml --output detailed  # Detailed with tool info
pymcpevals run evals.yaml --output json      # Full JSON
pymcpevals run evals.yaml --output junit --output-file results.xml  # CI/CD
```

### Simple Interface

```bash
# Direct evaluation: pymcpevals eval <config> <server>
pymcpevals eval evals.yaml server.py
```

## Configuration

### YAML Configuration

```yaml
# Model configuration
model:
  provider: openai     # openai, anthropic, gemini, etc.
  name: gpt-4         # Model name
  # api_key: ${OPENAI_API_KEY}  # Optional, uses env var

# Server configuration  
server:
  # For local servers (stdio transport)
  command: ["python", "my_server.py"]
  env:
    DEBUG: "true"
    
  # For remote servers (HTTP transport)  
  # url: "https://api.example.com/mcp"
  # headers:
  #   Authorization: "Bearer ${API_TOKEN}"

# Evaluations to run
evaluations:
  - name: "basic_functionality"
    description: "Test core server capabilities"  
    prompt: "What can you help me with?"
    expected_result: "Should describe available tools and capabilities"
    threshold: 3.0  # Minimum score to pass (1-5 scale)
    tags: ["basic"]
    
  - name: "specific_task"
    description: "Test domain-specific functionality"
    prompt: "Help me analyze my sales data for trends"
    expected_result: "Should use appropriate tools to analyze sales data"
    expected_tools: ["query_database", "analyze_trends"]  # Programmatic validation
    threshold: 3.5
    tags: ["analysis", "data"]

  - name: "multi_step_task"
    description: "Test multi-step problem solving"
    turns:
      - role: "user"
        content: "I need help with my weather data analysis"
        expected_tools: ["get_weather"]  # Per-turn validation
      - role: "user"
        content: "Can you compare today's weather with last week?"
        expected_tools: ["get_weather", "compare_data"]
    expected_result: "Should gather weather data and perform comparison"
    threshold: 4.0
    tags: ["multi-step"]

# Global settings
timeout: 30.0      # Timeout per evaluation
parallel: false    # Run evaluations in parallel
```

### Environment Variables

```bash
# API keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
```

## Server Transport Support

PyMCPEvals uses [FastMCP](https://github.com/jlowin/fastmcp) for server connections:

### Local Servers (Stdio)

```yaml
server:
  command: ["python", "server.py"]
  env:
    DEBUG: "true"
```

### Remote Servers (HTTP)

```yaml
server:
  url: "https://api.example.com/mcp"  
  headers:
    Authorization: "Bearer ${API_TOKEN}"
    Custom-Header: "value"
```

## Example Output

### Table View (--output table)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Name                                     â”‚ Status â”‚ Acc â”‚ Comp â”‚ Rel â”‚ Clar â”‚ Reas â”‚ Avg  â”‚ Tools â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What is 15 + 27?                         â”‚ PASS   â”‚ 4.5 â”‚ 4.2  â”‚ 5.0 â”‚ 4.8  â”‚ 4.1  â”‚ 4.52 â”‚ âœ“     â”‚
â”‚ What happens if I divide 10 by 0?        â”‚ PASS   â”‚ 4.0 â”‚ 4.1  â”‚ 4.5 â”‚ 4.2  â”‚ 3.8  â”‚ 4.12 â”‚ âœ“     â”‚
â”‚ Multi-turn test                          â”‚ PASS   â”‚ 4.2 â”‚ 4.5  â”‚ 4.8 â”‚ 4.1  â”‚ 4.3  â”‚ 4.38 â”‚ âœ“     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

Summary: 3/3 passed (100.0%) - Average: 4.34/5.0
```

### Detailed View (--output detailed)

```
                                    Evaluation Results                                    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test                    â”‚ Status â”‚ Scoreâ”‚ Expected Tools     â”‚ Tools Used         â”‚ Time   â”‚ Errors â”‚ Notes                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What is 15 + 27?        â”‚ PASS   â”‚ 4.5  â”‚ add                â”‚ add                â”‚ 12ms   â”‚ 0      â”‚ OK                           â”‚
â”‚ What happens if I div...â”‚ PASS   â”‚ 4.1  â”‚ divide             â”‚ divide             â”‚ 8ms    â”‚ 1      â”‚ Handled error correctly      â”‚
â”‚ Multi-turn test         â”‚ PASS   â”‚ 4.4  â”‚ add, multiply      â”‚ add, multiply      â”‚ 23ms   â”‚ 0      â”‚ Tool chaining successful     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”§ Tool Execution Details:
â€¢ add: Called 2 times, avg 10ms, 100% success rate
â€¢ divide: Called 1 time, 8ms, handled error gracefully  
â€¢ multiply: Called 1 time, 13ms, 100% success rate

Summary: 3/3 passed (100.0%) - Average: 4.33/5.0
```

## Development

```bash
# Install in development mode
git clone https://github.com/akshay5995/pymcpevals
cd pymcpevals
pip install -e ".[dev]"

# Run tests
pytest

# Format code
black src/
ruff check src/
mypy src/
```

## Key Benefits

### For MCP Server Developers
- **ğŸ¯ Server-Focused Testing**: Test your server capabilities, not LLM behavior
- **âœ… Instant Tool Validation**: Get immediate feedback if wrong tools are called (no LLM needed)
- **ğŸ”§ Tool Execution Insights**: See success rates, timing, and error handling
- **ğŸ”„ Multi-turn Validation**: Test tool chaining and state management
- **ğŸ“Š Capability Scoring**: LLM judges server tool performance, ignoring conversation style
- **ğŸ› ï¸ Easy Integration**: Works with any MCP server via FastMCP

### For Development Teams  
- **ğŸš€ CI/CD Integration**: JUnit XML output for automated testing pipelines
- **ğŸ“ˆ Progress Tracking**: Monitor improvement over time with consistent scoring
- **ğŸ”„ Regression Testing**: Ensure new changes don't break existing functionality
- **âš–ï¸ Model Comparison**: Test across different LLM providers

## Acknowledgments

ğŸ™ **Huge kudos to [mcp-evals](https://github.com/mclenhard/mcp-evals)** - This Python package was heavily inspired by the excellent Node.js implementation by [@mclenhard](https://github.com/mclenhard).

If you're working in a Node.js environment, definitely check out the original [mcp-evals](https://github.com/mclenhard/mcp-evals) project, which also includes GitHub Action integration and monitoring capabilities.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality  
4. Ensure all tests pass
5. Submit a pull request

## License

MIT - see LICENSE file.
