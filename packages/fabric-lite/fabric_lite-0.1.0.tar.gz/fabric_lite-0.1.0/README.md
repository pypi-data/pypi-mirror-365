# Fabric Lite

Fabric Lite это простой cli интерфейс для взаимодействия с LLM, вдохновленный Fabric.

## Возможности

- Взаимодействие с AI моделями через CLI.
- Использование системных промптов из markdown файлов в директории `patterns/`.
- Поддержка любых Open AI совместимых провайдеров API (например ollama).

## Установка

1.  **Клонирование репозитория:**
    ```bash
    git clone https://github.com/Forwall100/fabric-lite.git
    cd fabric-lite
    ```

2.  **Установка зависимостей:**
    ```bash
    pip install .
    ```
    > Зависимости управляются файлом `pyproject.toml`.

> **Внимание:** Для работы приложения необходимо настроить переменные окружения. Создайте файл `.env` в корневой директории проекта со следующими параметрами:
    ```dotenv
    OPENAI_API_KEY="ваш_openai_api_key"
    OPENAI_API_URL="https://api.openai.com/v1/chat/completions" # Или ваш предпочтительный API endpoint
    FABRIC_MODEL_NAME="gpt-3.5-turbo" # Или ваша предпочтительная модель
    ```
    Вы можете использовать любой OpenAI-совместимый API, просто укажите соответствующий `OPENAI_API_URL`.

## Использование

### Базовое взаимодействие

Отправьте запрос к AI:

```bash
fabric-lite "Какая столица Франции?"
```

### Использование системных промптов (Patterns)

Чтобы использовать системный промпт, укажите его имя с помощью флага `-p` или `--pattern`. Файл с промптом должен находиться в директории `patterns/` (например, `patterns/my_pattern.md`).

```bash
fabric-lite -p my_pattern "Расскажи историю о драконе."
```

### Направление вывода
```bash
cat myservice.log | fabric-lite "Найди и объясни ошибку в этом .log файле"
```

### Список доступных промптов

Чтобы увидеть все доступные файлы системных промптов:

```bash
fabric-lite -l
```
