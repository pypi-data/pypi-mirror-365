"""Web scraping with screenshot fallback for Cloudflare-protected sites."""
import asyncio
import io
import os
import logging
import sys
from typing import Optional

import google.generativeai as genai
from PIL import Image
from playwright.async_api import async_playwright
from langchain_community.document_loaders import WebBaseLoader

from .detector import behind_cloudflare

# Configure logging to show progress by default
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger(__name__)


def _run_async(coro):
    """
    Run async function, handling both new and existing event loops.
    This prevents the RuntimeError: asyncio.run() cannot be called from a running event loop.
    """
    try:
        # Try to get the current event loop
        loop = asyncio.get_running_loop()
        # If we're here, there's already a running loop
        # We need to run the coroutine in a new thread to avoid conflicts
        import concurrent.futures
        import threading
        
        def run_in_thread():
            # Create a new event loop for this thread
            new_loop = asyncio.new_event_loop()
            try:
                return new_loop.run_until_complete(coro)
            finally:
                new_loop.close()
        
        # Run in a separate thread with its own event loop
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(run_in_thread)
            return future.result()
            
    except RuntimeError:
        # No running event loop, safe to use asyncio.run()
        return asyncio.run(coro)


async def _screenshot_ocr(
    url: str,
    api_key: Optional[str] = None,
    timeout: int = 120000,
    headless: bool = True,
    scroll_pause: float = 1.0,
) -> str:
    """
    Take a full-page screenshot and extract text using Gemini OCR.
    
    Args:
        url: The URL to scrape
        api_key: Gemini API key
        timeout: Page load timeout in milliseconds
        headless: Run browser in headless mode
        scroll_pause: Time to wait between scrolls
        
    Returns:
        Extracted text from the screenshot
        
    Raises:
        ValueError: If no API key is provided
        Exception: If scraping fails
    """
    # Configure Gemini API
    api_key = api_key or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("Gemini API key is required. Set GEMINI_API_KEY environment variable or pass api_key parameter.")
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash-lite")

    logger.info("üì∏ Taking screenshot and extracting content (headless: %s)...", headless)
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=headless)
        try:
            page = await browser.new_page(
                user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            
            # Set default timeout for ALL page operations
            page.set_default_timeout(timeout)
            
            await page.goto(url, timeout=timeout)
            await page.wait_for_load_state("load")  # More reliable than "networkidle"

            # Scroll to bottom to load all content
            prev_height = 0
            scroll_attempts = 0
            while True:
                curr_height = await page.evaluate("document.body.scrollHeight")
                if curr_height == prev_height:
                    break
                    
                prev_height = curr_height
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(scroll_pause)
                scroll_attempts += 1

            # Take full-page screenshot
            png_data = await page.screenshot(full_page=True)
            
        finally:
            await browser.close()

    # Process with Gemini
    img = Image.open(io.BytesIO(png_data))
    response = model.generate_content(
        ["Extract all text exactly as it appears on this webpage:", img],
        generation_config={"temperature": 0}
    )
    
    extracted_text = response.text.strip()
    return extracted_text


def _fast_load(url: str) -> str:
    """
    Load webpage content using LangChain WebBaseLoader.
    
    Args:
        url: The URL to load
        
    Returns:
        Extracted text content
    """
    docs = WebBaseLoader([url]).load()
    content = "\n".join(doc.page_content for doc in docs)
    return content


def peek(
    url: str,
    api_key: Optional[str] = None,
    force_ocr: bool = False,
    timeout: int = 120000,
    headless: bool = True,
    scroll_pause: float = 1.0,
) -> str:
    """
    Load webpage content with automatic Cloudflare detection and fallback.
    
    Args:
        url: The URL to scrape
        api_key: Gemini API key for OCR
        force_ocr: If True, skip fast scraping and go directly to OCR method
        timeout: Page load timeout in milliseconds for OCR method
        headless: Run browser in headless mode for OCR method
        scroll_pause: Time in seconds to wait between scrolls for OCR method
        
    Returns:
        Extracted text content from the webpage
        
    Raises:
        ValueError: If OCR method is needed but no Gemini API key is available
    """
    logger.info("üéØ Starting CloudflarePeek for: %s", url)
    
    # Check if site is behind Cloudflare or if user wants to force OCR
    if force_ocr:
        logger.info("üîí Using OCR method (force mode)")
        result = _run_async(
            _screenshot_ocr(url, api_key, timeout, headless, scroll_pause)
        )
        logger.info("‚úÖ Completed! (%d characters extracted)", len(result))
        return result
    
    cf_protected = behind_cloudflare(url)
    if cf_protected:
        logger.info("üõ°Ô∏è Cloudflare detected - using OCR method")
        result = _run_async(
            _screenshot_ocr(url, api_key, timeout, headless, scroll_pause)
        )
        logger.info("‚úÖ Completed! (%d characters extracted)", len(result))
        return result

    # Try fast scraping first
    try:
        logger.info("‚ö° Attempting fast scraping...")
        result = _fast_load(url)
        logger.info("‚úÖ Completed! (%d characters extracted)", len(result))
        return result
    except Exception as e:
        logger.info("‚ö° Fast scraping failed - using OCR method")
        result = _run_async(
            _screenshot_ocr(url, api_key, timeout, headless, scroll_pause)
        )
        logger.info("‚úÖ Completed! (%d characters extracted)", len(result))
        return result