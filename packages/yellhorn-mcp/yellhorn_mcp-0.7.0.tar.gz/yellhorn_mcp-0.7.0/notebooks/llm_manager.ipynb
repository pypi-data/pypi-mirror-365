{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yellhorn MCP Example in Notebook with LLM Manager\n",
    "\n",
    "Instruction: Swap model to get different behavior LLM Manager config (Normal, Test full chunking, Test full with retry, Test full chunking & retry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Import required Yellhorn MCP components\n",
    "from yellhorn_mcp.token_counter import TokenCounter\n",
    "from yellhorn_mcp.llm_manager import LLMManager\n",
    "\n",
    "# Import API clients\n",
    "from google import genai\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from yellhorn_mcp.server import format_metrics_section, calculate_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Keys\n",
    "\n",
    "Set up API keys for Gemini and/or OpenAI. You can either set them in environment variables or directly in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Set API keys directly (not recommended for production)\n",
    "GEMINI_API_KEY = \"\"\n",
    "OPENAI_API_KEY = \"\"\n",
    "MODEL = \"gemini-2.5-flash\"  # or any OpenAI model like \"gpt-4o\"\n",
    "REPO_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Option 2: Get API keys from environment variables (recommended)\n",
    "# GEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set environment variables for server access\n",
    "# os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "# os.environ[\"REPO_PATH\"] = REPO_PATH\n",
    "# os.environ[\"YELLHORN_MCP_MODEL\"] = MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Working with LLMManager\n",
    "\n",
    "Now let's set up and use LLMManager, which provides unified access to different LLM APIs with automatic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API clients\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create LLMManager with custom configuration\n",
    "config = {\n",
    "    \"safety_margin_tokens\": 200,\n",
    "    \"overlap_ratio\": 0.1,\n",
    "    \"aggregation_strategy\": \"concatenate\",\n",
    "    \"chunk_strategy\": \"paragraph\",\n",
    "    # Experimental limits to test chunking & retry behavior\n",
    "    \"model_limits\" : {\n",
    "        \"gpt-4o\": 30000,\n",
    "        \"gemini-2.0-flash-exp\": 10000, \n",
    "        \"o4-mini\": 30000,\n",
    "        \"gemini-2.5-pro\": 1_000_000\n",
    "    }\n",
    "}\n",
    "\n",
    "llm_manager = LLMManager(\n",
    "    openai_client=openai_client,\n",
    "    gemini_client=gemini_client,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "def log_callback(level, message):\n",
    "    \"\"\"Custom log callback function.\"\"\"\n",
    "    print(f\"[{level.upper()}] {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make API calls and handle async operations\n",
    "async def call_model(prompt, model, system_message=None, response_format=None):\n",
    "    \"\"\"Helper function to call a model using LLMManager.\"\"\"\n",
    "    try:\n",
    "        response_dict = await llm_manager.call_llm_with_usage(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "            system_message=system_message,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return response_dict\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test LLM Manager Simple vs Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple vs Chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:34:35] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:34:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=313958;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=288659;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI JSON Response:\n",
      "--------------------------------------------------\n",
      "<yellhorn_mcp.llm_manager.UsageMetadata object at 0x114cf6f50>\n",
      "Sure! Here’s a list of 3 programming languages with their key features:\n",
      "\n",
      "---\n",
      "\n",
      "**1. Python**\n",
      "- Easy-to-read, clean syntax\n",
      "- Extensive standard library\n",
      "- Dynamically typed and interpreted\n",
      "- Large community support\n",
      "- Widely used for web development, data science, automation, and AI\n",
      "\n",
      "---\n",
      "\n",
      "**2. JavaScript**\n",
      "- Runs natively in web browsers\n",
      "- Event-driven, asynchronous programming support\n",
      "- Prototype-based object orientation\n",
      "- Essential for front-end web development\n",
      "- Large ecosystem with frameworks like React, Angular, and Vue\n",
      "\n",
      "---\n",
      "\n",
      "**3. Java**\n",
      "- Statically typed and compiled to bytecode (runs on JVM)\n",
      "- Strong object-oriented programming support\n",
      "- Platform-independent (“write once, run anywhere”)\n",
      "- Robust standard library and tools\n",
      "- Commonly used for enterprise applications, Android development, and backend systems\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "---\n",
      "## Completion Metrics\n",
      "*   **Model Used**: N/A\n",
      "*   **Input Tokens**: N/A\n",
      "*   **Output Tokens**: N/A\n",
      "*   **Total Tokens**: N/A\n",
      "*   **Estimated Cost**: N/A\n"
     ]
    }
   ],
   "source": [
    "# Example 2: OpenAI call with JSON response\n",
    "json_prompt = \"Generate a list of 3 programming languages with their key features.\"\n",
    "\n",
    "model = \"gpt-4.1\"\n",
    "openai_json_response = await call_model(\n",
    "    prompt=json_prompt,\n",
    "    model=model,  # or any available OpenAI model\n",
    "    # response_format=\"json\"\n",
    ")\n",
    "\n",
    "print(\"OpenAI JSON Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(openai_json_response[\"usage_metadata\"])\n",
    "print(openai_json_response[\"content\"])\n",
    "print(format_metrics_section(\"gpt-4.1\",openai_json_response[\"usage_metadata\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellhorn_mcp.llm_manager import ChunkingStrategy\n",
    "from yellhorn_mcp.token_counter import TokenCounter\n",
    "\n",
    "chunks = ChunkingStrategy.split_by_paragraphs(\n",
    "    text=json_prompt,\n",
    "    max_tokens=5000,\n",
    "    token_counter=TokenCounter(),\n",
    "    model=\"gemini-2.0-flash-exp\"\n",
    ")\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(TokenCounter().count_tokens(chunk, \"gemini-2.0-flash-exp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/13/25 19:48:48] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/13/25 19:48:48]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=576016;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=881369;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Response:\n",
      "--------------------------------------------------\n",
      "**Token Chunking: An Overview**\n",
      "\n",
      "**What is Token Chunking?**\n",
      "Token chunking is a technique used in natural language processing (NLP) to break down text into smaller, manageable pieces called \"tokens.\" These tokens can be words, phrases, or even characters, depending on the context and the specific application. The process involves segmenting a continuous stream of text into discrete units that can be analyzed or processed by algorithms.\n",
      "\n",
      "**Why is Token Chunking Important for Large Language Models?**\n",
      "\n",
      "1. **Efficiency**: Large language models (LLMs) often deal with vast amounts of text data. Token chunking allows these models to process text in smaller segments, making computations more efficient and manageable.\n",
      "\n",
      "2. **Context Preservation**: By chunking text into meaningful units, models can better understand the context and relationships between words or phrases. This is crucial for tasks like sentiment analysis, translation, and summarization.\n",
      "\n",
      "3. **Memory Management**: LLMs have limitations on the amount of text they can process at once (often referred to as the \"context window\"). Token chunking helps fit larger texts into these constraints by breaking them into smaller, coherent parts.\n",
      "\n",
      "4. **Improved Performance**: Models trained on chunked data can achieve better performance in various NLP tasks. This is because chunking helps maintain the semantic integrity of the text, allowing the model to learn more effectively.\n",
      "\n",
      "5. **Flexibility**: Different applications may require different chunking strategies (e.g., word-level, sentence-level, or paragraph-level). Token chunking provides the flexibility to adapt to these varying needs.\n",
      "\n",
      "In summary, token chunking is a fundamental technique that enhances the efficiency, context understanding, and overall performance of large language models in processing and analyzing text data.\n",
      "\n",
      "\n",
      "---\n",
      "## Completion Metrics\n",
      "*   **Model Used**: `gpt-4o-mini`\n",
      "*   **Input Tokens**: 15024\n",
      "*   **Output Tokens**: 356\n",
      "*   **Total Tokens**: 15380\n",
      "*   **Estimated Cost**: $0.0025\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple Gemini call\n",
    "prompt = \"Explain what token chunking is and why it's important for large language models.\"*1000\n",
    "system_message = \"You are a helpful AI assistant that provides clear and concise explanations.\"\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "gemini_response = await call_model(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    system_message=system_message\n",
    ")\n",
    "\n",
    "print(\"Gemini Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(gemini_response[\"content\"])\n",
    "print(format_metrics_section(model,gemini_response[\"usage_metadata\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<yellhorn_mcp.llm_manager.UsageMetadata at 0x10cfdd810>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_response[\"usage_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Grounded Search\n",
    "\n",
    "Google Search Grounding is a feature available for Gemini models that allows them to search the web and include citations in their responses. This is particularly useful for getting up-to-date information and verifying facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the search grounding utilities\n",
    "from yellhorn_mcp.search_grounding import _get_gemini_search_tools, add_citations_from_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:22:16] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> AFC is enabled with max remote calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>.                               <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">models.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7118</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:22:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m AFC is enabled with max remote calls: \u001b[1;36m10\u001b[0m.                               \u001b]8;id=502880;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125787;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\u001b\\\u001b[2m7118\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:22:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                                     <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">:generateContent</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:22:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m                                                     \u001b]8;id=471735;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=61373;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94m:generateContent\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                     \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response WITHOUT Search Grounding:\n",
      "--------------------------------------------------\n",
      "Of course. Here is a summary of Tesla's (TSLA) stock price and its recent performance.\n",
      "\n",
      "As an AI, I cannot give you real-time, up-to-the-second stock data. Stock prices are highly volatile and change constantly during market hours.\n",
      "\n",
      "However, I can provide you with the most recent closing price and a summary of its recent performance.\n",
      "\n",
      "### **Tesla (TSLA) Stock Price**\n",
      "\n",
      "For the most current, live price, please check a reliable financial news source like:\n",
      "\n",
      "*   **Google Finance**\n",
      "*   **Yahoo Finance**\n",
      "*   **Bloomberg**\n",
      "*   **Reuters**\n",
      "\n",
      "As of the market close on **June 17, 2024**, the approximate stock price for Tesla (TSLA) was:\n",
      "\n",
      "*   **~$187.44**\n",
      "\n",
      "### **Recent Performance Summary**\n",
      "\n",
      "Tesla's stock has had a very eventful and volatile year in 2024. Here is a breakdown of its recent performance:\n",
      "\n",
      "*   **Last Trading Day (June 17, 2024):** The stock saw a significant gain of over **+5%**. This surge was largely attributed to reports that Tesla has received approval to test its advanced driver-assistance system (FSD) on some streets in Shanghai, a key step for its rollout in China.\n",
      "*   **Last Week:** The stock has been on an upward trend, driven by optimism following the shareholder meeting where investors approved Elon Musk's $56 billion pay package and the company's move of incorporation to Texas. This was seen as a vote of confidence in Musk's leadership.\n",
      "*   **Year-to-Date (YTD):** Despite recent gains, TSLA is still down significantly in 2024. The stock started the year around $250, and its price is still down approximately **-25%** YTD.\n",
      "*   **One-Year Performance:** Over the past 12 months, the stock has underperformed the broader market (like the S&P 500), showing a decline of roughly **-28%**.\n",
      "\n",
      "### **Key Factors Influencing Recent Performance**\n",
      "\n",
      "Several key factors are driving the volatility and performance of Tesla's stock:\n",
      "\n",
      "1.  **Increased Competition:** Competition in the EV market has intensified, particularly from Chinese automakers like BYD, which has put pressure on Tesla's global market share.\n",
      "2.  **Price Cuts and Margins:** Tesla has implemented several price cuts globally to spur demand, which has concerned investors about declining automotive gross margins.\n",
      "3.  **Delivery Numbers:** Tesla's Q1 2024 delivery numbers missed analyst expectations, marking the first year-over-year decline in deliveries in four years. All eyes are now on the upcoming Q2 delivery report.\n",
      "4.  **Future Growth Narrative:** The stock's valuation is heavily dependent on future growth. Positive news around the **Cybertruck production ramp**, the potential for a **lower-cost \"Model 2\"**, and advancements in **Full Self-Driving (FSD)** and the **Optimus robot** are critical for investor sentiment.\n",
      "5.  **CEO and Shareholder Votes:** The recent overwhelming approval of Elon Musk's compensation package has removed a major overhang of uncertainty about his future focus and leadership at the company.\n",
      "\n",
      "In summary, while Tesla has seen a strong rebound in the last week on positive news, the stock has faced significant headwinds throughout 2024 due to rising competition and concerns about slowing growth.\n",
      "\n",
      "***Disclaimer:** This information is for informational purposes only and does not constitute financial advice. You should consult with a qualified financial professional before making any investment decisions.*\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Compare responses with and without search grounding\n",
    "comparison_prompt = \"What is the current stock price of Tesla (TSLA) and its recent performance?\"\n",
    "\n",
    "# First, make a call without search grounding by not passing tools\n",
    "response_without_search = await llm_manager.call_llm_with_usage(\n",
    "    prompt=comparison_prompt,\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Response WITHOUT Search Grounding:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_without_search[\"content\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:25:01] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> AFC is enabled with max remote calls: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>.                               <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">models.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7118</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:25:01]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m AFC is enabled with max remote calls: \u001b[1;36m10\u001b[0m.                               \u001b]8;id=429424;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py\u001b\\\u001b[2mmodels.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=622662;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/google/genai/models.py#7118\u001b\\\u001b[2m7118\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:25:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                                     <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">:generateContent</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:25:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m                                                     \u001b]8;id=455330;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=543820;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94m:generateContent\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                     \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response WITH Search Grounding:\n",
      "--------------------------------------------------\n",
      "## Tesla Stock Shows Volatility with Recent Dip But Long-Term Gains\n",
      "\n",
      "**As of Friday, July 11, 2025, Tesla (TSLA) closed at approximately $313.51, reflecting a slight increase of 1.17% in the last 24 hours of trading.** This comes amid a period of mixed performance for the electric vehicle giant.\n",
      "\n",
      "While the daily performance shows a modest gain, a broader look reveals a recent downturn. The stock has seen a decline of 1.41% over the past week and a more significant drop of 6.25% over the last month.\n",
      "\n",
      "However, looking at the longer-term picture, Tesla's stock has demonstrated substantial growth. Over the last year, it has surged by 19.07%, and in the last 12 months, the price has risen by 26.40%. This indicates underlying strength and investor confidence in the company's future prospects. The 52-week trading range for the stock has been between a low of $182.00 and a high of $488.54.\n",
      "\n",
      "Tesla's market capitalization currently stands at a robust $1.01 trillion. The company is a key player in the consumer cyclical sector, specializing in auto manufacturing. Beyond its well-known electric vehicles like the Model S, Model 3, Model X, and Model Y, Tesla is also a significant force in the energy generation and storage sector with products like Powerwall, Powerpack, and Megapack.\n",
      "\n",
      "✓ Found 4 citation sources\n"
     ]
    }
   ],
   "source": [
    "# Now make the same call with search grounding\n",
    "response_with_search = await llm_manager.call_llm_with_citations(\n",
    "    prompt=comparison_prompt,\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0.0,\n",
    "    tools=_get_gemini_search_tools(\"gemini-2.5-pro\")\n",
    ")\n",
    "\n",
    "print(\"Response WITH Search Grounding:\")\n",
    "print(\"-\" * 50)\n",
    "print(response_with_search[\"content\"])\n",
    "\n",
    "# Show if citations were found\n",
    "if \"grounding_metadata\" in response_with_search:\n",
    "    grounding_meta = response_with_search[\"grounding_metadata\"]\n",
    "    if hasattr(grounding_meta, 'grounding_chunks') and grounding_meta.grounding_chunks:\n",
    "        print(f\"\\n✓ Found {len(grounding_meta.grounding_chunks)} citation sources\")\n",
    "else:\n",
    "    print(\"\\n✗ No grounding metadata found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OpenAI Deep Research\n",
    "\n",
    "OpenAI Deep Research models (`o3-deep-research` and `o4-mini-deep-research`) are specialized models that can perform in-depth research and analysis. According to the CHANGELOG, these models automatically have access to `web_search_preview` and `code_interpreter` tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Regular vs Deep Research Models\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:47:38] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:47:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=242221;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=662087;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Model response:  {'### Implementation Plan for Real-Time Collaboration in a Code Editor\\n\\n#### 1. Choose a Synchronization Model\\n- **Operational Transformation (OT)**: Suitable for text-based collaboration, widely used in Google Docs.\\n- **CRDTs (Conflict-free Replicated Data Types)**: Better for decentralized systems, handles conflicts naturally.\\n\\n**Decision**: Use OT for simplicity and existing library support.\\n\\n#### 2. Communication Protocol\\n- **WebSocket**: Efficient for real-time, bidirectional communication.\\n- **WebRTC**: More complex, used for peer-to-peer connections.\\n\\n**Decision**: Use WebSocket for server-client architecture.\\n\\n#### 3. Conflict Resolution Strategy\\n- Implement server-side logic to handle conflicts using OT.\\n- Use a central server to maintain the document state and broadcast changes.\\n\\n#### 4. Performance at Scale\\n- Use horizontal scaling with load balancers.\\n- Implement sharding for document storage.\\n- Optimize WebSocket connections with libraries like `gevent` or `asyncio`.\\n\\n#### 5. Example Code\\n\\n```python\\n# Install necessary libraries\\n# pip install autobahn twisted\\n\\nfrom autobahn.twisted.websocket import WebSocketServerProtocol, WebSocketServerFactory\\nfrom twisted.internet import reactor\\nfrom twisted.python import log\\nimport sys\\n\\nlog.startLogging(sys.stdout)\\n\\nclass CodeEditorServerProtocol(WebSocketServerProtocol):\\n    def onConnect(self, request):\\n        print(f\"Client connecting: {request.peer}\")\\n\\n    def onOpen(self):\\n        print(\"WebSocket connection open.\")\\n\\n    def onMessage(self, payload, isBinary):\\n        if not isBinary:\\n            message = payload.decode(\\'utf8\\')\\n            print(f\"Text message received: {message}\")\\n            # Here, apply OT logic to update document state\\n            self.broadcastUpdate(message)\\n\\n    def onClose(self, wasClean, code, reason):\\n        print(f\"WebSocket connection closed: {reason}\")\\n\\n    def broadcastUpdate(self, message):\\n        # Broadcast the updated document state to all connected clients\\n        for client in self.factory.clients:\\n            client.sendMessage(message.encode(\\'utf8\\'))\\n\\nclass CodeEditorServerFactory(WebSocketServerFactory):\\n    def __init__(self, url):\\n        super().__init__(url)\\n        self.clients = []\\n\\n    def buildProtocol(self, addr):\\n        protocol = CodeEditorServerProtocol()\\n        protocol.factory = self\\n        self.clients.append(protocol)\\n        return protocol\\n\\nif __name__ == \\'__main__\\':\\n    factory = CodeEditorServerFactory(\"ws://localhost:9000\")\\n    reactor.listenTCP(9000, factory)\\n    reactor.run()\\n```\\n\\n#### 6. Testing and Deployment\\n- Test with multiple clients to ensure real-time updates.\\n- Deploy using a cloud provider with auto-scaling capabilities.\\n\\n#### 7. Future Enhancements\\n- Consider CRDTs for decentralized collaboration.\\n- Explore WebRTC for peer-to-peer connections if needed.\\n\\nThis plan provides a basic framework for implementing real-time collaboration in a code editor using Python.'}\n",
      "Regular Model (gpt-4o) Response Length: 2947 chars\n",
      "Token Usage: 0\n",
      "Estimated Cost: $0.0000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Compare regular model vs Deep Research model on the same task\n",
    "comparison_task = \"\"\"\n",
    "Create a very short and concise implementation plan for adding real-time collaboration features \n",
    "to a code editor, similar to Google Docs but for code. Consider the most relevant python libraries and write example code and run it:\n",
    "- Operational Transformation vs CRDTs\n",
    "- WebSocket vs WebRTC\n",
    "- Conflict resolution strategies\n",
    "- Performance at scale\n",
    "\"\"\"\n",
    "\n",
    "print(\"Comparing Regular vs Deep Research Models\\n\")\n",
    "\n",
    "# First try with regular gpt-4o\n",
    "regular_model = \"gpt-4o\"\n",
    "try:\n",
    "    regular_response = await llm_manager.call_llm_with_usage(\n",
    "        prompt=comparison_task,\n",
    "        model=regular_model,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    print(f\"Regular Model response: \", {regular_response[\"content\"]})\n",
    "    \n",
    "    print(f\"Regular Model ({regular_model}) Response Length: {len(regular_response['content'])} chars\")\n",
    "    print(f\"Token Usage: {regular_response['usage_metadata'].total_tokens}\")\n",
    "    estimated_cost = calculate_cost(\n",
    "        regular_model,\n",
    "        regular_response['usage_metadata'].prompt_tokens,\n",
    "        regular_response['usage_metadata'].completion_tokens\n",
    "    )\n",
    "    print(f\"Estimated Cost: ${estimated_cost:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with {regular_model}: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:47:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Enabling Deep Research tools for model o4-mini-deep-research        <a href=\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">llm_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">508</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:47:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Enabling Deep Research tools for model o4-mini-deep-research        \u001b]8;id=964967;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py\u001b\\\u001b[2mllm_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=52942;file:///Users/sravanj/project_work/yellhorn-mcp/yellhorn_mcp/llm_manager.py#508\u001b\\\u001b[2m508\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[07/12/25 17:50:43] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/responses</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span>  <a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[07/12/25 17:50:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/responses\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m  \u001b]8;id=449305;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=618910;file:///opt/anaconda3/envs/sravan-yellhorn/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Research Model (o4-mini-deep-research) Response Length: 3340 chars\n",
      "Token Usage: 0\n",
      "Estimated Cost: $0.0000\n",
      "\n",
      "First 500 chars of Deep Research response:\n",
      "- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (...\n"
     ]
    }
   ],
   "source": [
    "# Now try with deep research model\n",
    "deep_model = \"o4-mini-deep-research\"\n",
    "try:\n",
    "    deep_response = await llm_manager.call_llm_with_usage(\n",
    "        prompt=comparison_task,\n",
    "        model=deep_model,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    print(f\"Deep Model response: \", {deep_response[\"content\"]})\n",
    "    \n",
    "    print(f\"Deep Research Model ({deep_model}) Response Length: {len(deep_response['content'])} chars\")\n",
    "    print(f\"Token Usage: {deep_response['usage_metadata'].total_tokens}\")\n",
    "    estimated_cost = calculate_cost(\n",
    "        deep_model,\n",
    "        deep_response['usage_metadata'].prompt_tokens,\n",
    "        deep_response['usage_metadata'].completion_tokens\n",
    "    )\n",
    "    print(f\"Estimated Cost: ${estimated_cost:.4f}\")\n",
    "    \n",
    "    # Show a snippet of the response to see the difference\n",
    "    print(\"\\nFirst 500 chars of Deep Research response:\")\n",
    "    print(deep_response['content'][:500] + \"...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with {deep_model}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **OT vs CRDT:** Modern editors often favor CRDTs (like Ypy/Y-CRDT) for simpler merge semantics and offline edits.  CRDTs ensure *eventual consistency* without explicit coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)), whereas OT requires complex transform functions and a central server.  For example, a toy CRDT string merge (using unique IDs) might look like:\n",
      "  \n",
      "  ```python\n",
      "  doc1 = []\n",
      "  doc2 = []\n",
      "  doc1.append(('1_A', 'A'))   # user1 inserts 'A'\n",
      "  doc2.append(('1_B', 'B'))   # user2 inserts 'B'\n",
      "  merged = sorted(doc1 + doc2, key=lambda x: x[0])\n",
      "  print(''.join(char for _,char in merged))  # AB\n",
      "  ```\n",
      "\n",
      "- **WebSocket vs WebRTC:** Use WebSockets for a server-based broadcast model (e.g. with Python’s `websockets` or FastAPI), as in many CRDT stacks.  (WebRTC/P2P is possible but adds browser–signaling overhead.)  For instance, a simple WebSocket echo server/client pair with `websockets`:\n",
      "  \n",
      "  ```python\n",
      "  import asyncio, websockets\n",
      "\n",
      "  async def handler(ws, path):\n",
      "      async for msg in ws:\n",
      "          await ws.send(f\"Echo: {msg}\")\n",
      "\n",
      "  async def main():\n",
      "      server = await websockets.serve(handler, \"localhost\", 8765)\n",
      "      async with websockets.connect(\"ws://localhost:8765\") as ws:\n",
      "          await ws.send(\"Hello\")\n",
      "          print(await ws.recv())  # Echo: Hello\n",
      "      server.close()\n",
      "      await server.wait_closed()\n",
      "\n",
      "  asyncio.run(main())\n",
      "  ```\n",
      "\n",
      "- **Conflict resolution strategies:** With CRDTs, conflicts are resolved automatically by merge rules (each character or operation has a globally unique ID), avoiding manual conflict logic ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)).  With OT, you must implement transform functions (like inclusion/exclusion transforms) to reorder edits.  For simple data, one might fall back to *last-writer-wins* or version vectors.  In practice, libraries like Ypy handle the merge under the hood, while OT libraries (e.g. `python-ottype`) provide transform APIs.\n",
      "\n",
      "- **Performance at scale:** Use incremental updates and broadcasts rather than full-text snaps. Batch operations or diffs (state-vectors or deltas) for network efficiency.  Scale WebSocket servers with async frameworks (Uvicorn/Gunicorn or `pycrdt-websocket` like Jupyter’s) and use horizontal sharding or pub/sub (Redis) for multi-server sync.  Persist history (e.g. YDoc checkpoints) so new clients can catch up without replaying all ops.  In short, optimize by compressing CRDT updates and distributing load across nodes.  \n",
      "\n",
      "**Sources:** CRDTs guarantee eventual consistency without coordination ([www.codingeasypeasy.com](https://www.codingeasypeasy.com/blog/build-a-real-time-collaborative-editor-with-fastapi-and-crdts-a-comprehensive-guide#:~:text=scenario,are%20applied%20in%20different%20orders)); JupyterLab’s collaborative editing uses a shared Y-CRDT (YDoc) over WebSocket endpoints ([jupyterlab-realtime-collaboration.readthedocs.io](https://jupyterlab-realtime-collaboration.readthedocs.io/en/latest/developer/architecture.html#:~:text=,file%20management%20and%20kernel%20system)).\n"
     ]
    }
   ],
   "source": [
    "print(deep_response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Between Search Grounding and Deep Research\n",
    "\n",
    "1. **Search Grounding (Gemini)**:\n",
    "   - Automatically searches the web for relevant information\n",
    "   - Adds inline citations to responses\n",
    "   - Best for factual queries requiring current information\n",
    "   - No additional cost beyond regular API usage\n",
    "\n",
    "2. **Deep Research Models (OpenAI)**:\n",
    "   - Specialized models with web search and code interpreter tools\n",
    "   - Designed for complex, multi-step research tasks\n",
    "   - Can execute code and analyze results\n",
    "   - Higher cost but more comprehensive analysis\n",
    "   - May require special API access\n",
    "\n",
    "Both features enhance the LLM's ability to provide accurate, up-to-date information, but they serve different use cases and have different cost/performance tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Replace existing model with unified Model Calling Service (OpenRouter or Litellm)\"\n",
    "description = \"\"\"\n",
    "Describe how to replace and consolidate LLM model calls in yellhorn with Gemini, Open AI, etc. with OpenRouter or LiteLLM\n",
    "\"\"\"\n",
    "\n",
    "user_task = f\"Title: {title}, Description: {description}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Curate Context at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_curate_context, mock_github_command\n",
    "from yellhorn_mcp.server import curate_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call create_workplan with our mock context\n",
    "context_result = await (run_curate_context(\n",
    "        user_task=user_task,\n",
    "        repo_path=REPO_PATH,\n",
    "        gemini_client=gemini_client,\n",
    "        openai_client=openai_client,\n",
    "        llm_manager=llm_manager,\n",
    "        model=MODEL,\n",
    "        codebase_reasoning=\"full\",  # Use \"none\" for faster processing\n",
    "        log_callback=log_callback,\n",
    "        github_command_func=mock_github_command\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Path to yellhorn context\")\n",
    "print(context_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .yellhorncontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the path from the context_result string\n",
    "import re\n",
    "\n",
    "# Extract the file path using regex\n",
    "match = re.search(r'at\\s+(.+?)\\s+with', context_result)\n",
    "if match:\n",
    "    context_file_path = match.group(1)\n",
    "    print(f\"Extracted path: {context_file_path}\")\n",
    "else:\n",
    "    # Fallback: try to find any path-like string\n",
    "    match = re.search(r'(/[^\\s]+\\.yellhorncontext)', context_result)\n",
    "    if match:\n",
    "        context_file_path = match.group(1)\n",
    "        print(f\"Extracted path: {context_file_path}\")\n",
    "    else:\n",
    "        print(\"Could not extract path from result string\")\n",
    "        context_file_path = None\n",
    "\n",
    "# Now you can use context_file_path to read the file\n",
    "if context_file_path:\n",
    "    with open(context_file_path, 'r') as f:\n",
    "        context = f.read()\n",
    "    print(\"Yellhorn context:\")\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Create Workplan\n",
    "\n",
    "Now let's demonstrate how to use the create_workplan MCP tool to generate implementation plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_create_workplan, mock_github_command\n",
    "from yellhorn_mcp.server import process_workplan_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call process_workplan_async with our mock context\n",
    "workplan_result = await (run_create_workplan(\n",
    "        title=title,\n",
    "        detailed_description=description,\n",
    "        repo_path=REPO_PATH,\n",
    "        gemini_client=gemini_client,\n",
    "        openai_client=openai_client,\n",
    "        llm_manager=llm_manager,\n",
    "        model=MODEL,\n",
    "        codebase_reasoning=\"full\",\n",
    "        log_callback=log_callback,\n",
    "        github_command_func=mock_github_command,\n",
    "        background_task_timeout=180\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Workplan Created:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Issue URL: {workplan_result['issue_url']}\")\n",
    "print(f\"Issue Number: {workplan_result['issue_number']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Judge Workplan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.mock_context import run_judge_workplan, mock_github_command\n",
    "from yellhorn_mcp.server import process_judgement_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workplan content (you would typically get this from a GitHub issue)\n",
    "WORKPLAN_CONTENT = \"\"\"\n",
    "# Example Workplan: Add Token Counter Feature\n",
    "\n",
    "## Summary\n",
    "Implement a token counting system using tiktoken to prevent token overflow in LLM calls.\n",
    "\n",
    "## Implementation Steps\n",
    "1. Create TokenCounter class with tiktoken integration\n",
    "2. Add token counting to LLMManager\n",
    "3. Implement automatic prompt chunking when limits are exceeded\n",
    "4. Update all LLM call sites to use the new system\n",
    "\n",
    "## Files to Modify\n",
    "- `yellhorn_mcp/llm_manager.py`: Add token counting integration\n",
    "- `yellhorn_mcp/server.py`: Update LLM call sites\n",
    "\n",
    "## New Files to Create\n",
    "- `yellhorn_mcp/token_counter.py`: Core token counting functionality\n",
    "\"\"\"\n",
    "\n",
    "# Example diff content (you would typically get this from git diff)\n",
    "DIFF_CONTENT = \"\"\"\n",
    "diff --git a/yellhorn_mcp/token_counter.py b/yellhorn_mcp/token_counter.py\n",
    "new file mode 100644\n",
    "index 0000000..1234567\n",
    "--- /dev/null\n",
    "+++ b/yellhorn_mcp/token_counter.py\n",
    "@@ -0,0 +1,50 @@\n",
    "+# Token counting utilities using tiktoken.\n",
    "+\n",
    "+import tiktoken\n",
    "+from typing import Optional\n",
    "+\n",
    "+class TokenCounter:\n",
    "+    \\\"\\\"\\\"Token counter using tiktoken for accurate token counting.\\\"\\\"\\\"\n",
    "+    \n",
    "+    def __init__(self, model: str):\n",
    "+        self.model = model\n",
    "+        self.encoding = tiktoken.encoding_for_model(model)\n",
    "+    \n",
    "+    def count_tokens(self, text: str) -> int:\n",
    "+        \\\"\\\"\\\"Count tokens in the given text.\\\"\\\"\\\"\n",
    "+        return len(self.encoding.encode(text))\n",
    "\n",
    "diff --git a/yellhorn_mcp/llm_manager.py b/yellhorn_mcp/llm_manager.py\n",
    "index abcd123..efgh456 100644\n",
    "--- a/yellhorn_mcp/llm_manager.py\n",
    "+++ b/yellhorn_mcp/llm_manager.py\n",
    "@@ -10,6 +10,7 @@ from typing import Dict, List, Optional, Any, Union\n",
    " from openai import AsyncOpenAI\n",
    " import google.genai as genai\n",
    " from .usage_metadata import UsageMetadata\n",
    "+from .token_counter import TokenCounter\n",
    " \n",
    " class LLMManager:\n",
    "     \\\"\\\"\\\"Unified LLM manager with token counting and chunking support.\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# Judge workplan parameters\n",
    "base_ref = \"main\"\n",
    "head_ref = \"feature/token-counter\"\n",
    "subissue_to_update = \"456\"  # GitHub issue number for the sub-issue to update\n",
    "parent_workplan_issue_number = \"123\"  # Original workplan issue number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call judge_workplan with our mock context\n",
    "await run_judge_workplan(\n",
    "    workplan_content=WORKPLAN_CONTENT,\n",
    "    diff_content=DIFF_CONTENT,\n",
    "    base_ref=base_ref,\n",
    "    head_ref=head_ref,\n",
    "    subissue_to_update=subissue_to_update,\n",
    "    parent_workplan_issue_number=parent_workplan_issue_number,\n",
    "    repo_path=REPO_PATH,\n",
    "    gemini_client=gemini_client,\n",
    "    openai_client=openai_client,\n",
    "    llm_manager=llm_manager,\n",
    "    model=MODEL,\n",
    "    base_commit_hash=\"abc123\",  # Optional: actual commit hash\n",
    "    head_commit_hash=\"def456\",  # Optional: actual commit hash\n",
    "    debug=False,  # Set to True to see the full prompt used\n",
    "    codebase_reasoning=\"full\",  # Options: \"full\", \"lsp\", \"file_structure\", \"none\"\n",
    "    disable_search_grounding=False,\n",
    "    github_command_func=mock_github_command,\n",
    "    log_callback=log_callback,\n",
    "    wait_for_background_tasks=True,\n",
    "    background_task_timeout=180\n",
    ")\n",
    "\n",
    "print(\"\\nJudgement completed successfully!\")\n",
    "print(f\"Check GitHub sub-issue #{subissue_to_update} for the detailed judgement results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sravan-yellhorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
