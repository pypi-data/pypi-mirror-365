# Copyright 2022 Huawei Technologies Co., Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""Check Model Input Config."""
import json
import re
import os
from functools import wraps
from typing import Union
import numpy as np
import mindspore.common.dtype as mstype
import mindspore as ms
from mindspore.context import ParallelMode
from mindspore.parallel._utils import _get_parallel_mode, _is_sharding_propagation

from ..tools.utils import get_predict_run_mode, is_pynative
from ..version_control import get_lazy_inline, get_predict_lazy_inline
from ..tools.logger import logger

# pylint: disable=W0212
CONFIG_NAME = "config.json"
WEIGHTS_NAME = "mindspore_model.ckpt"
WEIGHTS_INDEX_NAME = "mindspore_model.ckpt.index.json"
FEATURE_EXTRACTOR_NAME = "preprocessor_config.json"
PROCESSOR_NAME = "processor_config.json"
IMAGE_PROCESSOR_NAME = FEATURE_EXTRACTOR_NAME
MAX_INT32 = 2147483647

str_to_ms_type = {
    "float16": mstype.float16,
    "float32": mstype.float32,
    "bfloat16": mstype.bfloat16,
    "int8": mstype.int8
}


def convert_mstype(ms_type: str = "float16"):
    """Convert the string type to MindSpore type."""
    if isinstance(ms_type, mstype.Float):
        return ms_type
    ms_type = str(ms_type).lower()
    if ms_type == "float16":
        return mstype.float16
    if ms_type == "float32":
        return mstype.float32
    if ms_type == "bfloat16":
        return mstype.bfloat16
    if ms_type == "int8":
        return mstype.int8
    raise KeyError(f"Supported data type keywords include: "
                   f"[float16, float32, bfloat16, int8], but get {ms_type}")


def reverse_dict(d: dict):
    new_d = {}
    for k, v in d.items():
        if v in new_d:
            raise ValueError(f"Different keys in dict have same values.")
        new_d[v] = k
    return new_d


def is_json_serializable(obj):
    try:
        json.dumps(obj)
        return True
    except TypeError:
        return False


def check_fine_grain_interleave_valid(fine_grain_interleave, parallel_config):
    """Check the fine grain interleave condition"""
    if fine_grain_interleave is None or parallel_config is None:
        return False
    return fine_grain_interleave > 1 and parallel_config.model_parallel > 1


def check_use_3d_tensor_parallel_valid(config):
    """Check the use_3d_tensor_parallel condition"""
    use_3d_tensor_parallel = getattr(config, "use_3d_tensor_parallel", False)
    is_config_valid = config is not None and config.parallel_config is not None
    if not use_3d_tensor_parallel or not is_config_valid:
        return False
    if not config.use_flash_attention:
        raise ValueError(f"When the use_3d_tensor_parallel = True, the use_flash_attention must be True ")
    if config.parallel_config.get_ulysses_cp_num() > 1:
        raise ValueError(f"Currently, when the use_3d_tensor_parallel = True, "
                         "the cp_ds of the ulysses context parallel must be 1")
    if _get_parallel_mode() in (ParallelMode.AUTO_PARALLEL,) and _is_sharding_propagation():
        raise ValueError(f"Currently, when the use_3d_tensor_parallel = True, the auto parallel is not supported")
    if config.moe_config is not None and config.moe_config.expert_num > 1:
        raise ValueError(f"Currently, when the use_3d_tensor_parallel = True, the MoE is not supported")
    if not config.parallel_config.use_seq_parallel:
        raise ValueError(f"Currently, when the use_3d_tensor_parallel = True, the use_seq_parallel must be True")
    if check_fine_grain_interleave_valid(config.fine_grain_interleave, config.parallel_config):
        raise ValueError("Currently, when the use_3d_tensor_parallel = True, "
                         "the fine_grain_interleave is not supported")
    tp_x = getattr(config, "tp_x", 1)
    tp_y = getattr(config, "tp_y", 1)
    tp_z = getattr(config, "tp_z", 1)
    model_parallel = config.parallel_config.model_parallel
    if model_parallel > 1 and tp_x * tp_y * tp_z != config.parallel_config.model_parallel:
        raise ValueError("tp_x * tp_y * tp_z should be equal to model_parallel, but got "
                         "tp_x={}, tp_y={}, tp_z={}, model_parallel={}.".format(tp_x, tp_y, tp_z, model_parallel))
    if model_parallel > 1:
        logger.info(f"use_3d_tensor_parallel is True, (tp_x, tp_y, tp_z): ({tp_x}, {tp_y}, {tp_z})")
        return True
    return False


def check_swap_enabled(swap_config):
    if isinstance(swap_config, dict):
        return swap_config["swap"]
    return swap_config.swap


def jit(func):
    """jit decorator."""

    @wraps(func)
    def decorator(*args, **kwargs):
        if not get_predict_run_mode():
            raise ValueError("Jit is only supported in predict mode now.")
        if is_pynative():
            return func(*args, **kwargs)
        return ms.jit(func, jit_level='O0', infer_boost='on')(*args, **kwargs)

    return decorator


def dict_from_json_file(json_file: Union[str, os.PathLike]):
    """method to read json."""
    if not os.path.exists(json_file):
        raise ValueError(
            f"{json_file} does not exist. Please check files in given path."
        )
    json_file = os.path.realpath(json_file)
    with open(json_file, "r", encoding="utf-8") as reader:
        text = reader.read()
    return json.loads(text)

ms_type_to_str = reverse_dict(str_to_ms_type)

lazy_inline = get_lazy_inline
predict_lazy_inline = get_predict_lazy_inline


class LayerSetting:
    r"""
    Class for setting offset, pipeline stage, swap and select recompute for each transformer layer.
    Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.

        Args:
            num_layers(int) - The total layers in the model.
            offset(Union[int, List[int], List[List[int]]]) - The layer offset for each (mini) stage.
            parallel_config(dict) - Parallel Config.
            pp_interleave_num(int) - The number of pp-interleave. When it is larger than 1,
                each stage will be divided into mini stages by pp_interleave_num.
    """

    def __init__(self, num_layers, offset, parallel_config, pp_interleave_num=1, start_stage=0, stage_num=0):
        default_patterns = [r'feed_forward\.mul', r'feed_forward\.w1\.activation\.silu']
        default_comm_patterns = [r'.*\.norm']
        default_off_patterns = []
        default_comm_off_patterns = []
        try:
            use_pp_interleave = ms.get_auto_parallel_context("pipeline_interleave")
        except ValueError:
            logger.warning(f"Current MindSpore version do not pipeline interleave. `pp_interleave_num` is set to 1.")
            use_pp_interleave = False
        self.num_layers = num_layers
        if stage_num != 0:
            self.pp = stage_num
            self.start_stage = start_stage
        else:
            self.start_stage = 0
            self.pp = parallel_config.pipeline_stage
        self.recompute = parallel_config.recompute
        self.swap = parallel_config.swap
        self.backward_prefetch = 'backward_prefetch'
        self.layers = 'layers'
        self.gradient_aggregation_group = parallel_config.gradient_aggregation_group
        self.pp_interleave_num = pp_interleave_num if use_pp_interleave else 1
        self.offset = np.array(offset, np.int32)
        self._check_inputs()
        self.offset = np.broadcast_to(self.offset, (self.pp_interleave_num, self.pp))

        avg_layer = self.num_layers // (self.pp * self.pp_interleave_num)
        self.layer_list = np.ones((self.pp_interleave_num, self.pp), np.int32) * avg_layer + self.offset
        interleave_sum = np.insert(np.cumsum(np.sum(self.layer_list, axis=1))[:-1], 0, 0)
        self.layer_accu = np.cumsum(self.layer_list, axis=1) + interleave_sum.reshape(-1, 1)
        self.pp_ids = [np.searchsorted(self.layer_accu.reshape(-1), i + 1) % self.pp for i in range(self.num_layers)]
        self.interleave_ids = [np.where(i < self.layer_accu)[0][0] for i in range(self.num_layers)]
        logger.info(f"num_layers per stage: {self.layer_list.tolist()}")
        logger.info(f"Accumulated num_layers per stage: {self.layer_accu.tolist()}")
        logged_pp_ids = [(pp_id + self.start_stage) for pp_id in self.pp_ids]
        logger.info(f"Pipeline id list with start_stage: {logged_pp_ids}")
        logger.info(f"Interleave id list: {self.interleave_ids}")
        pre_pad = np.array([[0] + [self.layer_accu[i, -1] for i in range(len(self.layer_accu) - 1)]])
        self.layer_accu_mod = np.concatenate((pre_pad.T, self.layer_accu), axis=-1)

        if not isinstance(self.recompute, bool):
            if self.swap.swap:
                self.layer_recompute = self._format_recompute_list_select_layer_index(self.recompute.recompute)
            else:
                self.layer_recompute = self._format_recompute_list(
                    self.recompute.recompute)
            self.select_recompute = self._format_recompute_dict(
                self.recompute.select_recompute, default_patterns, self.swap.swap)
            self.select_comm_recompute = self._format_recompute_dict(
                self.recompute.select_comm_recompute, default_comm_patterns, self.swap.swap)
            self.select_recompute_exclude = self._format_recompute_dict(
                self.recompute.select_recompute_exclude, default_off_patterns)
            self.select_comm_recompute_exclude = self._format_recompute_dict(
                self.recompute.select_comm_recompute_exclude, default_comm_off_patterns)
            logger.info(f"Formative layer_recompute: {self.layer_recompute}")

            logger.info("The configuration of select_recompute_exclude and select_comm_recompute_exclude "
                        "have the highest priority.")
            # check repeat config and remove
            self._check_repeat_recompute(self.select_recompute, self.select_recompute_exclude, is_comm=False)
            self._check_repeat_recompute(self.select_comm_recompute, self.select_comm_recompute_exclude, is_comm=True)

            logger.info(f"Formative select_recompute: {self.select_recompute}")
            logger.info(f"Formative select_comm_recompute: {self.select_comm_recompute}")
            logger.info(f"Formative select_recompute_exclude: {self.select_recompute_exclude}")
            logger.info(f"Formative select_comm_recompute_exclude: {self.select_comm_recompute_exclude}")

        if self.swap.swap:
            self.layer_swap = []
            self.op_swap = dict()
            self.layer_swap = self._initialize_swap_list(self.swap.layer_swap)
            for key in self.swap.op_swap:
                self.op_swap[key] = self._initialize_swap_list(self.swap.op_swap[key])
            logger.info(f"Formative layer swap: {self.layer_swap}")
            logger.info(f"Formative op swap: {self.op_swap}")
            self._check_swap_recompute_conflict()

    @staticmethod
    def _check_repeat_pattern(key, select_recompute):
        """check and return the repeat pattern"""
        repeat_key = None
        key = key.split(r'\.')
        for pattern in select_recompute:
            split_pattern = pattern.split(r'\.')
            if len(key) != len(split_pattern):
                continue
            for p1, p2 in zip(key, split_pattern):
                if not re.fullmatch(p1, p2) and not re.fullmatch(p2, p1):
                    break
            else:
                repeat_key = pattern
                break
        return repeat_key

    def _check_repeat_recompute(self, select_recompute, select_recompute_exclude, is_comm=False):
        "Check if the select_recompute conflicts select_recompute_exclude."
        comm = '_comm' if is_comm else ''
        for key in select_recompute_exclude:
            repeat_key = self._check_repeat_pattern(key, select_recompute)
            if repeat_key:
                select_recompute.pop(repeat_key)
                logger.info(f"The pattern {repeat_key} in select{comm}_recompute conflicts with "
                            f"select{comm}_recompute_exclude and will be removed.")

    def _check_swap_recompute_conflict(self):
        "Check if the layer or operator is enable swap and recompute at the same time."
        if isinstance(self.recompute.recompute, bool) and self.recompute.recompute:
            logger.warning(f"All layers and ops that are enabled swap do not work!\
                           Because recompute = {self.recompute.recompute}.")
        if isinstance(self.recompute.recompute, list) and self.recompute.recompute:
            for layer_idx in self.recompute.recompute:
                self._check_layer_swap_recompute_conflict(layer_idx, self.layer_swap)
            for op_name, _ in self.op_swap.items():
                self._check_op_swap_recompute_conflict(op_name, self.recompute.recompute)

        if isinstance(self.recompute.select_recompute, dict):
            for op_name, recompute_layers in self.recompute.select_recompute.items(): # op_name->str, recompute_info->[]
                if isinstance(recompute_layers, bool) and recompute_layers:
                    logger.warning(f"{op_name} operator in all layers that is enabled swap do not work!\
                                   Because it is enabled recompute.")
                    continue
                for layer_idx in recompute_layers:
                    self._check_layer_swap_recompute_conflict(layer_idx, self.layer_swap, op_name)
                if op_name in self.op_swap.keys():
                    self._check_op_swap_recompute_conflict(op_name, recompute_layers)

    def _check_layer_swap_recompute_conflict(self, layer_idx, layer_list, op_name="All"):
        "Check if the layer is enable swap and recompute at the same time."
        for swap_layer in layer_list:
            if isinstance(swap_layer.get(self.layers), bool) and swap_layer.get(self.layers):
                logger.warning(f"{op_name} operator in layer {layer_idx} that\
                               is enabled swap do not work! Because it is enabled recompute.")
            else:
                if layer_idx in swap_layer.get(self.layers):
                    logger.warning(f"{op_name} operator in layer {layer_idx} that\
                                   is enabled swap do not work! Because it is enabled recompute.")

    def _check_op_swap_recompute_conflict(self, op_name, layer_list):
        "Check if the operator is enable swap and recompute at the same time."
        if isinstance(layer_list, bool) and layer_list:
            logger.warning(f"{op_name} operator that is enabled swap do not work! Because it is enabled recompute.")
        else:
            op_swap_layer_list = self.op_swap[op_name]
            op_swap_layers = set()
            for swap_info in op_swap_layer_list:
                if isinstance(swap_info.get(self.layers), bool) and swap_info.get(self.layers):
                    op_swap_layers.clear()
                    op_swap_layers = True
                    break
                op_swap_layers = op_swap_layers.union(set(swap_info.get(self.layers)))
            for layer_idx in layer_list:
                if isinstance(op_swap_layers, bool) or layer_idx in op_swap_layers:
                    logger.warning(f"{op_name} operator in layer {layer_idx} that\
                                   is enabled swap do not work! Because it is enabled recompute.")

    def _initialize_swap_list(self, swap_list):
        """Initialize the swap list by creating swap configurations for each item."""
        if self.swap.swap and not swap_list:
            return []
        result = []
        for item in swap_list:
            layers = self._format_recompute_list_select_layer_index(item.get(self.layers))
            swap_config = self._create_swap_dict(
                item.get(self.backward_prefetch),
                layers
            )
            result.append(swap_config)
        return result

    @staticmethod
    def _create_swap_dict(backward_prefetch, layers):
        """Create a dictionary for swap configuration with backward_prefetch and layers."""
        return {'backward_prefetch': backward_prefetch, 'layers': layers}

    def set_swap(self, layer, layer_id):
        """Set swap for a specific layer based on its layer_id."""
        if self.swap.swap:
            if self.layer_swap and isinstance(self.layer_swap[0].get(self.layers), bool):
                if self.layer_swap[0].get(self.layers):
                    layer.offload(backward_prefetch=self.layer_swap[0].get(self.backward_prefetch))
                    logger.info(f"Set layer swap at layer {layer_id} \
                                and value is: {self.layer_swap[0].get(self.backward_prefetch)}")
            else:
                self._set_layer_swap(layer, layer_id)
            self._set_op_swap(layer, layer_id)

    def _set_op_swap(self, layer, layer_id):
        """Set swap for operations in the layer based on patterns and layer_id."""
        log_ops = []
        for pattern in self.op_swap:
            for layer_swap in self.op_swap[pattern]:
                layers_id = layer_swap.get(self.layers)
                is_valid_bool = isinstance(layers_id, bool) and layers_id
                is_valid_list = isinstance(layers_id, list) and layer_id in layers_id
                if is_valid_bool or is_valid_list:
                    log = LayerSetting.set_pattern_swap(layer, pattern.split(r'\.'),
                                                        layer_swap.get(self.backward_prefetch))
                    if log:
                        log_ops.append(log)
                    break
        if log_ops:
            logger.info(f"Set op_swap at layer {layer_id}: {', '.join(log_ops)}")

    def _set_layer_swap(self, layer, layer_id):
        """Set swap for the entire layer based on the layer_id."""
        for layer_swap in self.layer_swap:
            if layer_id in layer_swap.get(self.layers):
                layer.offload(backward_prefetch=layer_swap.get(self.backward_prefetch))
                logger.info(f"Set layer swap at layer {layer_id} \
                            and value is: {layer_swap.get(self.backward_prefetch)}")
                break

    @staticmethod
    def set_pattern_swap(layer, p_list, value, info=''):
        """Set swap for operators in the layer based on a pattern list and value."""
        log_list = []
        log = ''
        if p_list:
            p = p_list.pop(0)
        else:
            return info
        if p_list:
            # pylint: disable=W0212
            for name, cell in layer._cells.items():
                if re.fullmatch(p, name):
                    log = LayerSetting.set_pattern_swap(cell, p_list, value, info + f'.{name}')
                    if log:
                        log_list.append(log[1:])
        else:
            for attr in dir(layer):
                if re.fullmatch(p, attr):
                    operator = getattr(layer, attr)
                    if hasattr(operator, '_offload'):
                        operator._offload(backward_prefetch=value)
                        log = f"{info}.{attr}, value={value}"
            # pylint: disable=W0212
            for name, cell in layer._cells.items():
                if re.fullmatch(p, name):
                    cell.offload(backward_prefetch=value)
                    log = f"{info}.{name}, value={value}"
        p_list.insert(0, p)
        if log_list:
            return " " + ", ".join(log_list)
        return log

    def set(self, layer, layer_id):
        """Set pipeline stage and recompute for each layer with a layer_id."""
        pp_id = int(self.pp_ids[layer_id]) + self.start_stage
        layer.pipeline_stage = pp_id
        dis = max(int((self.num_layers + 1) / self.gradient_aggregation_group), 1)
        if self.pp > 1:
            layer.set_comm_fusion(2)
        else:
            layer.set_comm_fusion(int((layer_id + self.offset[0, 0]) / dis) + 1)

        if isinstance(self.recompute, bool):
            if self.recompute:
                layer.recompute()
            return

        is_full_recompute = False
        if self.recompute.recompute:
            if isinstance(self.recompute.recompute, bool):
                layer.recompute(recompute_slice_activation=self.recompute.recompute_slice_activation)
                is_full_recompute = True
                logger.info(f"Set full recompute at layer {layer_id}")
            elif self._check_layer_rule(layer_id):
                layer.recompute()
                is_full_recompute = True
                logger.info(f"Set full recompute at layer {layer_id}")
        if not is_full_recompute:
            self._set_select_recompute(layer, layer_id, False, set_on=True)
            self._set_select_recompute(layer, layer_id, True, set_on=True)

        # select recompute exclude
        self._set_select_recompute(layer, layer_id, False, set_on=False)
        self._set_select_recompute(layer, layer_id, True, set_on=False)


    def set_recompute_select_layer_index(self, layer, layer_id):
        """Set swap for specific layer based on its layer_id when."""
        if isinstance(self.recompute, bool):
            if self.recompute:
                layer.recompute()
            return
        if self.recompute.recompute:
            if isinstance(self.recompute.recompute, bool):
                if self.recompute.recompute:
                    layer.recompute(recompute_slice_activation=self.recompute.recompute_slice_activation)
            else:
                if layer_id in self.layer_recompute:
                    layer.recompute()
                    logger.info(f"Set full recompute at layer {layer_id}")
                else:
                    self._set_select_recompute_layer_index(layer, layer_id, False)
                    self._set_select_recompute_layer_index(layer, layer_id, True)
        else:
            self._set_select_recompute_layer_index(layer, layer_id, False)
            self._set_select_recompute_layer_index(layer, layer_id, True)

    def _alloc_recompute_layer(self, select_recompute):
        """Average allocate recompute layer among different interleave."""
        res = np.zeros_like(self.layer_list)
        layer_by_stage = np.sum(self.layer_list, axis=0)
        for j, num in enumerate(select_recompute):
            num = min(num, layer_by_stage[j])
            while num:
                for i in range(self.pp_interleave_num):
                    if not num:
                        break
                    if res[i, j] < self.layer_list[i, j]:
                        res[i, j] += 1
                        num -= 1
        return res

    def _format_recompute_list(self, select_recompute):
        """Format recompute inputs into a list."""
        if isinstance(select_recompute, bool):
            if select_recompute:
                return self.layer_list.tolist()
            return np.zeros_like(self.layer_list).tolist()
        if isinstance(select_recompute, (list, tuple)):
            if all(isinstance(item, (int, bool)) for item in select_recompute):
                for i, item in enumerate(select_recompute):
                    if isinstance(item, bool):
                        select_recompute[i] = MAX_INT32 if item else 0
                return self._alloc_recompute_layer(select_recompute).tolist()
            if all(isinstance(item, str) for item in select_recompute):
                return self.layer_list.tolist()
            for i, sub in enumerate(select_recompute):
                if isinstance(sub, bool):
                    select_recompute[i] = self.layer_list[i] if sub else np.zeros_like(self.layer_list[i])
                elif isinstance(sub, (list, tuple)):
                    for j, item in enumerate(sub):
                        if isinstance(item, bool):
                            select_recompute[i][j] = self.layer_list[i][j] if item else 0
                        elif isinstance(item, int):
                            select_recompute[i][j] = min(select_recompute[i][j], self.layer_list[i, j])
                        else:
                            raise ValueError(f"Illegal input list for select_recompute: {select_recompute}")
                else:
                    raise ValueError(f"Illegal input list for select_recompute: {select_recompute}")
            return select_recompute
        raise ValueError(f"Illegal input list for select_recompute: {select_recompute}")

    @staticmethod
    def _format_recompute_list_select_layer_index(select_recompute):
        """Format recompute inputs into a list when using swap."""
        if isinstance(select_recompute, bool):
            if select_recompute:
                return select_recompute
            return []
        if isinstance(select_recompute, list):
            if all(isinstance(item, int) for item in select_recompute):
                return select_recompute
        raise ValueError(f"Illegal input list for select_recompute: {select_recompute}")

    def _format_recompute_dict(self, select_recompute, default_patterns, select_layer_index=False):
        """Format select_recompute inputs into a dict"""
        dic = {}
        if isinstance(select_recompute, (list, tuple)) and all(isinstance(item, str) for item in select_recompute):
            parttern = select_recompute
        elif isinstance(select_recompute, dict):
            parttern = select_recompute.keys()
        else:
            parttern = default_patterns
        for p in parttern:
            value = select_recompute[p] if isinstance(select_recompute, dict) else select_recompute
            if select_layer_index:
                dic[p] = self._format_recompute_list_select_layer_index(value)
            else:
                dic[p] = self._format_recompute_list(value)
        return dic

    def _check_layer_rule(self, layer_id):
        """Check whether a layer should be set full layer recompute."""
        pp_id = int(self.pp_ids[layer_id])
        v_id = int(self.interleave_ids[layer_id])
        if 0 <= layer_id - self.layer_accu_mod[v_id, pp_id] < self.layer_recompute[v_id][pp_id]:
            return True
        return False

    def _set_select_recompute(self, layer, layer_id, add_prim_attr=False, set_on=True):
        """Set select recompute on/off for a layer."""
        if set_on:
            select_recompute = self.select_comm_recompute if add_prim_attr else self.select_recompute
            action = "on"
        else:
            select_recompute = self.select_comm_recompute_exclude if add_prim_attr else self.select_recompute_exclude
            action = "exclude"
        pp_id = int(self.pp_ids[layer_id])
        v_id = int(self.interleave_ids[layer_id])
        log_ops = []
        for pattern, layers_dict in select_recompute.items():
            if 0 <= layer_id - self.layer_accu_mod[v_id, pp_id] < layers_dict[v_id][pp_id]:
                log = LayerSetting.set_pattern_recompute(layer, pattern.split(r'\.'), add_prim_attr, set_on)
                if log:
                    log_ops.append(log[1:])
        log_ops_str = ', '.join(log_ops)
        if log_ops_str:
            comm = 'comm ' if add_prim_attr else ''
            logger.info(f"Set select {comm}recompute {action} at layer {layer_id}: {log_ops_str}")

    def _set_select_recompute_layer_index(self, layer, layer_id, add_prim_attr=False):
        """Set select recompute for a layer when using swap."""
        select_recompute = self.select_comm_recompute if add_prim_attr else self.select_recompute
        log_ops = []
        for pattern, layers_recompute in select_recompute.items():
            if (isinstance(layers_recompute, bool) and layers_recompute) or layer_id in layers_recompute:
                log = LayerSetting.set_pattern_recompute(layer, pattern.split(r'\.'), add_prim_attr)
                if log:
                    log_ops.append(log[1:])
        log_ops_str = ', '.join(log_ops)
        if log_ops_str:
            comm = 'comm ' if add_prim_attr else ''
            logger.info(f"Set select {comm}recompute at layer {layer_id}: {log_ops_str}")

    def _check_inputs(self):
        """Check the inputs of offset."""
        if self.offset.ndim >= 1 and self.offset.shape[-1] != self.pp:
            raise ValueError(f"offset.shape[-1] should equal to `pp` ({self.pp}), "
                             f"but got ({self.offset.shape[-1]}). `offset`: {self.offset}")
        if self.offset.ndim >= 2 and self.offset.shape[-2] != self.pp_interleave_num:
            raise ValueError(f"offset.shape[-2] should equal to `pp_interleave_num` ({self.pp_interleave_num}), "
                             f"but got ({self.offset.shape[-2]}). `offset`: {self.offset}")
        if self.offset.sum() != self.num_layers % (self.pp * self.pp_interleave_num):
            r = self.num_layers % (self.pp * self.pp_interleave_num)
            raise ValueError(f"The sum of `offset` ({self.offset.sum()}) should equal to remainder of `num_layers` "
                             f"({self.num_layers}) % (pp ({self.pp}) * pp_interleave_num ({self.pp_interleave_num})) "
                             f"= {r}")

    @staticmethod
    def set_pattern_recompute(layer, p_list, add_prim_attr=False, set_on=True, info=''):
        """Set an operator recompute status on/off for a given key-value pair in select_recompute dict."""
        log_list = []
        log = ''
        if p_list:
            p = p_list.pop(0)
        else:
            return info
        if p_list:
            # pylint: disable=W0212
            for name, cell in layer._cells.items():
                if re.fullmatch(p, name):
                    log = LayerSetting.set_pattern_recompute(cell, p_list, add_prim_attr, set_on, info + f'.{name}')
                    if log:
                        log_list.append(log[1:])
        else:
            for attr in dir(layer):
                if re.fullmatch(p, attr):
                    operator = getattr(layer, attr)
                    if add_prim_attr:
                        operator.add_prim_attr("recompute_comm_op", set_on)
                        log = f"{info}.{attr}"
                    elif hasattr(operator, 'recompute'):
                        operator.recompute(set_on)
                        log = f"{info}.{attr}"
            # pylint: disable=W0212
            for name, cell in layer._cells.items():
                if re.fullmatch(p, name):
                    if not set_on:
                        logger.info(f"For select recompute/comm_recompute exclude, {info.replace('.', '', 1)}.{name} "
                                    "is expected to be operation but got cell, "
                                    "this configuration will not be effective.")
                        continue
                    if add_prim_attr:
                        logger.info(f"For communication recompute, {info.replace('.', '', 1)}.{name} "
                                    "is expected to be operation but got cell, "
                                    "this configuration will not be effective.")
                        continue
                    cell.recompute()
                    log = f"{info}.{name}"
        p_list.insert(0, p)
        if log_list:
            return " " + ", ".join(log_list)
        return log

    def __call__(self, layer, layer_id):
        if self.swap.swap:
            self.set_recompute_select_layer_index(layer, layer_id)
            self.set_swap(layer, layer_id)
        else:
            self.set(layer, layer_id)
