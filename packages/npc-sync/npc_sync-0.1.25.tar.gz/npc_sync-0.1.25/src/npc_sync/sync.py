"""
Dataset object for loading and unpacking an HDF5 dataset generated by
    sync.py

@author: derricw

Allen Institute for Brain Science
"""

from __future__ import annotations

import datetime
import io
import logging
import os
from collections.abc import Iterable, Sequence
from typing import TYPE_CHECKING, Any, Literal, Union

import h5py
import npc_io
import npc_session
import numpy as np
import numpy.typing as npt
import scipy.signal
import upath
from typing_extensions import Self, TypeAlias

if TYPE_CHECKING:
    import matplotlib.axes
    import matplotlib.figure


import npc_stim

logger = logging.getLogger(__name__)

SyncPathOrDataset: TypeAlias = Union[npc_io.PathLike, h5py.File, "SyncDataset"]

FIRST_SOUND_ON_SYNC_DATE = datetime.date(2023, 8, 31)
"""Prior to this date, there's no sync line with "sound running" signal, on any rig: need to
use NI-DAQ analog recording on OpenEphys PXI to get sound onset times."""

FIRST_GALVO_ON_SYNC_DATE = datetime.date(2024, 4, 19)
"""Prior to this date, opto experiments using the galvo system did not have a "galvo motor
running" signal, on any rig. A copy of the analog signal may have been recorded on the OpenEphys DAQ."""

MONITOR_CENTER_REFRESH_TIME = 0.008
"""Best estimate of time from photodiode flip (at top of screen) to updated
content at the center of the screen. Screen refreshes in stages from top to bottom,
in 16 ms: measured by Corbett."""

AVERAGE_VSYNC_TO_DIODE_FLIP_TIME = 0.01955  #! sometimes shifts to ~0.035

CONSTANT_MONITOR_LAG: float = (
    MONITOR_CENTER_REFRESH_TIME + AVERAGE_VSYNC_TO_DIODE_FLIP_TIME
)
"""Best estimate of the average time between vsync falling edge and the center of
the screen updating, in seconds. For use when a reliable photodiode signal
is unavailable."""

MIN_VSYNC_DIODE_FLIP_SEPARATION_SEC = 0.015  # 0.022 is typical

FRAME_RATE = 59.95
FRAME_INTERVAL = 1 / FRAME_RATE
# we've seen apparently real flip intervals of 0.29 * 59.95, and the line between anomalous and real isn't clear-cut
SUSPICIOUS_INTERVAL_THRESHOLD = 0.3 / FRAME_RATE
DEFINITE_SHORT_INTERVAL_THRESHOLD = 0.05 / FRAME_RATE
# at 60 fps we should never have diode-flip intervals this small: consider them anomalous


def get_plot_dir(mkdir: bool = False) -> upath.UPath | None:
    if (p := os.environ.get("SYNC_PLOT_DIR", None)) is None:
        return None
    path = upath.UPath(p).resolve()
    if mkdir:
        path.mkdir(parents=True, exist_ok=True)
    return path


def get_sync_data(sync_path_or_data: SyncPathOrDataset) -> SyncDataset:
    """Open a path or file-like object and return a SyncDataset object."""
    if isinstance(sync_path_or_data, SyncDataset):
        return sync_path_or_data
    return SyncDataset(sync_path_or_data)


def get_single_sync_path(
    dir_or_paths: npc_io.PathLike | Iterable[npc_io.PathLike],
    date: (
        str
        | datetime.date
        | datetime.datetime
        | npc_session.DateRecord
        | npc_session.DatetimeRecord
        | None
    ) = None,
) -> upath.UPath:
    """From an iterable of paths, return the one with the expected date and a
    .sync or .h5 extension.

    >>> get_single_sync_path('s3://aind-ephys-data/ecephys_676909_2023-12-14_12-43-11/behavior')
    S3Path('s3://aind-ephys-data/ecephys_676909_2023-12-14_12-43-11/behavior/20231214T124311.h5')
    """
    if isinstance(dir_or_paths, str):
        dir_or_paths = npc_io.from_pathlike(dir_or_paths)
    if not isinstance(dir_or_paths, Iterable):
        if (d := npc_io.from_pathlike(dir_or_paths)).is_dir():
            dir_or_paths = d.iterdir()
        else:
            dir_or_paths = [dir_or_paths]

    if date is not None:
        try:
            date = npc_session.DatetimeRecord(date)
        except ValueError:
            date = npc_session.DateRecord(date)

    sync_files = []
    all_paths: list[upath.UPath] = [npc_io.from_pathlike(p) for p in dir_or_paths]  # type: ignore[arg-type, union-attr]
    for p in all_paths:
        if p.suffix not in (".sync", ".h5"):
            continue
        if date is not None:
            try:
                if isinstance(date, npc_session.DatetimeRecord):
                    file_date: npc_session.DatetimeRecord | npc_session.DateRecord = (
                        npc_session.DatetimeRecord(p.stem)
                    )
                else:
                    file_date = npc_session.DateRecord(p.stem)
            except ValueError:
                continue
            if file_date != date:
                continue
        sync_files.append(p)

    if not len(sync_files) == 1:
        raise ValueError(f"Expected 1 sync file, found {sync_files = }")
    return sync_files[0]


def get_bit(uint_array: npt.NDArray, bit: int) -> npt.NDArray[np.uint8]:
    """
    Returns a bool array for a specific bit in a uint ndarray.

    Parameters
    ----------
    uint_array : (numpy.ndarray)
        The array to extract bits from.
    bit : (int)
        The bit to extract.

    """
    return np.bitwise_and(uint_array, 2**bit).astype(bool).astype(np.uint8)


def get_sync_line_for_stim_onset(
    waveform_type: (
        str | Literal["sound", "audio", "opto", "galvo", "laser_488", "laser_633"]
    ),
    date: datetime.date | None = None,
) -> int:
    """Return the sync line index for the given waveform type.

    - `opto` currently defaults to laser 488
    """
    if any(label in waveform_type for label in ("aud", "sound")):
        if date and date < FIRST_SOUND_ON_SYNC_DATE:
            raise ValueError(
                f"Sound only recorded on sync since {FIRST_SOUND_ON_SYNC_DATE.isoformat()}: {date = }"
            )
        return 1
    elif any(label in waveform_type for label in ("opto", "488")):
        return 11
    elif "galvo" in waveform_type:
        if date and date < FIRST_GALVO_ON_SYNC_DATE:
            raise ValueError(
                f"Galvo only recorded on sync since {FIRST_GALVO_ON_SYNC_DATE.isoformat()}: {date = }"
            )
        return 9
    elif "633" in waveform_type:
        if date and date < FIRST_GALVO_ON_SYNC_DATE:
            raise ValueError(
                f"Laser 688 only recorded on sync since {FIRST_GALVO_ON_SYNC_DATE.isoformat()}: {date = }"
            )
        return 10
    else:
        raise ValueError(f"Unexpected value: {waveform_type = }")


class SyncDataset:
    """
    A sync dataset.  Contains methods for loading
        and parsing the binary data.

    Parameters
    ----------
    path : str
        Path to HDF5 file.

    Examples
    --------
    >>> dset = SyncDataset('s3://aind-ephys-data/ecephys_676909_2023-12-14_12-43-11/behavior/20231214T124311.h5')
    >>> dset.validate(opto=True, audio=True)

    >>> with SyncDataset('my_h5_file.h5') as d: # doctest: +SKIP
    ...     dset.validate()

    The sync file documentation from MPE can be found at
    sharepoint > Instrumentation > Shared Documents > Sync_line_labels_discussion_2020-01-27-.xlsx  # NOQA E501
    Direct link:
    https://alleninstitute.sharepoint.com/:x:/s/Instrumentation/ES2bi1xJ3E9NupX-zQeXTlYBS2mVVySycfbCQhsD_jPMUw?e=Z9jCwH


    """

    stim_paths: tuple[upath.UPath, ...] | None

    def __init__(
        self,
        path: npc_io.PathLike | Self,
        stim_paths: Iterable[npc_io.PathLike] | None = None,
    ) -> None:
        if isinstance(path, self.__class__):
            self = path
        else:
            self.path = npc_io.from_pathlike(path)
            self.dfile = self.load(path)
        if stim_paths is None:
            self.stim_paths = None
        else:
            self.stim_paths = tuple(npc_io.from_pathlike(p) for p in stim_paths)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}({self.path.as_posix()})"

    def validate(
        self,
        camstim: bool = True,
        mvr: bool = True,
        barcodes: bool = True,
        licks: bool = False,
        opto: bool = False,
        audio: bool = False,
    ) -> None:
        """
        Check all lines are present and have events.

        - if opto or audio are True, work out which line indices correspond and
          check those too
        """
        if not self.line_labels:
            raise AssertionError("Sync file has no line labels.")

        lines: list[str | int] = []
        if camstim:
            lines.extend(["vsync_stim", "stim_photodiode", "stim_running"])
        if mvr:
            lines.extend(
                [
                    f"{cam}_cam_{suffix}"
                    for cam in ("beh", "eye", "face")
                    for suffix in ("frame_readout", "exposing")
                ]
            )
        if barcodes:
            lines.append("barcode_ephys")
        if licks:
            lines.append("lick_sensor")
        if opto:
            has_laser_633_and_galvo = self.start_time.date() >= FIRST_GALVO_ON_SYNC_DATE
            try:
                self._check_line(a := self.get_line_for_stim_onset("laser_488"))
            except AssertionError:
                if has_laser_633_and_galvo:
                    self._check_line(b := self.get_line_for_stim_onset("laser_633"))
                else:
                    raise AssertionError(
                        f"Sync file has no events on opto lines {a = } or {b = }"
                    ) from None
            if has_laser_633_and_galvo:
                self._check_line(self.get_line_for_stim_onset("galvo"))
        if audio and self.start_time.date() >= FIRST_SOUND_ON_SYNC_DATE:
            lines.append(self.get_line_for_stim_onset("audio"))

        for line in lines:
            self._check_line(line)

        if camstim:
            self._check_camstim_lines()
        logger.info(f"Sync file passed validation for {lines = }")

    def _check_line(self, label_or_index: str | int) -> None:
        """Verify line is present and has events, or raise AssertionError."""
        stats = self.line_stats(label_or_index)
        if stats is None:
            raise AssertionError(f"Sync file has no events on line {label_or_index}")

    def _check_camstim_lines(self) -> None:
        self._check_stim_photodiode()
        self._check_vsyncs()

    def _check_stim_photodiode(self) -> None:
        try:
            _ = self.expected_diode_flip_rate
        except ValueError as exc:
            raise AssertionError(
                "Frame rate estimated from diode flips is abnormal."
            ) from exc

    def _check_vsyncs(self) -> None:
        try:
            _ = self.vsync_times_in_blocks
        except ValueError as exc:
            raise AssertionError(
                "vsyncs should be divisible into blocks corresponding to individual stims presented, but they appear abnormal."
            ) from exc

    def _process_times(self) -> npt.NDArray[np.int64]:
        """
        Preprocesses the time array to account for rollovers.
            This is only relevant for event-based sampling.

        """
        times = self.get_all_events()[:, 0:1].astype(np.int64)

        intervals = np.ediff1d(times, to_begin=np.uint8(0))
        rollovers = np.where(intervals < 0)[0]

        for i in rollovers:
            times[i:] += 4294967296

        return times

    def load(self, path) -> h5py.File:
        """
        Loads an hdf5 sync dataset.

        Parameters
        ----------
        path : str
            Path to hdf5 file.

        """
        if isinstance(path, h5py.File):
            raise TypeError(
                f"{self.__class__.__name__} expects a path, not a h5py.File object (no longer supported after July 2025)."
            )
        else:
            try:
                self.dfile = h5py.File(path, "r")
            except OSError:
                path = npc_io.from_pathlike(path)
                if path.protocol in ("", "file"):
                    self.dfile = h5py.File(io.BytesIO(path.read_bytes()), "r")
                else:
                    self.dfile = h5py.File(path.open(mode="rb"), "r")
        return self.dfile

    @property
    def meta_data(self) -> dict[str, Any]:
        return eval(self.dfile["meta"][()])

    @property
    def line_labels(self) -> Sequence[str]:
        return self.meta_data["line_labels"]

    @property
    def times(self) -> npt.NDArray[np.int64]:
        return self._process_times()

    def get_line_for_stim_onset(
        self,
        waveform_type: Literal[
            "sound", "audio", "opto", "galvo", "laser_488", "laser_633"
        ],
    ) -> int:
        line = get_sync_line_for_stim_onset(
            waveform_type=waveform_type, date=self.start_time.date()
        )
        if "opto" not in waveform_type:
            return line
        if np.any(self.get_edges("all", line, units="seconds")):
            return line
        return get_sync_line_for_stim_onset("laser_633", self.start_time.date())

    @property
    def sample_freq(self) -> float:
        try:
            return float(self.meta_data["ni_daq"]["sample_freq"])
        except KeyError:
            return float(self.meta_data["ni_daq"]["counter_output_freq"])

    def get_bit(self, bit: int) -> npt.NDArray[np.uint8]:
        """
        Returns the values for a specific bit.

        Parameters
        ----------
        bit : int
            Bit to return.
        """
        return get_bit(self.get_all_bits(), bit)

    def get_line(self, line: str | int) -> npt.NDArray[np.uint8]:
        """
        Returns the values for a specific line.

        Parameters
        ----------
        line : str
            Line to return.

        """
        bit = self._line_to_bit(line)
        return self.get_bit(bit)

    def get_bit_changes(self, bit: int) -> npt.NDArray[np.uint8]:
        """
        Returns the first derivative of a specific bit.
            Data points are 1 on rising edges and 255 on falling edges.

        Parameters
        ----------
        bit : int
            Bit for which to return changes.

        """
        bit_array = self.get_bit(bit)
        return np.ediff1d(bit_array, to_begin=np.uint8(0))

    def get_line_changes(self, line: str | int) -> npt.NDArray[np.uint8]:
        """
        Returns the first derivative of a specific line.
            Data points are 1 on rising edges and 255 on falling edges.

        Parameters
        ----------
        line : (str)
            Line name for which to return changes.

        """
        bit = self._line_to_bit(line)
        return self.get_bit_changes(bit)

    def get_all_bits(self) -> npt.NDArray:
        """
        Returns the data for all bits.

        """
        return np.array(self.dfile["data"][()][:, -1])

    def get_all_times(self, units: Literal["samples", "seconds"]) -> npt.NDArray[Any]:
        """
        Returns all counter values.

        Parameters
        ----------
        units : str
            Return times in 'samples' or 'seconds'

        """
        if self.meta_data["ni_daq"]["counter_bits"] == 32:
            times = self.get_all_events()[:, 0]
        else:
            times = self.times
        if units.lower() == "samples":
            return times
        elif units.lower() in ["seconds", "sec", "secs"]:
            freq = self.sample_freq
            return times / freq
        else:
            raise ValueError("Only 'samples' or 'seconds' are valid units.")

    def get_all_events(self) -> npt.NDArray[Any]:
        """
        Returns all counter values and their cooresponding IO state.
        """
        return np.array(self.dfile["data"][()])

    def get_events_by_bit(self, bit: int, units=Literal["seconds", "samples"]):
        """
        Returns all counter values for transitions (both rising and falling)
            for a specific bit.

        Parameters
        ----------
        bit : int
            Bit for which to return events.

        """
        changes = self.get_bit_changes(bit)
        return self.get_all_times(units)[np.where(changes != 0)]

    def get_events_by_line(
        self, line: str | int, units: Literal["samples", "seconds"] = "samples"
    ):
        """
        Returns all counter values for transitions (both rising and falling)
            for a specific line.

        Parameters
        ----------
        line : str
            Line for which to return events.

        """
        line = self._line_to_bit(line)
        return self.get_events_by_bit(line, units)

    def _line_to_bit(self, line: str | int) -> int:
        """
        Returns the bit for a specified line.  Either line name and number is
            accepted.

        Parameters
        ----------
        line : str
            Line name for which to return corresponding bit.

        """
        if type(line) is int:
            return line
        elif type(line) is str:
            return self.line_labels.index(line)
        else:
            raise TypeError("Incorrect line type.  Try a str or int.")

    def _bit_to_line(self, bit: int) -> str:
        """
        Returns the line name for a specified bit.

        Parameters
        ----------
        bit : int
            Bit for which to return the corresponding line name.
        """
        return self.line_labels[bit]

    def get_rising_edges(
        self, line: str | int, units: Literal["samples", "seconds"] = "samples"
    ) -> npt.NDArray[Any]:
        """
        Returns the counter values for the rizing edges for a specific bit or
            line.

        Parameters
        ----------
        line : str
            Line for which to return edges.

        """
        bit = self._line_to_bit(line)
        changes = self.get_bit_changes(bit)
        return self.get_all_times(units)[np.where(changes == 1)]

    def get_edges(
        self,
        kind: Literal["rising", "falling", "all"],
        keys: str | int | Sequence[str | int],
        units: Literal["seconds", "samples"],
    ) -> npt.NDArray:
        """Utility function for extracting edge times from a line

        Parameters
        ----------
        kind : One of "rising", "falling", or "all". Should this method return
            timestamps for rising, falling or both edges on the appropriate
            line
        keys : These will be checked in sequence. Timestamps will be returned
            for the first which is present in the line labels
        units : one of "seconds", "samples", or "indices". The returned
            "time"stamps will be given in these units.
        raise_missing : If True and no matching line is found, a KeyError will
            be raised

        Returns
        -------
        An array of edge times. If raise_missing is False and none of the keys
            were found, returns None.

        Raises
        ------
        KeyError : none of the provided keys were found among this dataset's
            line labels

        """
        if kind == "falling":
            fn = self.get_falling_edges
        elif kind == "rising":
            fn = self.get_rising_edges
        elif kind == "all":
            rising = self.get_edges("rising", keys, units)
            falling = self.get_edges("falling", keys, units)
            return np.sort(np.concatenate([rising, falling]))

        if isinstance(keys, (str, int)):
            keys = [keys]

        for key in keys:
            try:
                result = fn(key, units)
            except ValueError:
                continue
            else:
                return result
        raise KeyError(f"none of {keys} were found in this dataset's line labels")

    def get_falling_edges(
        self, line: str | int, units: Literal["samples", "seconds"] = "samples"
    ):
        """
        Returns the counter values for the falling edges for a specific bit
            or line.

        Parameters
        ----------
        line : str
            Line for which to return edges.

        """
        bit = self._line_to_bit(line)
        changes = self.get_bit_changes(bit)
        return self.get_all_times(units)[np.where(changes == 255)]

    def get_nearest(
        self,
        source: str,
        target: str,
        source_edge: Literal["rising", "falling"] = "rising",
        target_edge: Literal["rising", "falling"] = "rising",
        direction: Literal["previous", "next"] = "previous",
        units: Literal["indices", "samples", "seconds"] = "indices",
    ) -> npt.NDArray:
        """
        For all values of the source line, finds the nearest edge from the
            target line.

        By default, returns the indices of the target edges.

        Args:
            source (str, int): desired source line
            target (str, int): desired target line
            source_edge [Optional(str)]: "rising" or "falling" source edges
            target_edge [Optional(str): "rising" or "falling" target edges
            direction (str): "previous" or "next". Whether to prefer the
                previous edge or the following edge.
            units (str): "indices"

        """
        source_edges = getattr(self, f"get_{source_edge.lower()}_edges")(
            source.lower(), units="samples"
        )  # E501
        target_edges = getattr(self, f"get_{target_edge.lower()}_edges")(
            target.lower(), units="samples"
        )  # E501
        indices = np.searchsorted(target_edges, source_edges, side="right")
        if direction.lower() == "previous":
            indices[np.where(indices != 0)] -= 1
        elif direction.lower() == "next":
            indices[np.where(indices == len(target_edges))] = -1
        if units in ["indices", "index"]:
            return indices
        elif units == "samples":
            return target_edges[indices]
        elif units in ["sec", "seconds", "second"]:
            return target_edges[indices] / self.sample_freq
        else:
            raise KeyError("Invalid units.  Try 'seconds', 'samples' or 'indices'")

    def get_analog_channel(
        self,
        channel: int,
        start_time: float = 0.0,
        stop_time: float | None = None,
        downsample: int = 1,
    ) -> npt.NDArray:
        """
        Returns the data from the specified analog channel between the
            timepoints.

        Args:
            channel (int, str): desired channel index or label
            start_time (Optional[float]): start time in seconds
            stop_time (Optional[float]): stop time in seconds
            downsample (Optional[int]): downsample factor

        Returns:
            ndarray: slice of data for specified channel

        Raises:
            KeyError: no analog data present

        """
        if isinstance(channel, str):
            channel_index = self.analog_meta_data["analog_labels"].index(channel)
            channel = self.analog_meta_data["analog_channels"].index(channel_index)

        if "analog_data" in self.dfile.keys():
            dset = np.array(self.dfile["analog_data"])
            analog_meta = self.get_analog_meta()
            sample_rate = analog_meta["analog_sample_rate"]
            start = int(start_time * sample_rate)
            if stop_time:
                stop = int(stop_time * sample_rate)
                return dset[start:stop:downsample, channel]
            else:
                return dset[start::downsample, channel]
        else:
            raise KeyError("No analog data was saved.")

    def get_analog_meta(self) -> Any:
        """
        Returns the metadata for the analog data.
        """
        if "analog_meta" in self.dfile.keys():
            return eval(self.dfile["analog_meta"].value)
        else:
            raise KeyError("No analog data was saved.")

    @property
    def analog_meta_data(self) -> Any:
        return self.get_analog_meta()

    def line_stats(self, line, print_results=True) -> dict[str, Any] | None:
        """
        Quick-and-dirty analysis of a bit.

        ##TODO: Split this up into smaller functions.

        """
        # convert to bit
        bit = self._line_to_bit(line)

        # get the bit's data
        bit_data = self.get_bit(bit)
        total_data_points = len(bit_data)

        # get the events
        events = self.get_events_by_bit(bit, units="samples")
        total_events = len(events)

        # get the rising edges
        rising = self.get_rising_edges(bit)
        total_rising = len(rising)

        # get falling edges
        falling = self.get_falling_edges(bit)
        total_falling = len(falling)

        # get labels
        label = self.line_labels[bit]

        if total_events <= 0:
            if print_results:
                logger.info("*" * 70)
                logger.info("No events on line: %s" % line)
                logger.info("*" * 70)
            return None
        # period
        try:
            period = self.period(line)
        except IndexError:  # not enough edges
            period = {}

        avg_period = period.get("avg")
        max_period = period.get("max")
        min_period = period.get("min")
        period_sd = period.get("sd")

        # freq
        avg_freq = self.frequency(line) if period else None

        # duty cycle
        duty_cycle = self.duty_cycle(line)

        if print_results:
            logger.info("*" * 70)
            if total_events <= 10:
                logger.warning("Sparse events on line: %s" % line)
            else:
                logger.info("Quick stats for line: %s" % line)
            logger.info("Label: %s" % label)
            logger.info("Bit: %i" % bit)
            logger.info("Data points: %i" % total_data_points)
            logger.info("Total transitions: %i" % total_events)
            logger.info("Rising edges: %i" % total_rising)
            logger.info("Falling edges: %i" % total_falling)
            logger.info("Average period: %s" % avg_period)
            logger.info("Minimum period: %s" % min_period)
            logger.info("Max period: %s" % max_period)
            logger.info("Period SD: %s" % period_sd)
            logger.info("Average freq: %s" % avg_freq)
            logger.info("Duty cycle: %s" % duty_cycle)
            logger.info("*" * 70)

        return {
            "line": line,
            "label": label,
            "bit": bit,
            "total_data_points": total_data_points,
            "total_events": total_events,
            "total_rising": total_rising,
            "total_falling": total_falling,
            "avg_period": avg_period,
            "min_period": min_period,
            "max_period": max_period,
            "period_sd": period_sd,
            "avg_freq": avg_freq,
            "duty_cycle": duty_cycle,
        }

    def period(
        self, line: str | int, edge: Literal["rising", "falling"] = "rising"
    ) -> dict[str, Any]:
        """
        Returns a dictionary with avg, min, max, and st of period for a line.
        """
        bit = self._line_to_bit(line)

        if edge.lower() == "rising":
            edges = self.get_rising_edges(bit)
        elif edge.lower() == "falling":
            edges = self.get_falling_edges(bit)
        else:
            raise ValueError(
                f'edge should be one of ("rising", "falling") not {edge!r}'
            )

        if len(edges) > 2:
            timebase_freq = self.meta_data["ni_daq"]["counter_output_freq"]
            avg_period = np.mean(np.ediff1d(edges[1:])) / timebase_freq
            max_period = np.max(np.ediff1d(edges[1:])) / timebase_freq
            min_period = np.min(np.ediff1d(edges[1:])) / timebase_freq
            period_sd = np.std(avg_period)

        else:
            raise IndexError("Not enough edges for period: %i" % len(edges))

        return {
            "avg": avg_period,
            "max": max_period,
            "min": min_period,
            "sd": period_sd,
        }

    def frequency(
        self, line: str | int, edge: Literal["rising", "falling"] = "rising"
    ) -> float:
        """
        Returns the average frequency of a line.
        """
        period = self.period(line, edge)
        return 1.0 / period["avg"]

    def duty_cycle(self, line: str | int) -> Literal["fix me"]:
        """
        Doesn't work right now.  Freezes python for some reason.

        Returns the duty cycle of a line.

        """
        return "fix me"
        bit = self._line_to_bit(line)

        rising = self.get_rising_edges(bit)
        falling = self.get_falling_edges(bit)

        total_rising = len(rising)
        total_falling = len(falling)

        if total_rising > total_falling:
            rising = rising[:total_falling]
        elif total_rising < total_falling:
            falling = falling[:total_rising]
        else:
            pass

        if rising[0] < falling[0]:
            # line starts low
            high = falling - rising
        else:
            # line starts high
            high = np.concatenate(
                falling, self.get_all_events()[-1, 0]
            ) - np.concatenate(0, rising)

        total_high_time = np.sum(high)
        all_events = self.get_events_by_bit(bit)
        total_time = all_events[-1] - all_events[0]
        return 1.0 * total_high_time / total_time

    @property
    def stats(self) -> list[dict[str, Any]]:
        """
        Quick-and-dirty analysis of all bits.  Prints a few things about each
            bit where events are found.
        """
        bits = []
        for i in range(32):
            bits.append(self.line_stats(i, print_results=True))
        active_bits = [x for x in bits if x is not None]
        logger.info("Active bits: ", len(active_bits))
        for bit in active_bits:
            logger.info("*" * 70)
            logger.info("Bit: %i" % bit["bit"])
            logger.info("Label: %s" % self.line_labels[bit["bit"]])
            logger.info("Rising edges: %i" % bit["total_rising"])
            logger.info("Falling edges: %i" % bit["total_falling"])
            logger.info("Average freq: %s" % bit["avg_freq"])
            logger.info("Duty cycle: %s" % bit["duty_cycle"])
        logger.info("*" * 70)
        return active_bits

    @property
    def start_time(self) -> datetime.datetime:
        return datetime.datetime.fromisoformat(self.meta_data["start_time"])

    @property
    def stop_time(self) -> datetime.datetime:
        return self.start_time + datetime.timedelta(seconds=self.total_seconds)

    @npc_io.cached_property
    def stim_running_edges(
        self,
    ) -> tuple[npt.NDArray[np.floating], npt.NDArray[np.floating]]:
        """Rising edges and falling edges on stim_running line, filtered for
        erroneous events and guaranteed to be of equal length.

        - excludes falling edges alone at start, and rising edges alone at end
        - excludes rising+falling pairs that contain exactly one vsync events:
            we have one session (DRpilot_726088_20240618) where a stim was cancelled
            immediately after starting and has no data, but it has a short stim
            running block which causes problems
        """
        stim_running_rising_edges = self.get_rising_edges(
            "stim_running", units="seconds"
        )
        stim_running_falling_edges = self.get_falling_edges(
            "stim_running", units="seconds"
        )

        if any(stim_running_rising_edges) and any(stim_running_falling_edges):
            if stim_running_rising_edges[0] > stim_running_falling_edges[0]:
                stim_running_falling_edges = stim_running_falling_edges[1:]
            if stim_running_falling_edges[-1] < stim_running_rising_edges[-1]:
                stim_running_falling_edges = np.concatenate(
                    [stim_running_falling_edges, [self.total_seconds]]
                )
        assert len(stim_running_rising_edges) == len(stim_running_falling_edges)
        vsyncs = self.get_falling_edges("vsync_stim", units="seconds")
        idx_for_removal = []
        for idx, (on, off) in enumerate(
            zip(stim_running_rising_edges, stim_running_falling_edges)
        ):
            if (s := vsyncs[(vsyncs >= on) & (vsyncs <= off)].size) == 1:
                # keeping this strict for now to only affect one session
                logger.warning(
                    f"stim_running block [{on:.2f}:{off:.2f}] has {s} vsync(s)"
                    " - excluding from analysis"
                )
                idx_for_removal.append(idx)
        stim_running_rising_edges = np.delete(
            stim_running_rising_edges, idx_for_removal
        )
        stim_running_falling_edges = np.delete(
            stim_running_falling_edges, idx_for_removal
        )
        return stim_running_rising_edges, stim_running_falling_edges

    def filter_on_stim_running(
        self, data: npt.NDArray[np.floating]
    ) -> npt.NDArray[np.floating]:
        """Filter data to only include times when stim_running is high.

        Data must be in seconds relative to first sample."""
        if self.stim_running_edges[0].size == 0:
            return data
        mask = [False] * len(data)
        for on, off in zip(*self.stim_running_edges):
            mask |= (data >= on) & (data <= off)

        return data[mask]

    def divide_into_stim_running_blocks(
        self, data: npt.NDArray[np.floating]
    ) -> tuple[npt.NDArray[np.floating], ...]:
        """Divide data into blocks corresponding to stim_running being high.

        Data must be in seconds relative to first sample."""
        if self.stim_running_edges[0].size == 0:
            return (data,)
        blocks = []
        for on, off in zip(*self.stim_running_edges):
            blocks.append(data[(data >= on) & (data <= off)])

        return tuple(blocks)

    @property
    def total_seconds(self) -> float:
        return self.meta_data["total_samples"] / self.sample_freq

    @npc_io.cached_property
    def vsync_times_in_blocks(self) -> tuple[npt.NDArray[np.floating], ...]:
        """Blocks of vsync falling edge times, in seconds relative to first
        sample: one block per stimulus.
        """
        vsync_rising_edges: npt.NDArray[np.floating] = self.get_rising_edges(
            "vsync_stim", units="seconds"
        )
        vsync_falling_edges: npt.NDArray[np.floating] = self.get_falling_edges(
            "vsync_stim", units="seconds"
        )
        # ensure first vsync is rising
        vsync_falling_edges = (
            vsync_falling_edges
            if vsync_rising_edges[0] < vsync_falling_edges[0]
            else vsync_falling_edges[1:]
        )

        vsync_times_in_blocks = list(
            reshape_into_blocks(vsync_falling_edges, min_gap=1.0)
        )

        block_lengths = np.array([len(block) for block in vsync_times_in_blocks])
        logger.info(
            f"Found {len(vsync_times_in_blocks)} blocks of vsync events with lengths {block_lengths}"
        )
        stim_running_rising_edges, stim_running_falling_edges = self.stim_running_edges
        if any(stim_running_rising_edges) and any(stim_running_falling_edges):
            assert len(stim_running_rising_edges) == len(vsync_times_in_blocks)
            for idx, block in enumerate(vsync_times_in_blocks):
                vsync_times_in_blocks[idx] = self.filter_on_stim_running(block)

        assert all(block.size > 0 for block in vsync_times_in_blocks)
        return tuple(vsync_times_in_blocks)

    @npc_io.cached_property
    def expected_diode_flip_rate(self) -> int:
        """Best-guess at what the diode flip period should be, e.g. 1 s for
        MPE/pipeline recordings, 1/60 s for Sam's TaskControl scripts."""
        med = np.median(
            np.diff(self.get_edges("all", "stim_photodiode", units="seconds"))
        )
        diode_assymmetry = 0.05  # s
        for period in (1 / 60, 1):
            if 0.9 * period - diode_assymmetry < med < 1.1 * period + diode_assymmetry:
                return int(1 / period)
        raise ValueError(f"Unexpected diode flip period: {med} sec")

    @npc_io.cached_property
    def expected_frame_display_rate(self) -> int:
        """Best-guess at what the screen display rate should be. Currently
        [2023] should only be 60 fps."""
        med = np.median(np.diff(self.get_falling_edges("vsync_stim", units="seconds")))
        for period in (1 / 60, 1 / 120, 1 / 144, 1 / 300):
            if 0.9 * period < med < 1.1 * period:
                return int(1 / period)
        raise ValueError(f"Unexpected vsync period: {med} sec")

    @npc_io.cached_property
    def is_block_using_diode(self) -> tuple[bool, ...]:
        if getattr(self, "_blocks_using_diode", None) is None:
            _ = self.frame_display_time_blocks
        return self._blocks_using_diode

    @npc_io.cached_property
    def block_index_to_stim_path(self) -> dict[int, upath.UPath | None]:
        if self.stim_paths is None:
            logger.warning(
                "getting all hdf5 files available in same folder as sync file: pass a list on init to use specific files"
            )
            self.stim_paths = tuple(upath.UPath(self.path).parent.glob("*.hdf5"))
        return npc_stim.get_stim_block_to_path(
            stim_paths=self.stim_paths,
            sync_data=self,
        )

    @npc_io.cached_property
    def expected_vsyncs_per_block(self) -> dict[int, int | None]:
        result = {}
        for block_idx, path in self.block_index_to_stim_path.items():
            if path is None:
                result[block_idx] = None
                continue
            try:
                stim_data = npc_stim.get_stim_data(path)
            except Exception as exc:
                logger.warning(
                    f"Failed to load stim data for block {block_idx} at {path}: {exc}"
                )
                result[block_idx] = None
                continue
            result[block_idx] = npc_stim.get_total_stim_frames(stim_data)
        return result

    @npc_io.cached_property
    def frame_display_time_blocks(self) -> tuple[npt.NDArray[np.floating], ...]:
        """Blocks of adjusted diode times: one block per stimulus."""
        vsync_times_in_blocks = self.vsync_times_in_blocks

        diode_rising_edges = self.get_rising_edges("stim_photodiode", units="seconds")
        diode_falling_edges = self.get_falling_edges("stim_photodiode", units="seconds")
        assert abs(len(diode_rising_edges) - len(diode_falling_edges)) < 2

        if self.stim_running_edges[0].size > 0:  # has stim running signal
            diode_rising_edges_in_blocks = self.divide_into_stim_running_blocks(
                diode_rising_edges
            )
            diode_falling_edges_in_blocks = self.divide_into_stim_running_blocks(
                diode_falling_edges
            )
        else:
            diode_rising_edges_in_blocks = reshape_into_blocks(
                self.filter_on_stim_running(diode_rising_edges),
                min_gap=1.0,
            )
            diode_falling_edges_in_blocks = reshape_into_blocks(
                self.filter_on_stim_running(diode_falling_edges),
                min_gap=1.0,
            )

        if any(
            len(diode_edge_blocks) < len(vsync_times_in_blocks)
            for diode_edge_blocks in (
                diode_rising_edges_in_blocks,
                diode_falling_edges_in_blocks,
            )
        ):
            # too few blocks of diode flips - affected some early DRPilot
            # sessions with near-constant diode flips between stimulus
            # blocks, and no stim-running signal to filter them

            # - vsyncs necessarily precede diode flips
            # - split existing diode flip blocks at the first vsync time for
            #   each vsync block
            # - we'll clean up any excess diode flips at the end of the block later
            # to see the problem compare these two lists:
            # [(v[0], v[-1]) for v in vsync_times_in_blocks]
            # [(v[0], v[-1]) for v in diode_falling_edges_in_blocks]
            for v_start in (v[0] for v in vsync_times_in_blocks):
                assert len(diode_rising_edges_in_blocks) == len(
                    diode_falling_edges_in_blocks
                )
                new_rising_edges: list[npt.NDArray] = []
                new_falling_edges: list[npt.NDArray] = []
                for d_idx, (d_start, d_stop) in enumerate(
                    (d[0], d[-1]) for d in diode_falling_edges_in_blocks
                ):
                    if d_start < v_start < d_stop and (
                        split := np.searchsorted(
                            diode_falling_edges_in_blocks[d_idx], v_start
                        )
                    ) not in (0, len(diode_falling_edges_in_blocks[d_idx])):
                        for new, old in zip(
                            (new_rising_edges, new_falling_edges),
                            (
                                diode_rising_edges_in_blocks[d_idx],
                                diode_falling_edges_in_blocks[d_idx],
                            ),
                        ):
                            new.extend((old[:split], old[split:]))
                    else:
                        for new, old in zip(
                            (new_rising_edges, new_falling_edges),
                            (
                                diode_rising_edges_in_blocks[d_idx],
                                diode_falling_edges_in_blocks[d_idx],
                            ),
                        ):
                            new.append(old)
                diode_rising_edges_in_blocks, diode_falling_edges_in_blocks = (
                    tuple(new_rising_edges),
                    tuple(new_falling_edges),
                )

        if any(
            len(diode_edge_blocks) > len(vsync_times_in_blocks)
            for diode_edge_blocks in (
                diode_rising_edges_in_blocks,
                diode_falling_edges_in_blocks,
            )
        ):
            # more blocks of diode flips than blocks of vsyncs
            def is_mismatch(edges, block) -> bool:
                return edges[-1] < block[0]

            for idx, vsync_block in enumerate(vsync_times_in_blocks):
                # work through blocks in order,
                # discard blocks with diode flips that don't match any vsyncs
                while is_mismatch(diode_rising_edges_in_blocks[idx], vsync_block):
                    diode_rising_edges_in_blocks = tuple(
                        block
                        for i, block in enumerate(diode_rising_edges_in_blocks)
                        if i != idx
                    )
                while is_mismatch(diode_falling_edges_in_blocks[idx], vsync_block):
                    diode_falling_edges_in_blocks = tuple(
                        block
                        for i, block in enumerate(diode_falling_edges_in_blocks)
                        if i != idx
                    )

        frame_display_time_blocks: list[npt.NDArray[np.floating] | None] = []
        blocks_using_diode = [True] * len(vsync_times_in_blocks)
        for block_idx, (vsyncs, rising, falling) in enumerate(
            zip(
                vsync_times_in_blocks,
                diode_rising_edges_in_blocks,
                diode_falling_edges_in_blocks,
            )
        ):

            #! decide how to toggle this: env var?
            fallback_or_none: npt.NDArray[np.floating] | None = (
                self.constant_lag_frame_display_time_blocks[block_idx]
            )

            # keep flips after first vsync + an empirically determined min latency
            # between vsync and screen update
            falling = falling[
                falling > (vsyncs[0] + MIN_VSYNC_DIODE_FLIP_SEPARATION_SEC)
            ]
            rising = rising[rising > (vsyncs[0] + MIN_VSYNC_DIODE_FLIP_SEPARATION_SEC)]

            # keep flips only up to a certain time after the last vsync
            MAX_VSYNC_DIODE_FLIP_SEPARATION_SEC = 5 / FRAME_RATE
            falling = falling[
                falling < (vsyncs[-1] + MAX_VSYNC_DIODE_FLIP_SEPARATION_SEC)
            ]
            rising = rising[rising < (vsyncs[-1] + MAX_VSYNC_DIODE_FLIP_SEPARATION_SEC)]

            is_first_frame_on = rising[0] < falling[0]  # may be updated below

            def concat_flips(rising, falling):
                """Concatenate the rising and falling edges into a single array."""
                return np.sort(np.concatenate((rising, falling)))

            # July 2025 we updated the photodiode interval from every frame to every 3 frames
            vsyncs_per_diode_flip: int = (
                1
                if self.start_time < datetime.datetime(2025, 7, 15)
                else round(np.mean(np.diff(concat_flips(rising, falling))) * FRAME_RATE)
            )

            if (
                vsyncs_per_diode_flip == 1
                and self.expected_vsyncs_per_block.get(block_idx, None) is None
            ):
                logger.warning(
                    f"Block {block_idx} has no stim file available to verify number of vsyncs - ensure stim files are in same dir as sync file when sync square flips on every frame"
                )
                # we could fail here, but it's extremely rare that the number of vsyncs is different
                # to the number of frames in the stim file, and we always intend to have stim files available

            if vsyncs_per_diode_flip == 1 and self.expected_vsyncs_per_block[
                block_idx
            ] != len(vsyncs):
                logger.warning(
                    f"Block {block_idx} has {len(vsyncs)} vsyncs, expected "
                    f"{self.expected_vsyncs_per_block[block_idx]} from stim file: Skipping"
                )
                frame_display_time_blocks.append(fallback_or_none)
                blocks_using_diode[block_idx] = False
                continue

            def split_flips(concat_flips, is_first_frame_on):
                """Split the concatenated flips into rising and falling edges."""
                if is_first_frame_on:
                    return (
                        concat_flips[:: 2 * vsyncs_per_diode_flip],
                        concat_flips[
                            vsyncs_per_diode_flip :: 2 * vsyncs_per_diode_flip
                        ],
                    )
                else:
                    return (
                        concat_flips[
                            vsyncs_per_diode_flip :: 2 * vsyncs_per_diode_flip
                        ],
                        concat_flips[:: 2 * vsyncs_per_diode_flip],
                    )

            flips = concat_flips(rising, falling)
            while any(anomalous_interval_indices(flips, vsyncs_per_diode_flip)):

                indices = anomalous_interval_indices(flips, vsyncs_per_diode_flip)
                # just delete one interval then check again:
                logger.warning(
                    f"Removing anomalous short diode interval in block {block_idx}"
                )
                flips = np.delete(flips, slice(indices[0], indices[0] + 2))
                if 0 in indices:
                    is_first_frame_on = not is_first_frame_on

            if vsyncs_per_diode_flip == 1 and np.abs(len(flips) - len(vsyncs)) > 5:
                logger.warning(
                    f"Block {block_idx} has {len(flips)} diode flips (after basic filtering), "
                    f"but {len(vsyncs)} vsyncs: Skipping"
                )
                frame_display_time_blocks.append(fallback_or_none)
                blocks_using_diode[block_idx] = False
                continue

            rising, falling = split_flips(flips, is_first_frame_on)

            assert self.block_index_to_stim_path[block_idx] is not None

            # script_frame_intervals = h5py.File(self.block_index_to_stim_path[block_idx])['frameIntervals'][:]

            stim_id = f"{self.block_index_to_stim_path[block_idx].parent.name}_{block_idx}_{self.block_index_to_stim_path[block_idx].stem}"

            try:
                times = get_frame_display_times(
                    vsync_times=vsyncs,
                    on_flip_times=rising,
                    off_flip_times=falling,
                    vsyncs_per_flip=vsyncs_per_diode_flip,
                    adjust_dropped_frames=True,
                    check_lengths=False,
                    check_vsyncs=True,
                    stim_id=stim_id,
                )
            except (AssertionError, IndexError):
                logger.exception(
                    f"Failed to get correct frame display times for {stim_id} block {block_idx}."
                    + (
                        "Using vsync times + constant lag"
                        if fallback_or_none is not None
                        else "No frame display times will be available"
                    )
                )
                frame_display_time_blocks.append(fallback_or_none)
                blocks_using_diode[block_idx] = False
                continue

            if vsyncs_per_diode_flip == 1:
                while len(times) < len(vsyncs):
                    if len(vsyncs) - len(times) > 3:
                        raise AssertionError(
                            f"{stim_id}: Block {block_idx} has {len(times)} frame display times, "
                            f"but {len(vsyncs)} vsyncs: too many missing frames"
                        )
                    times = add_missing_diode_flips_for_truncated_sync(times, vsyncs)

                if (len(times) == len(vsyncs) + 1) and times[-1] > vsyncs[-1] + (
                    times[:-1] - vsyncs
                ).mean():
                    # special case where we have an extra flip at the end, which doesn't correspond to a vsync
                    # - can happen if the last frame's sync square is high and the screen then returns to black
                    times = times[:-1]
                if len(times) - len(vsyncs) < 5:
                    # if we have a few more frames than vsyncs, we can assume these are just extra frames
                    # at the end of the block caused by spontaneous flicker, so we remove them
                    times = times[: len(vsyncs)]
                if len(times) != len(vsyncs):
                    logger.error(
                        f"{stim_id}: Block {block_idx} has {len(times)} frame display times, "
                        f"but {len(vsyncs)} vsyncs"
                    )
                    frame_display_time_blocks.append(fallback_or_none)
                    blocks_using_diode[block_idx] = False
                    continue

            frame_display_time_blocks.append(times + MONITOR_CENTER_REFRESH_TIME)
            logger.info(
                f"Added frame display times for {self.block_index_to_stim_path[block_idx]}"
            )
            continue

        assert (
            len(frame_display_time_blocks)
            == len(self.vsync_times_in_blocks)
            == len(blocks_using_diode)
        )
        self._blocks_using_diode = tuple(blocks_using_diode)
        return tuple(frame_display_time_blocks)

    @property
    def constant_lag_frame_display_time_blocks(
        self,
    ) -> tuple[npt.NDArray[np.floating], ...]:
        """Blocks of vsync times + a constant: one block per stimulus. For use
        when a reliable photodiode signal is unavailable."""
        return tuple(
            block + CONSTANT_MONITOR_LAG for block in self.vsync_times_in_blocks
        )

    def plot_all(
        self,
        start_time: float,
        stop_time: float | None = None,
        auto_show: bool = True,
    ) -> None:
        """
        Plot all active bits.

        Yikes.  Come up with a better way to show this.

        """
        import matplotlib.pyplot as plt

        for bit in range(32):
            if len(self.get_events_by_bit(bit)) > 0:
                self.plot_bit(
                    bit,
                    start_time,
                    stop_time,
                    auto_show=False,
                )
        if auto_show:
            plt.show()

    def plot_bits(
        self,
        bits: Sequence[int],
        start_time: float = 0.0,
        end_time: float | None = None,
        auto_show: bool = True,
    ) -> matplotlib.figure.Figure:
        """
        Plots a list of bits.
        """
        import matplotlib.pyplot as plt

        subplots = len(bits)
        f, axes = plt.subplots(subplots, sharex=True, sharey=True)
        if not isinstance(axes, Iterable):
            axes = [axes]

        for bit, ax in zip(bits, axes):
            self.plot_bit(bit, start_time, end_time, auto_show=False, axes=ax)
        # f.set_size_inches(18, 10, forward=True)
        f.subplots_adjust(hspace=0)

        if auto_show:
            plt.show()

        return f

    def plot_bit(
        self,
        bit,
        start_time: float = 0.0,
        end_time: float | None = None,
        auto_show: bool = True,
        axes=None,
        name="",
    ) -> matplotlib.figure.Figure:
        """
        Plots a specific bit at a specific time period.
        """
        import matplotlib.pyplot as plt

        times = self.get_all_times(units="seconds")
        if not end_time:
            end_time = 2**32

        window = (times < end_time) & (times > start_time)

        if axes:
            ax = axes
        else:
            ax = plt

        if not name:
            name = self._bit_to_line(bit)
        if not name:
            name = str(bit)

        bit = self.get_bit(bit)
        ax.step(times[window], bit[window], where="post")
        if hasattr(ax, "set_ylim"):
            ax.set_ylim(-0.1, 1.1)
        else:
            axes_obj = plt.gca()
            axes_obj.set_ylim(-0.1, 1.1)
        # ax.set_ylabel('Logic State')
        # ax.yaxis.set_ticks_position('none')
        plt.setp(ax.get_yticklabels(), visible=False)
        ax.set_xlabel("time (seconds)")
        ax.legend([name])

        if auto_show:
            plt.show()

        return plt.gcf()

    def plot_line(
        self,
        line,
        start_time: float = 0.0,
        end_time: float | None = None,
        auto_show: bool = True,
    ) -> None:
        """
        Plots a specific line at a specific time period.
        """
        import matplotlib.pyplot as plt

        bit = self._line_to_bit(line)
        self.plot_bit(bit, start_time, end_time, auto_show=False)

        # plt.legend([line])
        if auto_show:
            plt.show()

    def plot_lines(
        self,
        lines,
        start_time: float = 0.0,
        end_time: float | None = None,
        auto_show: bool = True,
    ) -> matplotlib.figure.Figure:
        """
        Plots specific lines at a specific time period.
        """
        import matplotlib.pyplot as plt

        bits = []
        for line in lines:
            bits.append(self._line_to_bit(line))
        fig = self.plot_bits(
            bits,
            start_time,
            end_time,
            auto_show=False,
        )

        plt.subplots_adjust(left=0.025, right=0.975, bottom=0.05, top=0.95)
        if auto_show:
            plt.show()

        return fig

    @property
    def stim_onsets(self) -> npt.NDArray[np.floating]:
        if any(stim_running := self.get_rising_edges("stim_running", units="seconds")):
            return stim_running
        return np.array([block[0] for block in self.vsync_times_in_blocks])

    @property
    def stim_offsets(self) -> npt.NDArray[np.floating]:
        if any(stim_running := self.get_falling_edges("stim_running", units="seconds")):
            return stim_running
        return np.array([block[-1] for block in self.frame_display_time_blocks])

    def plot_stim_onsets(self) -> matplotlib.figure.Figure:
        import matplotlib.pyplot as plt

        # plot beginning of stims
        fig, _ = plt.subplots(len(self.stim_onsets))
        axes = fig.axes
        fig.suptitle("Stim onsets")
        for ind, (son, vs, dvs) in enumerate(
            zip(
                self.stim_onsets,
                self.vsync_times_in_blocks,
                self.frame_display_time_blocks,
            )
        ):
            labels = []
            axes[ind].plot(vs, 0.5 * np.ones(len(vs)), "|")
            labels.append("stim vsyncs")
            axes[ind].plot(dvs, 0.5 * np.ones(len(dvs)), "|", ms=20)
            labels.append("frame display (estimated)")

            # focus on the start of vsyncs if they occur well after the stim-TTL onset
            x0, x1 = max(son - 1, vs[0] - 1), max(son + 2, vs[0] + 2)

            self.plot_bit(4, x0, x1, axes=axes[ind], auto_show=False)
            labels.append("diode-measured sync square")
            self.plot_bit(5, x0, x1, axes=axes[ind], auto_show=False)
            labels.append("stim running or ends of block")
            axes[ind].set_xlim((x0, x1))
            axes[ind].legend(
                labels,
                fontsize=8,
                loc="upper center",
                bbox_to_anchor=(0.5, 1.05),
                ncol=len(labels),
                fancybox=True,
            )
            if len(fig.axes) > 1:
                axes[ind].set_title(f"visual stim {ind}", fontsize=8)
                legend = axes[ind].get_legend()
                if ind > 0 and legend is not None:
                    legend.remove()
        fig.set_size_inches(10, 5 * len(fig.axes))
        fig.subplots_adjust(hspace=0.3)
        return fig

    def plot_stim_offsets(self) -> matplotlib.figure.Figure:
        import matplotlib.pyplot as plt

        # plot end of stims
        fig, _ = plt.subplots(len(self.stim_offsets))
        axes = fig.axes

        fig.suptitle("Stim offsets")
        for ind, (soff, vs, dvs) in enumerate(
            zip(
                self.stim_offsets,
                self.vsync_times_in_blocks,
                self.frame_display_time_blocks,
            )
        ):
            labels = []
            axes[ind].plot(vs, 0.5 * np.ones(len(vs)), "|")
            labels.append("stim vsyncs")
            axes[ind].plot(dvs, 0.5 * np.ones(len(dvs)), "|", ms=20)
            labels.append("frame display (estimated)")

            # focus on the end of vsyncs if they occur well before the stim-TTL offset
            x0, x1 = min(soff - 2, vs[-1] - 2), min(soff + 1, vs[-1] + 1)

            self.plot_bit(4, x0, x1, axes=axes[ind], auto_show=False)
            labels.append("diode-measured sync square")
            self.plot_bit(5, x0, x1, axes=axes[ind], auto_show=False)
            labels.append("stim running or ends of block")
            axes[ind].set_xlim((x0, x1))
            axes[ind].legend(
                labels,
                fontsize=8,
                loc="upper center",
                bbox_to_anchor=(0.5, 1.05),
                ncol=len(labels),
                fancybox=True,
            )
            if len(fig.axes) > 1:
                axes[ind].set_title(f"visual stim {ind}", fontsize=8)
                legend = axes[ind].get_legend()
                if ind > 0 and legend is not None:
                    legend.remove()
        fig.set_size_inches(10, 5 * len(fig.axes))
        fig.subplots_adjust(hspace=0.3)
        return fig

    def plot_diode_measured_sync_square_flips(
        self,
    ) -> matplotlib.figure.Figure:
        """Plot the diode-measured sync-square changes that should occur every 1 s while stim is running."""
        import matplotlib.pyplot as plt

        stim_ons, stim_offs = self.stim_onsets, self.stim_offsets

        # we want the diode flips that occur after the stim-running TTL goes high
        # and after the vsyncs start
        all_diode_flips = np.concatenate(
            [
                self.get_rising_edges("stim_photodiode", units="seconds"),
                self.get_falling_edges("stim_photodiode", units="seconds"),
            ]
        )
        all_vsyncs = self.get_falling_edges("vsync_stim", units="seconds")

        frequency = self.expected_diode_flip_rate
        expected_period = 1 / frequency

        # get the intervals in parts (one for each stimulus)
        diode_flips_per_stim = self.divide_into_stim_running_blocks(all_diode_flips)

        num_diode_flips_per_stim = np.array([len(_) for _ in diode_flips_per_stim])
        # add ` width_ratios=num_diode_flips/min(num_diode_flips)``
        fig, _ = plt.subplots(
            1,
            len(diode_flips_per_stim),
            sharey=True,
            gridspec_kw={
                "width_ratios": num_diode_flips_per_stim / min(num_diode_flips_per_stim)
            },
        )
        fig.suptitle(
            f"diode-measured sync-square flip intervals, {expected_period = } s"
        )
        y_deviations_from_expected_period: list[float] = []
        for idx, (ax, d) in enumerate(zip(fig.axes, diode_flips_per_stim)):
            # add horizontal line at expected period
            ax.axhline(expected_period, linewidth=0.5, c="k", linestyle="--", alpha=0.3)
            plt.sca(ax)
            intervals = np.diff(d)
            times = np.diff(d) / 2 + d[:-1]  # plot at mid-point of interval
            markerline, stemline, baseline = plt.stem(
                times, intervals, bottom=expected_period
            )
            plt.setp(stemline, linewidth=0.5, alpha=0.3)
            plt.setp(markerline, markersize=0.5, alpha=0.8)
            plt.setp(baseline, visible=False)

            y_deviations_from_expected_period.append(max(intervals - expected_period))
            y_deviations_from_expected_period.append(max(expected_period - intervals))
            if len(fig.axes) > 1:
                ax.set_title(f"stim {idx}", fontsize=8)
            ax.set_xlabel("time (s)")
            if idx == 0:
                ax.set_ylabel("flip interval (s)")
            ax.set_xlim(min(d) - 20, max(d) + 20)

        for ax in fig.axes:
            # after all ylims are established
            ax.set_ylim(
                bottom=max(
                    0,
                    expected_period - np.max(np.abs(y_deviations_from_expected_period)),
                ),
            )
            ticks_with_period = sorted(set(ax.get_yticks()) | {expected_period})
            ax.set_yticks(ticks_with_period)
            if idx == 0:
                ax.set_yticklabels([f"{_:.3f}" for _ in ticks_with_period])
        fig.set_layout_engine("tight")

        return fig

    def close(self) -> None:
        """
        Closes the dataset.
        """
        self.dfile.close()

    def __enter__(self) -> Self:
        """
        So we can use context manager (with...as) like any other open file.

        Examples
        --------
        >>> with Dataset('my_data.h5') as d: # doctest: +SKIP
        ...     d.stats()

        """
        return self

    def __exit__(self, type, value, traceback) -> None:
        """
        Exit statement for context manager.
        """
        self.close()


def reshape_into_blocks(
    indices: Sequence[float] | npt.NDArray[np.floating],
    min_gap: int | float | None = None,
) -> tuple[npt.NDArray[np.floating], ...]:
    """
    Find the large gaps in indices and split at each gap.

    For example, if two blocks of stimuli were recorded in a single sync
    file, there will be one larger-than normal gap in frame timestamps.

    - default min gap threshold: median + 3 * std (won't work well for short seqs)

    >>> reshape_into_blocks([0, 1, 2, 103, 104, 105], min_gap=100)
    (array([0, 1, 2]), array([103, 104, 105]))

    >>> reshape_into_blocks([0, 1, 2, 3])
    (array([0, 1, 2, 3]),)
    """
    indices = np.array(indices)
    intervals = np.diff(indices)
    long_interval_threshold = (
        min_gap
        if min_gap is not None
        else (np.median(intervals) + 3 * np.std(intervals))
    )

    gaps_between_blocks = []
    for interval_index, interval in zip(
        intervals.argsort()[::-1], sorted(intervals)[::-1]
    ):
        if interval > long_interval_threshold:
            # large interval found
            gaps_between_blocks.append(interval_index + 1)
        else:
            break

    if not gaps_between_blocks:
        # a single block of timestamps
        return (np.array(indices),)

    # create blocks as intervals [start:end]
    gaps_between_blocks.sort()
    blocks = []
    start = 0
    for end in gaps_between_blocks:
        blocks.append(indices[start:end])
        start = end
    # add end of last block
    blocks.append(indices[start:])

    # filter out blocks with a single sample (not a block)
    blocks = [block for block in blocks if len(block) > 1]

    # filter out blocks with long avg timstamp interval (a few, widely-spaced timestamps)
    blocks = [
        block for block in blocks if np.median(np.diff(block)) < long_interval_threshold
    ]

    return tuple(blocks)


def adjust_diode_flip_intervals(
    diode_flips: npt.NDArray[np.floating],
) -> npt.NDArray[np.floating]:
    """
    diode flip intervals have a bimodal distribution due to asymmetry of
    photodiode thresholding: adjust every other interval to get a closer
    estimate of actual transition time for each diode flip
    """
    diode_flips = np.array(diode_flips)  # make a copy
    original_intervals = np.diff(diode_flips)
    outliers = np.logical_or(
        np.percentile(original_intervals, 5) > original_intervals,
        original_intervals > np.percentile(original_intervals, 95),
    )
    intervals = original_intervals[~outliers]

    # the two distributions are symmetric about the mean
    deviation = np.mean(np.abs(intervals - np.mean(intervals)))
    if np.mean(original_intervals[0::2]) > np.mean(original_intervals[1::2]):
        sign = 1
    else:
        sign = -1

    for idx in range(1, len(diode_flips) - 1, 2):
        # alternate on every short/long interval and expand/contract
        # interval
        diode_flips[idx] -= sign * 0.5 * deviation
        diode_flips[idx + 1] += sign * 0.5 * deviation
    return diode_flips


def add_missing_diode_flips_for_truncated_sync(
    diode_flips: npt.NDArray,
    vsyncs: npt.NDArray,
) -> npt.NDArray[np.float64]:
    approx_interval = np.mean(diode_flips - vsyncs[: len(diode_flips)])
    idx = np.searchsorted(vsyncs, diode_flips[-1], side="left")
    while diode_flips[-1] < vsyncs[-1] + approx_interval:
        logger.info(
            "Creating extra diode flip at end of stim block to account for truncated sync recording"
        )
        diode_flips = np.array([*diode_flips, vsyncs[idx - 1] + approx_interval])
        idx += 1
    return diode_flips


def add_missing_diode_flip_at_stim_onset(
    diode_flips: npt.NDArray,
    vsyncs: npt.NDArray,
) -> npt.NDArray[np.float64]:
    """
    Transition from white screen to white sync-square on first frame results
    in a missing diode flip (same for black-to-black).
    Create a flip after the first vsync with an interval
    based on the statistics of other intervals in the
    recording.
    Shouldn't have any side-effects, and the first frame likely contains nothing important.
    """
    logger.info(
        "Creating extra diode flip at start of stim block to account for white-to-white or black-to-black transition"
    )
    common_len = min([len(vsyncs) - 1, len(diode_flips)])
    avg_vsync_to_flip_interval = np.median(
        adjust_diode_flip_intervals(diode_flips[:common_len])
        - vsyncs[1 : common_len + 1]
    )
    # TODO could be more accurate by adding short/long interval as appropriate
    # - diode_flips are pre-adjustment, but we're currently adding the expected
    #   post-adjustment interval
    t = vsyncs[0] + avg_vsync_to_flip_interval
    assert t < diode_flips[0]
    return np.array([t, *diode_flips])


def discard_erroneous_diode_flips_at_stim_offset(
    diode_flips: npt.NDArray, vsyncs: npt.NDArray
) -> npt.NDArray[np.float64]:
    """
    - after a stimulus the screen usually turns grey, causing
    the diode to flip at least one more time after all stim
    vsyncs are finished.
    - the grey screen itself may be on the threshold of diode activation, causing
    multiple additional flips

    remove the last flip if doesn't look time-locked with a
    vsync (ie. vsync-to-flip interval is an outlier)
    """
    while len(diode_flips[diode_flips > vsyncs[-1]]) > 1 and len(diode_flips) > len(
        vsyncs
    ):
        diode_flips = diode_flips[:-1]
    return diode_flips


def get_frame_display_times(
    vsync_times: npt.NDArray,
    on_flip_times: npt.NDArray,
    vsyncs_per_flip: int,
    is_first_frame_off: bool | None = None,
    off_flip_times: npt.NDArray | None = None,
    adjust_dropped_frames: bool = True,
    check_lengths: bool = True,
    check_vsyncs: bool = True,
    stim_id: str | None = None,
) -> npt.NDArray[np.float64]:
    if vsyncs_per_flip == 1:
        # default to using ON flip times (half length of vsyncs) upsampled 2x to put OFF frame halfway between ON
        # frames
        # - if ON flip interval is longer than two screen refreshes (ie a "dropped" frame), check
        #   whether the two corresponding vsync intervals show one longer and one shorter, then use this
        #   to determine the position of the OFF frame
        # - if the first frame is OFF, we can't adjust the first vsync time: we have to create it

        # some checks to make sure arrays are the expected lengths:
        if is_first_frame_off is None and off_flip_times is None:
            raise ValueError(
                "Either is_first_frame_off or off_flip_times must be provided to get_frame_display_times"
            )
        if is_first_frame_off is None:
            is_first_frame_off = off_flip_times[0] < on_flip_times[0]
        if off_flip_times is not None:
            is_last_frame_on = on_flip_times[-1] > off_flip_times[-1]
        elif len(vsync_times) % 2:
            is_last_frame_on = not is_first_frame_off
        else:
            is_last_frame_on = is_first_frame_off
        if check_lengths:
            # lengths aren't actually critical for the function to work
            assert (
                len(vsync_times)
                == len(on_flip_times) * 2 + is_first_frame_off - is_last_frame_on
            )
        if off_flip_times is not None:
            assert (
                len(off_flip_times)
                == len(on_flip_times) + is_first_frame_off - is_last_frame_on
            )
            assert (off_flip_times[0] < on_flip_times[0]) == is_first_frame_off
            assert (on_flip_times[-1] > off_flip_times[-1]) == is_last_frame_on
        if check_lengths:
            display_times = np.full(len(vsync_times), np.nan)
        else:
            display_times = np.full(
                len(on_flip_times) * 2 + is_first_frame_off - is_last_frame_on, np.nan
            )

        on_intervals = np.diff(on_flip_times)

        # Create OFF frame times as midpoints between ON frames
        fake_off_flip_times = on_flip_times[:-1] + 0.5 * on_intervals
        if not is_last_frame_on:
            # the last OFF flip happens after the last ON flip - currently missing a time value
            fake_off_flip_times = np.append(
                fake_off_flip_times, fake_off_flip_times[-1] + on_intervals[-1]
            )

        if is_first_frame_off:
            fake_off_flip_times = np.insert(
                fake_off_flip_times, 0, fake_off_flip_times[0] - on_intervals[0]
            )

        if adjust_dropped_frames:
            if off_flip_times is None:
                raise ValueError(
                    "off_flip_times must be provided if adjust_dropped_frames is True"
                )

            median_refresh_interval = np.median(on_intervals) / 2

            for idx in np.where(on_intervals > 2.5 * median_refresh_interval)[0]:

                # get the off frame idx we're going to adjust:
                off_frame_idx = idx + is_first_frame_off

                # check for correctness so we can raise in debugger and examine values from previous loop:
                if is_first_frame_off:
                    assert all(
                        (
                            on_flip_times[:off_frame_idx]
                            - fake_off_flip_times[:off_frame_idx]
                        )
                        > 0
                    ), "Adjusting OFF flips in previous loop has disturbed order of ON-OFF sequences"
                else:
                    assert all(
                        (
                            fake_off_flip_times[:off_frame_idx]
                            - on_flip_times[:off_frame_idx]
                        )
                        > 0
                    ), "Adjusting OFF flips in previous loop has disturbed order of ON-OFF sequences"

                n_refresh_in_interval: int = round(
                    on_intervals[idx] / median_refresh_interval
                )
                assert (
                    n_refresh_in_interval >= 3
                ), f"should only be considering long intervals here (regular intervals are 2): {n_refresh_in_interval=}"

                #!
                # TODO check that we're considering frames that are dropped for more than one refresh
                # if there's more than one and we don't do it correctly we'll create a spurious long
                # interval following the real dropped interval
                #!

                ## try just using position of OFF flip - ignore vsyncs
                frames_to_off = (
                    off_flip_times[off_frame_idx] - on_flip_times[idx]
                ) / median_refresh_interval
                n_frames_to_off = asym_round(frames_to_off, 0.9, allow_zero=False)
                assert n_frames_to_off >= 1, "Trying to place OFF at ON frame"
                assert (
                    n_frames_to_off < n_refresh_in_interval
                ), "Trying to place OFF beyond ON-ON interval"
                fake_off_flip_times[off_frame_idx] = on_flip_times[
                    idx
                ] + n_frames_to_off * (on_intervals[idx] / n_refresh_in_interval)

        # Fill frame times
        display_times[0::2] = (
            fake_off_flip_times if is_first_frame_off else on_flip_times
        )
        display_times[1::2] = (
            on_flip_times if is_first_frame_off else fake_off_flip_times
        )

        common_len = min(len(vsync_times), len(display_times))
        if get_plot_dir():
            import matplotlib.pyplot as plt

            stim_dir = get_plot_dir() / f'{stim_id or "stim"}'
            stim_dir.mkdir(parents=True, exist_ok=True)

            plt.figure()
            plt.plot(display_times[:common_len] - vsync_times[:common_len])
            plt.title(f"display times minus vsync times\n{stim_id=}")
            plt.xlabel("frame idx")
            plt.ylabel("display time - vsync time (s)")
            plt.savefig(stim_dir / "display_times_minus_vsyncs.png")
            plt.close()

            plt.figure()
            plt.hist(np.diff(vsync_times), label="vsyncs", bins=100, log=True)
            plt.hist(np.diff(display_times), label="display_times", bins=100, log=True)
            plt.legend()
            plt.xlim(0.0, median_refresh_interval * 3.5)
            plt.xlabel("interval (s)")
            plt.ylabel("count")
            plt.title(f"vsyncs and display times intervals\n{stim_id=}")
            plt.savefig(stim_dir / "vsync_display_time_hist.png")
            plt.close()

        long_len = 2  # frames
        if (
            adjust_dropped_frames
            and check_vsyncs
            and "spontaneous" not in (stim_id or "").lower()
        ):  # quiescent frame timings don't matter
            long_thr = long_len - 0.9
            long_display_time_idx = np.where(
                np.diff(display_times) > long_thr * median_refresh_interval
            )[0]
            # vsyncs should be long at these same idx
            # - vsyncs intervals aren't discretized, so it's difficult to find a threshold for a dropped frame
            # - just check the vsyncs that correspond to lon display times (which are discretized)
            all_vsync_diffs = np.diff(vsync_times)
            # all_vsync_diffs =  script_frame_intervals #! <---- if unsure about vsyncs can check against script intervals - so far this always results in the same offsets (eg long intervals are synced between script and vsyncs)
            long_vsync_misalignment = []
            corresponding_long_vsyncs = []
            max_shift = 6
            for idx in long_display_time_idx:
                shifts = list(range(0, -max_shift, -1)) + list(range(1, max_shift))
                shifts = [
                    shift
                    for shift in shifts
                    if (idx + shift > 0) and (idx + shift < len(all_vsync_diffs))
                ]
                shifted_vsync_diffs = [all_vsync_diffs[idx + shift] for shift in shifts]
                shift_idx = np.argmax(shifted_vsync_diffs)
                if (
                    v := shifted_vsync_diffs[shift_idx]
                ) < long_thr * median_refresh_interval:
                    raise IndexError(
                        f"{stim_id}: Could not find long vsync for {idx=} with {max_shift=} either side of long frame display time: signals have become too misaligned"
                    )
                long_vsync_misalignment.append(
                    shifts[shift_idx] * -1
                )  # inv sign to express vsync idx relative to flip idx (ie -1 means long vsync idx is 1 less than long diode flip idx)
                corresponding_long_vsyncs.append(v)
            long_vsync_misalignment = np.array(long_vsync_misalignment)
            if get_plot_dir():
                plt.figure()
                plt.step(long_display_time_idx, long_vsync_misalignment)
                plt.xlabel("frame idx")
                plt.ylabel(
                    "misalignment with vsyncs\n(+ve means index of long flips is shifted to right relative to vsyncs)"
                )
                plt.title(
                    f"long frame display times misalignment with vsyncs\n{stim_id=}"
                )
                plt.savefig(stim_dir / "long_frame_misalignment_with_vsyncs.png")
                plt.close()

            smoothed_long_vsync_misalignment = scipy.signal.medfilt(
                long_vsync_misalignment, kernel_size=3
            )
            # tolerate some misalignment:
            # - we often get a few frames that are misaligned individually, but not sequentially
            # - can't currently explain them, but if they don't cause a systematic shift then we can't correct them
            min_long_interval_num = 3
            if len(long_vsync_misalignment) < min_long_interval_num:
                # check vsyncs or stim file also have few long intervals
                assert (
                    x := len(
                        all_vsync_diffs[
                            all_vsync_diffs > long_thr * median_refresh_interval
                        ]
                    )
                ) < min_long_interval_num * 2, f"{stim_id}: found fewer than {min_long_interval_num} long intervals in photodiode signal, but {x} long intervals in vsync signal"
                pass
            elif len(
                smoothed_long_vsync_misalignment[smoothed_long_vsync_misalignment == 1]
            ) > 0.90 * len(smoothed_long_vsync_misalignment):
                logger.warning(
                    f"All {len(smoothed_long_vsync_misalignment)} long frame display times are misaligned with vsyncs by +1 frame: we have an extra diode flip at the start of the stim block to remove"
                )
                assert (
                    x := display_times[0] - vsync_times[0]
                ) < MIN_VSYNC_DIODE_FLIP_SEPARATION_SEC, f"Expected first display time to have lower than usual latency after first vsync: got {x} s"
                display_times = display_times[1:]  # remove the first frame
            elif len(
                smoothed_long_vsync_misalignment[smoothed_long_vsync_misalignment == -1]
            ) > 0.90 * len(smoothed_long_vsync_misalignment):
                # - first sync square used to be white, which would usually be detected when transitioning from the "initializing" grey screen, so we'd be missing the diode flip
                # corresponding to the first vsync.
                # - this was changed to always be black first on Aug 25 2023: https://github.com/samgale/DynamicRoutingTask/commit/cb6af876f4e42ddef820a8aea0f955faa8dfcb68
                # - it's still possible to miss the first sync square frame when black if the phantom flickering during the initializing grey screen caused a black frame to precede the first frame
                logger.warning(
                    f"All {len(smoothed_long_vsync_misalignment)} long frame display times are misaligned with vsyncs by -1 frame: likely missed a diode flip at the start of the stim block"
                )
                assert (
                    x := (display_times[0] - vsync_times[0])
                ) > 2 * MIN_VSYNC_DIODE_FLIP_SEPARATION_SEC, f"Expected first display time to have higher than usual latency after first vsync: got {x} s"
                display_times = np.insert(
                    display_times, 0, display_times[0] - median_refresh_interval
                )  # add the first frame

            elif len(
                smoothed_long_vsync_misalignment[smoothed_long_vsync_misalignment == 0]
            ) < 0.75 * len(long_vsync_misalignment):
                raise AssertionError(
                    f"{stim_id}: Too many frames are misaligned with vsyncs - check plots."
                )
                # get longest run of non-zero elements (misaligned idx):
                misaligned_idx = np.where(long_vsync_misalignment != 0)[0]
                first_mismatch_idx = [
                    i for i, v in enumerate(long_vsync_misalignment) if v != 0
                ][0]
                first_long_mismatch = long_display_time_idx[first_mismatch_idx]
                last_idx_before_mismatch = (
                    None
                    if first_mismatch_idx == 0
                    else long_display_time_idx[first_mismatch_idx - 1]
                )

        # common_len = min(len(vsync_times), len(display_times))
        # assert np.all(display_times[:common_len] - vsync_times[:common_len] > 0)
        ## ^ at the moment this may be violated (very infrequently, and only on a couple of frames in a block) so it's disabled
    else:
        # photodiode signal changes every N screen refreshes (here N = 3)
        # where frames are dropped, N = 3 + ?
        #    _____________           _____________              _____________
        #    |   :   :   |   :   :   |   :   :   |   :      :   |   :   :   |
        # ___|   :   :   |___:___:___|   :   :   |___:______:___|   :   :   |__
        #                                                ^ 1 "dropped" frame

        # the OFF edge times are unreliable, so first use ON edges only, and discretize OFF edges to
        # be some multiple of screen refreshes from preceding ON edge:
        if on_flip_times[0] > off_flip_times[0]:
            raise NotImplementedError(
                f"{stim_id} | first diode edge in block is falling - this should not be possible with default TaskControl settings."
                "Either the min vsync-diode flip interval needs shortening or we have spontaneous flicker. Needs handling."
            )
        n_on_to_off_refreshes = asym_round(
            (off_flip_times - on_flip_times[: len(off_flip_times)]) / FRAME_INTERVAL,
            threshold=0.25,
            allow_zero=False,
        )
        adjusted_off_flip_times = (
            on_flip_times[: len(off_flip_times)]
            + n_on_to_off_refreshes * FRAME_INTERVAL
        )
        flips = np.sort(np.concatenate([on_flip_times, adjusted_off_flip_times]))

        # - flips wont be exactly N-frames long, but must be longer than (N-1 + some variability)
        #   frames
        # - only check up to last flip, incase it's a rising edge caused by transition to desktop
        assert np.all(
            np.diff(flips[:-1]) > ((vsyncs_per_flip - 0.75) * FRAME_INTERVAL)
        ), f"{stim_id} has diode flip intervals that are too short: {min(np.diff(flips)) * FRAME_INTERVAL=}, {np.mean(np.diff(flips)) * FRAME_INTERVAL=} (expected {vsyncs_per_flip})"

        # now find display times between each rising-falling edge pair
        times = []
        regular_intervals = np.arange(0, 3 * FRAME_INTERVAL, FRAME_INTERVAL)
        for idx, (flip, n_refreshes) in enumerate(
            zip(flips[:-1], np.round(np.diff(flips) / FRAME_INTERVAL))
        ):
            #! last flip will be skipped: append after loop

            n_dropped = n_refreshes - vsyncs_per_flip
            if n_dropped == 0:
                times.extend(flip + regular_intervals)
                continue
            # use corresponding vsync intervals to determine which frame(s) were dropped
            assert (
                n_dropped > 0
            ), f"{stim_id} has diode flip intervals that are too short: {n_refreshes=} (expected {vsyncs_per_flip})"
            vsync_intervals = np.diff(
                vsync_times[idx * vsyncs_per_flip : (idx + 1) * vsyncs_per_flip + 1]
            )

            if n_dropped == 1:
                # if only one frame was dropped, use the single longest vsync interval
                dropped_idx = np.argmax(vsync_intervals)
                t = flip
                for i in range(vsyncs_per_flip):
                    t += FRAME_INTERVAL
                    if i == dropped_idx:
                        t += FRAME_INTERVAL
                    times.append(t)
                continue

            # figure out how many frames were dropped,
            # and how many screen refreshes dropped for each
            long_thr = 1.1 * FRAME_INTERVAL
            long_idx = np.where(vsync_intervals > long_thr)[0]
            long_refreshes = asym_round(
                vsync_intervals[long_idx] / FRAME_INTERVAL,
                threshold=0.1,
                allow_zero=False,
            )
            # long_refreshes is number of screen refreshes for each dropped frame
            # the total should complement the number of other non-dropped frames (which have one screen refresh each):
            assert (
                sum(long_refreshes) + (vsyncs_per_flip - long_idx.size) == n_refreshes
            ), f"{stim_id} | Faulty locating of dropped frames - something doesn't add up: {vsync_intervals=}, {long_refreshes=} ({long_thr=})"
            t = flip
            for i in range(vsyncs_per_flip):
                t += FRAME_INTERVAL
                if i == long_idx:
                    t += FRAME_INTERVAL * long_refreshes[list(long_idx).index(i)]
                times.append(t)
        # add the last flip, which we necessarily missed out in the for-loop above:
        times.append(flips[-1])

        # at this point, we could have up to vsync_intervals -1 frames not enclosed by a diode flip at
        # the end of the experiment, for example:
        #    _____________           _____________________ <-- display stays bright after last frame
        #    |   :   :   |   :   :   |   :   :
        # ___|   :   :   |___:___:___|   :   :
        n_missing = len(vsync_times) - len(times)
        # the last n_missing vsyncs would also have to occur after the current last diode flip time:
        min_latency = np.min(times - vsync_times[: len(times)])
        is_missing_from_end = np.all(
            vsync_times[-n_missing:] > (times[-1] + min_latency)
        )
        if 0 > n_missing < (vsyncs_per_flip - 1) and is_missing_from_end:
            for _ in range(n_missing):
                times.append(times[-1] + FRAME_INTERVAL)

    assert not np.any(np.isnan(display_times))
    assert np.all(np.diff(display_times) > 0)

    return display_times


def is_part_of_normal_pair(
    prev_interval_dur, suspicious_interval_dur, next_interval_dur
) -> bool:
    """
    Normal pairs sum to 2 frames (without any dropped frames), and typically alternate long-short intervals:
    >>> is_part_of_normal_pair(1.3 / 59.95, 0.7 / 59.95, 1.3 / 59.95)
    True

    Should be robust to short intervals on either side of the suspicious interval:
    >>> is_part_of_normal_pair(0.05 / 59.95, 0.7 / 59.95, 1.3 / 59.95)
    True
    >>> is_part_of_normal_pair(1.3 / 59.95, 0.7 / 59.95, 0.05 / 59.95)
    True

    Should allow for an integer number of frames to accommodate dropped frames:
    >>> is_part_of_normal_pair(2.3 / 59.95, 0.7 / 59.95, 2.3 / 59.95)
    True
    >>> is_part_of_normal_pair(1.3 / 59.95, 1.7 / 59.95, 0.1 / 59.95)
    True

    Must detect anomalous intervals that don't sum to an integer number of frames:
    >>> is_part_of_normal_pair(1.3 / 59.95, 0.01 / 59.95, 0.7 / 59.95)
    False

    Must detect anomalous intervals that don't sum to an integer number of frames:
    >>> is_part_of_normal_pair(2.3 / 59.95, 0.01 / 59.95, 0.7 / 59.95)
    False

    Must be robust to short intervals on either side of the suspicious interval:
    >>> is_part_of_normal_pair(1.3 / 59.95, 0.01 / 59.95, 0.01 / 59.95)
    False
    >>> is_part_of_normal_pair(0.01 / 59.95, 0.01 / 59.95, 1.3 / 59.95)
    False
    """
    if (suspicious_interval_dur * FRAME_RATE) < DEFINITE_SHORT_INTERVAL_THRESHOLD:
        # if the suspicious interval is extremely short, it can't be real
        return False

    result = False
    for other in (prev_interval_dur, next_interval_dur):
        if other is None:
            # if there's no next or previous interval, we can't check the pair
            continue
        interval_pair_sum = suspicious_interval_dur + other

        # if the sum of the two intervals is less than 2 frames it's anomalous
        if interval_pair_sum < (2 * FRAME_INTERVAL) - SUSPICIOUS_INTERVAL_THRESHOLD:
            continue

        # if the sum of the two intervals is not close to an integer number of frames, it's  anomalous
        if round(interval_pair_sum * FRAME_RATE, 1) % 1 != 0:
            continue

        # after checking previous, we can assume the pair is normal
        result = True
        break

    return result


def anomalous_interval_indices(flips, num_vsyncs_per_diode_flip: float):
    intervals = np.diff(flips)
    short_intervals = []
    for idx in sorted(
        np.where(
            intervals
            < ((num_vsyncs_per_diode_flip - 1) * FRAME_INTERVAL)
            + SUSPICIOUS_INTERVAL_THRESHOLD
        )[0]
    ):
        if idx in short_intervals:
            continue
        # a short blip on the line must occur as an ON-OFF pair, due to the way sync detects rising/falling edges (e.g. cannot have to rising)
        # ie both indices must be present, so we can delete the first one

        if num_vsyncs_per_diode_flip == 1 and is_part_of_normal_pair(
            prev_interval_dur=intervals[idx - 1] if idx - 1 >= 0 else None,
            suspicious_interval_dur=intervals[idx],
            next_interval_dur=intervals[idx + 1] if idx + 1 < len(intervals) else None,
        ):
            # if the short interval actually sums to an integer number of intervals with one of its neighbours, it's not anomalous
            continue
        short_intervals.append(idx)
    short_intervals = np.sort(short_intervals)
    return short_intervals


def asym_round(
    x: npt.ArrayLike, threshold: float = 0.5, allow_zero: bool = False
) -> npt.NDArray[np.int_]:
    """
    For figuring out how many frames *should* have been displayed given a noisy interval.

    Like a regular round, but rounding up from >=`threshold` instead of the usual 0.5.
    Anything that would become zero is pushed to 1 if `allow_zero` is False (default, since 0 frames
    isn't possible).

    >>> asym_round([0.1, 1.19, 1.5, 2.21], threshold=0.2, allow_zero=False)
    array([1, 1, 2, 3])

    """
    x = np.asarray(x)
    if allow_zero:
        lower_values = np.floor(x)
    else:
        # take greater of floor(x) and 1, to avoid rounding to zero
        lower_values = np.maximum(np.floor(x), np.ones_like(x))
    # avoid using x - np.floor(x) which will incur floating point errors:
    return np.where(x < np.floor(x) + threshold, lower_values, np.ceil(x)).astype(int)


if __name__ == "__main__":
    SyncDataset(
        "//allen/programs/mindscope/workgroups/dynamicrouting/PilotEphys/Task 2 pilot/DRpilot_796848_20250716/20250716T124108.h5"
    ).frame_display_time_blocks
    from npc_sync import testmod

    testmod()
