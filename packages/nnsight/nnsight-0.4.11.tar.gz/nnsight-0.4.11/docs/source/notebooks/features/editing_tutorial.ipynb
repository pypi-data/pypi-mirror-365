{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edit module sets default nodes on the intervention graph to be executed on every future trace. Let's start by loading and dispatching a LanguageModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/autointerp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing is useful for attaching default modules to the graph such as LoRAs or SAEs. We declare a toy, passthrough SAE class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a simple torch module\n",
    "class SAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attach a module to a model's tree, simply set it as an attribute on a desired module. Note that edits must be of type `torch.nn.Module` in order to be attached to the tree. \n",
    "\n",
    "To set a default edit on a model's intervention graph, create an `edit` context and declare operations as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reference to the l0 Envoy\n",
    "submodule = model.transformer.h[0]\n",
    "# Set the SAE as a property on .sae\n",
    "submodule.sae = SAE()\n",
    "\n",
    "# Declare an edit context like you would a trace\n",
    "with model.edit(\"\"):\n",
    "    acts = submodule.output[0]\n",
    "    submodule.sae(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `.sae` attribute in future `trace` contexts will return the `l0` output as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"Hello, world!\"):\n",
    "    acts = submodule.sae.output.save()\n",
    "\n",
    "print(acts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also hook into submodules of attached modules. Let's edit the `SAE` class to include a passthrough `encoder` and `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Coder, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        self.encoder = Coder()\n",
    "        self.decoder = Coder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(\n",
    "            self.encoder(x)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the edit just as before, this time setting the `hook` kwarg to `True`. This tells NNsight that we'd like to call the `forward` method on the `SAE` module, passing inputs through all subhooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Create a reference to the l0 Envoy\n",
    "submodule = model.transformer.h[0]\n",
    "# Set the SAE as a property on .sae\n",
    "submodule.sae = SAE()\n",
    "\n",
    "# Declare an edit context like you would a trace\n",
    "with model.edit(\"\"):\n",
    "    acts = submodule.output[0]\n",
    "    submodule.sae(acts, hook=True)\n",
    "\n",
    "# Now we can call .encoder and other submodules!\n",
    "with model.trace(\"Hello, world!\"):\n",
    "    acts = submodule.sae.encoder.output.save()\n",
    "\n",
    "print(acts.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autointerp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
