{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d129fe7-5a5f-441f-bb3f-288073f30c44",
   "metadata": {},
   "source": [
    "# Multiple Token Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39793d68-c96e-4069-82a3-31c283eb08eb",
   "metadata": {},
   "source": [
    "When generating more than one token, use `<module>.next()` to denote following interventions should be applied to the subsequent generations for that module.\n",
    "\n",
    "Here we generate three tokens and save the hidden states of the last layer for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032125c8-3d90-4671-bd66-4cbbf090c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel('openai-community/gpt2', device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4040a6",
   "metadata": {},
   "source": [
    "## `.generate()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5077f6",
   "metadata": {},
   "source": [
    "NNsight's `LanguageModel` class supports multiple token generation with `.generate()`. You can control the number of new tokens generated by setting `max_new_tokens = N` within your call to `.generate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f91ee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  The Eiffel Tower is in the city of\n",
      "Generated Answer:   Paris, and\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "n_new_tokens = 3\n",
    "with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    out = model.generator.output.save()\n",
    "\n",
    "decoded_prompt = model.tokenizer.decode(out[0][0:-n_new_tokens].cpu())\n",
    "decoded_answer = model.tokenizer.decode(out[0][-n_new_tokens:].cpu())\n",
    "\n",
    "print(\"Prompt: \", decoded_prompt)\n",
    "print(\"Generated Answer: \", decoded_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983f255",
   "metadata": {},
   "source": [
    "## `.next()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6467e6",
   "metadata": {},
   "source": [
    "When generating more than one token, use `<module>.next()` to denote following interventions should be applied to the subsequent generations for that module.\n",
    "\n",
    "Here we generate three tokens and save the hidden states of the last layer for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e67bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "n_new_tokens = 3\n",
    "with model.generate('The Eiffel Tower is in the city of', max_new_tokens=n_new_tokens) as tracer:\n",
    "\n",
    "    hidden_states1 = model.transformer.h[-1].output[0].save()\n",
    "\n",
    "    hidden_states2 = model.transformer.h[-1].next().output[0].save()\n",
    "\n",
    "    hidden_states3 = model.transformer.h[-1].next().output[0].save()\n",
    "\n",
    "    out = model.generator.output.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb378f7b",
   "metadata": {},
   "source": [
    "Note how calling save before `tracer.next()` returns the hidden state across the initial prompt while calling save after returns the hidden state of each subsequent generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a342a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "tensor([[ 464,  412,  733,  417, 8765,  318,  287,  262, 1748,  286, 6342,   11,\n",
      "          290]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states1.shape) # hidden states across prompt & first generated token\n",
    "print(hidden_states2.shape) # only hidden states across next token\n",
    "print(hidden_states3.shape) # only hidden states across next token\n",
    "print(out) # model output tokens, including prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e12b3",
   "metadata": {},
   "source": [
    "Great, we've now successfully stored hidden states across multiple different rounds of token generation! However, if you're generating many tokens while applying interventions, using `.next()` requires you to set a loop within the tracing context, which can be clunky:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "150ce46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state length:  50\n"
     ]
    }
   ],
   "source": [
    "# Old approach:\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = model.transformer.h\n",
    "n_new_tokens = 50\n",
    "hidden_states = []\n",
    "with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    for i in range(n_new_tokens):\n",
    "        # Apply intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(layers[-1].output.save())\n",
    "\n",
    "        # Move to next generated token\n",
    "        layers[0].next()\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205876d",
   "metadata": {},
   "source": [
    "## `.all()` streamlines interventions on many generated tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64193814",
   "metadata": {},
   "source": [
    "With `nnsight 0.4` you can use `.all()` to recursively apply interventions to a model. Calling `.all()` on a module within a model will recursively apply its `.input` and `.output` across all iterations. Previously, we'd need to loop across each new generated token, saving the intervention for every generated token and calling `.next()` to move forward, as demonstrated in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f5d186",
   "metadata": {},
   "source": [
    "Let's try using `.all()` to streamline the multiple token generation process. We simply call `.all()` on the module where we are applying the intervention (in this case GPT-2's layers), apply our intervention, and append our hidden states (stored in an `nnsight.list()` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5554a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state length:  50\n"
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "# using .all():\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = model.transformer.h\n",
    "n_new_tokens = 50\n",
    "with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list\n",
    "\n",
    "    # Call .all() to apply intervention to each new token\n",
    "    with layers.all():\n",
    "\n",
    "        # Apply intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(layers[-1].output) # no need to call .save\n",
    "        # Don't need to loop or call .next()!\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc276fc2",
   "metadata": {},
   "source": [
    "Easy! Note that because `.all()` is recursive, it will only work to append outputs called on children of the module that `.all()` was called on. See example below for more information. TL;DR: apply `.all()` on the highest-level accessed module if interventions and outputs have different hierarchies within model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0d5fa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Recursive properties of .all()</summary>\n",
    "\n",
    "`.all()` recursively acts on model components. In the below code example, only the first token generation is saved, because `.all()` applied to `layers`, while the saved variable `hidden_states` is produced from `model.lm_head`, which is not a child of `layers`.\n",
    "\n",
    "```\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = model.transformer.h\n",
    "n_new_tokens = 3\n",
    "with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list\n",
    "\n",
    "    # Call .all() on layers\n",
    "    with layers.all():\n",
    "\n",
    "        # Apply same intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(model.lm_head.output) # no need to call .save, it's already initialized\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states)) # length is 1, meaning it only saved the first token generation\n",
    "```\n",
    "\n",
    "If you want to apply an intervention during multiple token generation while saving the state of a model component that isn't a child of that module, you can instead apply `.all()` to the full model:\n",
    "\n",
    "```\n",
    "prompt = 'The Eiffel Tower is in the city of'\n",
    "layers = model.transformer.h\n",
    "n_new_tokens = 3\n",
    "with model.generate(prompt, max_new_tokens=n_new_tokens) as tracer:\n",
    "    hidden_states = nnsight.list().save() # Initialize & .save() nnsight list\n",
    "\n",
    "    # Call .all() on model\n",
    "    with model.all():\n",
    "\n",
    "        # Apply same intervention - set first layer output to zero\n",
    "        layers[0].output[0][:] = 0\n",
    "\n",
    "        # Append desired hidden state post-intervention\n",
    "        hidden_states.append(model.lm_head.output) # no need to call .save\n",
    "\n",
    "print(\"Hidden state length: \",len(hidden_states)) # length is 3, as expected!\n",
    "```\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
