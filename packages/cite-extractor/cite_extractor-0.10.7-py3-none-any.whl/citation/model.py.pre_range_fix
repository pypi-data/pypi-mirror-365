import dspy
import logging
from typing import Dict, Optional, List
import fitz  # PyMuPDF
from .llm import get_llm_model

import re

class ImprovedPageNumberExtractor:
    """Enhanced page number extraction with pattern recognition and position consistency"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def extract_number_from_text(self, text: str) -> Optional[int]:
        """Extract page numbers using comprehensive patterns with priority order"""
        if not text:
            return None
        
        text = text.strip()
        if not text:
            return None
        
        # Patterns ordered by priority - most specific first
        patterns = [
            r'第\s*(\d+)\s*[页頁][，,]\s*共\s*\d+\s*[页頁]',  # "第 1 頁，共 20 頁"
            r'[·•∙・\-]\s*(\d+)\s*[·•∙・\-]',               # "·190·" or "•191•" - HIGH PRIORITY
            r'第\s*(\d+)\s*[页頁]',                            # "第1页" or "第 1 頁" 
            r'([1-9]\d*)\s*[页頁]',                           # "1页" or "123 頁"
            r'[页頁]\s*([1-9]\d*)',                           # "页1" or "頁 123"
            r'[pP]age\s+([1-9]\d*)',                         # "Page 123"
            r'[pP]\.?\s*([1-9]\d*)',                         # "p. 123" or "P.123"
            r'([1-9]\d*)ページ',                             # "123ページ"
            r'[\[\(]([1-9]\d*)[\]\)]',                       # "[123]" or "(123)"
            r'^([1-9]\d*)$',                                 # Pure number: "123" - LOWEST PRIORITY
            r'^([ivxlcdmIVXLCDM]+)$',                        # Roman numerals
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    value = match.group(1)
                    if value.isdigit():
                        num = int(value)
                        if 1 <= num <= 9999:  # Reasonable page number range
                            return num
                except (ValueError, IndexError):
                    continue
        
        return None
    

    def extract_total_pages_from_text(self, text: str) -> Optional[int]:
        """Extract total page count from patterns like '第 X 頁，共 Y 頁'"""
        if not text:
            return None
        
        # Pattern to extract total pages: "共 20 頁" 
        total_pattern = r'共\s*(\d+)\s*[页頁]'
        match = re.search(total_pattern, text, re.IGNORECASE)
        if match:
            try:
                total = int(match.group(1))
                if 1 <= total <= 9999:  # Reasonable range
                    return total
            except (ValueError, IndexError):
                pass
        
        return None

    def extract_text_by_position(self, page, position_type="footer"):
        """Extract text from specific positions (header/footer)"""
        page_rect = page.rect
        page_height = page_rect.height
        page_width = page_rect.width
        
        # Define position areas
        if position_type == "footer":
            # Bottom 10% of page
            search_rect = fitz.Rect(0, page_height * 0.9, page_width, page_height)
        elif position_type == "header":
            # Top 10% of page
            search_rect = fitz.Rect(0, 0, page_width, page_height * 0.1)
        else:
            # Full page
            search_rect = page_rect
        
        # Get text blocks in the specified area
        text_blocks = page.get_text("dict", clip=search_rect)["blocks"]
        
        position_texts = []
        for block in text_blocks:
            if "lines" in block:
                for line in block["lines"]:
                    line_text = ""
                    for span in line["spans"]:
                        line_text += span["text"]
                    if line_text.strip():
                        # Calculate relative position (left, center, right)
                        bbox = line["bbox"]
                        center_x = (bbox[0] + bbox[2]) / 2
                        if center_x < page_width * 0.33:
                            position = "left"
                        elif center_x > page_width * 0.67:
                            position = "right"
                        else:
                            position = "center"
                        
                        position_texts.append({
                            "text": line_text.strip(),
                            "position": position,
                            "bbox": bbox
                        })
        
        return position_texts
    
    def find_continuous_page_sequence(self, pdf_path: str, max_pages: int = 5) -> Dict[int, int]:
        """Find continuous page number sequences with consistent positioning"""
        doc = fitz.open(pdf_path)
        total_pages = min(doc.page_count, max_pages)
        
        # Collect all potential page numbers from each page
        page_candidates = {}
        
        for page_idx in range(total_pages):
            page = doc[page_idx]
            candidates = []
            
            # Check both header and footer
            for position_type in ["footer", "header"]:
                position_texts = self.extract_text_by_position(page, position_type)
                
                for text_info in position_texts:
                    page_num = self.extract_number_from_text(text_info["text"])
                    if page_num is not None:
                        candidates.append({
                            "page_num": page_num,
                            "position_type": position_type,
                            "position": text_info["position"],
                            "text": text_info["text"],
                            "bbox": text_info["bbox"]
                        })
            
            page_candidates[page_idx] = candidates
        
        doc.close()
        
        # Find the best continuous sequence with consistent positioning
        best_sequence = self._find_best_sequence(page_candidates)
        
        return best_sequence
    
    def _find_best_sequence(self, page_candidates: Dict[int, List[Dict]]) -> Dict[int, int]:
        """Find the best continuous sequence with consistent positioning"""
        if len(page_candidates) < 2:
            return {}
        
        import itertools
        
        # Generate all possible combinations
        page_indices = sorted(page_candidates.keys())
        candidate_lists = [page_candidates[page_idx] for page_idx in page_indices]
        
        # Filter out empty candidate lists
        non_empty_indices = []
        non_empty_lists = []
        for i, candidates in enumerate(candidate_lists):
            if candidates:
                non_empty_indices.append(page_indices[i])
                non_empty_lists.append(candidates)
        
        if len(non_empty_lists) < 2:
            return {}
        
        best_sequence = None
        best_score = -1
        
        for combination in itertools.product(*non_empty_lists):
            # Check position consistency
            if not self._check_position_consistency(combination):
                continue
            
            # Check numerical continuity
            sequence = {}
            for i, candidate in enumerate(combination):
                sequence[non_empty_indices[i]] = candidate["page_num"]
            
            continuity_score = self._score_continuity(sequence)
            position_score = self._score_position_consistency(combination)
            
            total_score = continuity_score + position_score
            
            if total_score > best_score:
                best_score = total_score
                best_sequence = sequence
        
        return best_sequence if best_sequence else {}
    
    def _check_position_consistency(self, combination: List[Dict]) -> bool:
        """Check if page numbers appear in consistent positions"""
        if len(combination) < 2:
            return True
        
        first_candidate = combination[0]
        reference_position_type = first_candidate["position_type"]
        reference_position = first_candidate["position"]
        
        # Allow some tolerance for position consistency
        consistent_position_type = 0
        consistent_position = 0
        
        for candidate in combination:
            if candidate["position_type"] == reference_position_type:
                consistent_position_type += 1
            if candidate["position"] == reference_position:
                consistent_position += 1
        
        # At least 70% should have consistent positioning
        threshold = len(combination) * 0.7
        return (consistent_position_type >= threshold or 
                consistent_position >= threshold)
    
    def _score_continuity(self, sequence: Dict[int, int]) -> float:
        """Score sequence based on numerical continuity (Rule 1)"""
        if len(sequence) < 2:
            return 0
        
        sorted_items = sorted(sequence.items())
        continuity_score = 0
        
        for i in range(len(sorted_items) - 1):
            page_idx1, page_num1 = sorted_items[i]
            page_idx2, page_num2 = sorted_items[i + 1]
            
            expected_diff = page_idx2 - page_idx1
            actual_diff = page_num2 - page_num1
            
            # Perfect continuity: numbers increase by 1 for consecutive pages
            if actual_diff == expected_diff and actual_diff > 0:
                continuity_score += 100
            elif actual_diff == 1:  # Sequential pages
                continuity_score += 90
            elif abs(actual_diff - expected_diff) <= 1:  # Close match
                continuity_score += 50
            else:
                continuity_score -= 30  # Penalty for poor continuity
        
        return continuity_score / (len(sorted_items) - 1) if len(sorted_items) > 1 else 0
    
    def _score_position_consistency(self, combination: List[Dict]) -> float:
        """Score position consistency (Rule 2)"""
        if len(combination) < 2:
            return 0
        
        # Count position types and positions
        position_types = {}
        positions = {}
        
        for candidate in combination:
            pos_type = candidate["position_type"]
            pos = candidate["position"]
            
            position_types[pos_type] = position_types.get(pos_type, 0) + 1
            positions[pos] = positions.get(pos, 0) + 1
        
        # Score based on consistency
        total_candidates = len(combination)
        max_position_type_count = max(position_types.values())
        max_position_count = max(positions.values())
        
        position_type_score = (max_position_type_count / total_candidates) * 50
        position_score = (max_position_count / total_candidates) * 50
        
        return position_type_score + position_score




class CitationLLM:
    """LLM handler for citation extraction using DSPy."""

    def __init__(self, llm_model="ollama/qwen3"):
        """Initialize the LLM."""
        self.llm = get_llm_model(llm_model, temperature=0.1)
        dspy.settings.configure(lm=self.llm)

    def _truncate_text(self, text: str, max_tokens: int = 2048) -> str:
        """Truncate text to a maximum number of tokens."""
        tokens = text.split()
        if len(tokens) > max_tokens:
            return " ".join(tokens[:max_tokens])
        return text

    def extract_book_citation(self, pdf_text: str) -> Dict:
        """Extract citation from book PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, publisher, year, location, editor, translator, volume, series, isbn, doi",
                "Extract citation information from book PDF text. Focus on cover and copyright pages (usually in first 5 pages). "
                "Look for title in the middle and upper part with biggest font size, author usually right under the title. "
                "In copyright page, find publish year and publisher. For Chinese text, extract information similarly. "
                "Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"Book LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with book LLM extraction: {e}")
            return {}

    def extract_thesis_citation(self, pdf_text: str) -> Dict:
        """Extract citation from thesis PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, thesis_type, year, publisher, location, doi",
                "Extract citation information from thesis PDF text. Focus on cover and title pages (usually in first 5 pages). "
                "Look for title in the middle and upper part with biggest font size, author usually right under the title. "
                "Identify if it's a PhD thesis or Master thesis. Publisher should be a university or college. "
                "For Chinese text, extract information similarly. Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"Thesis LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with thesis LLM extraction: {e}")
            return {}

    def extract_journal_citation(self, pdf_text: str) -> Dict:
        """Extract citation from journal PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, container_title, year, volume, issue, page_numbers, isbn, doi",
                "Extract citation information from journal PDF text. Focus on first page header and footer. "
                "Look for title in first line with biggest font size, author usually right under the title. "
                "Find journal name (as container_title), year, volume, and issue number in header or footer of first page. "
                "Page numbers format should be 'start-end' (e.g., '20-41'). "
                "For Chinese text, extract information similarly. Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Convert container_title back to container-title for compatibility
                    if key == "container_title":
                        citation_info["container-title"] = value.strip()
                    else:
                        citation_info[key] = value.strip()

            logging.info(f"Journal LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with journal LLM extraction: {e}")
            return {}

    def extract_bookchapter_citation(self, pdf_text: str) -> Dict:
        """Extract citation from book chapter PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, container_title, editor, publisher, year, location, page_numbers, isbn, doi",
                "Analyze the text from a book chapter and extract its citation metadata. "
                "Identify the following fields: "
                "- title: The title of the chapter itself. "
                "- author: The author(s) of the chapter. "
                "- container-title: The title of the book that contains the chapter. "
                "- editor: The editor(s) of the book, often found near 'edited by'. "
                "- publisher: The publisher of the book. "
                "- year: The publication year of the book. "
                "- page_numbers: The page range of the chapter (e.g., '20-41'). "
                "- location, isbn, doi: If available. "
                "For Chinese text, extract the information similarly. If a field is not found, return 'Unknown'.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Convert container_title back to container-title for compatibility
                    if key == "container_title":
                        citation_info["container-title"] = value.strip()
                    else:
                        citation_info[key] = value.strip()

            logging.info(f"Book chapter LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with book chapter LLM extraction: {e}")
            return {}

    def extract_page_numbers_for_journal_chapter(
        self,
        pdf_path: str,
        page_range: str = "1-5, -3"
    ) -> Dict:
        """
        Enhanced page number extraction using pattern recognition and position consistency.
        
        Args:
            pdf_path: Path to the PDF file
            page_range: Page range to analyze (e.g., "1-5, -3")
        
        Returns:
            Dict with page_numbers field if found
        """
        try:
            # Step 1: Try advanced pattern-based extraction first
            extractor = ImprovedPageNumberExtractor()
            
            # Use the enhanced pattern recognition with max 5 pages for now
            # TODO: Implement full page-range awareness as discussed
            page_sequence = extractor.find_continuous_page_sequence(pdf_path, 5)
            
            if page_sequence:
                # Convert to page range format
                page_numbers = list(page_sequence.values())
                start_page = min(page_numbers)
                
                # Try to extract total page count from any page text
                doc = fitz.open(pdf_path)
                total_pages = None
                for i in range(min(3, doc.page_count)):  # Check first 3 pages for total
                    page = doc[i]
                    for position_type in ["header", "footer"]:
                        position_texts = extractor.extract_text_by_position(page, position_type)
                        for text_info in position_texts:
                            total = extractor.extract_total_pages_from_text(text_info["text"])
                            if total:
                                total_pages = total
                                break
                        if total_pages:
                            break
                    if total_pages:
                        break
                doc.close()
                
                if total_pages and total_pages > start_page:
                    # Use the full document range
                    page_result = f"{start_page}-{total_pages}"
                    logging.info(f"Pattern-based page extraction found full range: {page_result} (from 共 {total_pages} 頁)")
                    return {"page_numbers": page_result}
                elif len(page_numbers) >= 2:
                    # Fallback to detected range
                    end_page = max(page_numbers)
                    page_result = f"{start_page}-{end_page}"
                    logging.info(f"Pattern-based page extraction found sample range: {page_result}")
                    return {"page_numbers": page_result}
                elif len(page_numbers) == 1:
                    # Single page
                    page_result = str(page_numbers[0])
                    logging.info(f"Pattern-based page extraction found single page: {page_result}")
                    return {"page_numbers": page_result}
            
            logging.info("Pattern-based extraction found no continuous sequence, falling back to LLM")
            
            # Step 2: Fallback to LLM-based method if pattern-based fails
            doc = fitz.open(pdf_path)
            if doc.page_count == 0:
                doc.close()
                return {}
            
            # Extract text from strategic pages for LLM analysis
            first_page_text = doc[0].get_text() if doc.page_count > 0 else ""
            second_page_text = doc[1].get_text() if doc.page_count > 1 else ""
            last_page_text = doc[doc.page_count - 1].get_text() if doc.page_count > 0 else ""
            second_to_last_page_text = doc[doc.page_count - 2].get_text() if doc.page_count > 1 else ""
            
            doc.close()
            
            signature = dspy.Signature(
                "first_page_text, second_page_text, last_page_text, second_to_last_page_text -> page_numbers",
                "Determine the page range (e.g., '20-41') for a document. "
                "1. Look for a number in the header or footer of the 'first_page_text'. This is the starting page. "
                "2. If not found, look for a number in the header or footer of the 'second_page_text'. If found, the starting page is that number minus 1. "
                "3. Look for a number in the header or footer of the 'last_page_text'. This is the ending page. "
                "4. If not found, look for a number in the header or footer of the 'second_to_last_page_text'. If found, the ending page is that number plus 1. "
                "If you can determine both a start and end page, return them as 'start-end'. Otherwise, return 'Unknown'.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(
                first_page_text=first_page_text,
                second_page_text=second_page_text,
                last_page_text=last_page_text,
                second_to_last_page_text=second_to_last_page_text,
            )

            citation_info = {}
            if result.page_numbers and result.page_numbers.lower() != "unknown":
                citation_info["page_numbers"] = result.page_numbers.strip()
                logging.info(f"LLM fallback page extraction result: {citation_info}")
            else:
                logging.warning("LLM fallback also failed to extract page numbers")

            return citation_info

        except Exception as e:
            logging.error(f"Error with page number extraction: {e}")
            return {}
            
            # Extract text from strategic pages for LLM analysis
            first_page_text = doc[0].get_text() if doc.page_count > 0 else ""
            second_page_text = doc[1].get_text() if doc.page_count > 1 else ""
            last_page_text = doc[doc.page_count - 1].get_text() if doc.page_count > 0 else ""
            second_to_last_page_text = doc[doc.page_count - 2].get_text() if doc.page_count > 1 else ""
            
            doc.close()
            
            signature = dspy.Signature(
                "first_page_text, second_page_text, last_page_text, second_to_last_page_text -> page_numbers",
                "Determine the page range (e.g., '20-41') for a document. "
                "1. Look for a number in the header or footer of the 'first_page_text'. This is the starting page. "
                "2. If not found, look for a number in the header or footer of the 'second_page_text'. If found, the starting page is that number minus 1. "
                "3. Look for a number in the header or footer of the 'last_page_text'. This is the ending page. "
                "4. If not found, look for a number in the header or footer of the 'second_to_last_page_text'. If found, the ending page is that number plus 1. "
                "If you can determine both a start and end page, return them as 'start-end'. Otherwise, return 'Unknown'.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(
                first_page_text=first_page_text,
                second_page_text=second_page_text,
                last_page_text=last_page_text,
                second_to_last_page_text=second_to_last_page_text,
            )

            citation_info = {}
            if result.page_numbers and result.page_numbers.lower() != "unknown":
                citation_info["page_numbers"] = result.page_numbers.strip()

            logging.info(f"Page number LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with page number LLM extraction: {e}")
            return {}

    def extract_citation_from_text(self, text: str, doc_type: str) -> Dict:
        """Extract citation based on document type after truncating long text."""
        truncated_text = self._truncate_text(text)

        if doc_type == "book":
            return self.extract_book_citation(truncated_text)
        elif doc_type == "thesis":
            return self.extract_thesis_citation(truncated_text)
        elif doc_type == "journal":
            return self.extract_journal_citation(truncated_text)
        elif doc_type == "bookchapter":
            return self.extract_bookchapter_citation(truncated_text)
        else:
            # Default fallback
            logging.warning(f"Unknown document type: {doc_type}, using book extraction")
            return self.extract_book_citation(truncated_text)

    def extract_citation_from_web_markdown(self, markdown_text: str) -> Dict:
        """Extracts citation fields from the markdown content of a webpage."""
        try:
            signature = dspy.Signature(
                "markdown_content -> title, author, date, container_title",
                "Analyze the markdown content of a webpage to extract citation information. "
                "Pay close attention to the main title, author by lines, and publication dates in first 600 words. "
                "The 'container-title' is the name of the overall website. "
                "Also, look for explicit citation hints like 'how to cite', '引用', '引用格式', '格式', '格式如下', '凡例'. "
                "Return 'Unknown' for any fields that cannot be found.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(markdown_content=self._truncate_text(markdown_text))

            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"LLM extraction from web markdown result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with web markdown LLM extraction: {e}")
            return {}

    def parse_search_results(self, search_response: str) -> Dict:
        """Parse the response from a search API to extract citation fields."""
        try:
            signature = dspy.Signature(
                "search_results -> container_title, editor, publisher, year, volume, issue, page_numbers, doi",
                "Parse the provided search engine results to find missing citation information for a book chapter or journal article. "
                "Extract fields like the book/journal title (as container-title), editor, publisher, year, etc. "
                "Return 'Unknown' for any fields that cannot be found.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(search_results=search_response)

            # Convert result to dictionary
            parsed_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Handle key conversion for CSL compatibility
                    if key == "container_title":
                        parsed_info["container-title"] = value.strip()
                    else:
                        parsed_info[key] = value.strip()

            logging.info(f"Parsed search results: {parsed_info}")
            return parsed_info
        except Exception as e:
            logging.error(f"Error parsing search results with LLM: {e}")
            return {}
