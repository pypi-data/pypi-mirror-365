import dspy
import logging
import re
import fitz  # PyMuPDF
from typing import Dict, Optional, List, Tuple
from .llm import get_llm_model


class ImprovedPageNumberExtractor:
    """Enhanced page number extraction with pattern recognition and position consistency"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def extract_number_from_text(self, text: str) -> Optional[int]:
        """Extract page numbers using comprehensive patterns with priority order"""
        if not text:
            return None
        
        text = text.strip()
        if not text:
            return None
        
        # Patterns ordered by priority - most specific first (based on page_extractor.py)
        patterns = [
            r'第\s*(\d+)\s*[页頁][，,]\s*共\s*\d+\s*[页頁]',  # "第 1 頁，共 20 頁"
            r'[·•∙・\-]\s*(\d+)\s*[·•∙・\-]',               # "·190·" or "•191•" - HIGH PRIORITY
            r'第\s*(\d+)\s*[页頁]',                            # "第1页" or "第 1 頁" 
            r'([1-9]\d*)\s*[页頁]',                           # "1页" or "123 頁"
            r'[页頁]\s*([1-9]\d*)',                           # "页1" or "頁 123"
            r'[pP]age\s+([1-9]\d*)',                         # "Page 123"
            r'[pP]\.?\s*([1-9]\d*)',                         # "p. 123" or "P.123"
            r'([1-9]\d*)ページ',                             # "123ページ"
            r'[\[\(]([1-9]\d*)[\]\)]',                       # "[123]" or "(123)"
            r'^([1-9]\d*)$',                                 # Pure number: "123" - LOWEST PRIORITY
            r'^([ivxlcdmIVXLCDM]+)$',                        # Roman numerals
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    value = match.group(1)
                    if value.isdigit():
                        num = int(value)
                        if 1 <= num <= 9999:  # Reasonable page number range
                            return num
                    elif re.match(r'^[ivxlcdmIVXLCDM]+$', value, re.IGNORECASE):
                        # Roman numerals - convert to int if needed
                        return self._roman_to_int(value.upper())
                except (ValueError, IndexError):
                    continue
        
        return None
    
    def _roman_to_int(self, roman: str) -> Optional[int]:
        """Convert roman numerals to integers"""
        roman_values = {
            'I': 1, 'V': 5, 'X': 10, 'L': 50, 
            'C': 100, 'D': 500, 'M': 1000
        }
        
        total = 0
        prev_value = 0
        
        for char in reversed(roman):
            if char not in roman_values:
                return None
            value = roman_values[char]
            if value < prev_value:
                total -= value
            else:
                total += value
            prev_value = value
        
        return total if total > 0 else None
    
    def extract_text_by_position(self, page, position_type="footer"):
        """Extract text from specific positions (header/footer)"""
        page_rect = page.rect
        page_height = page_rect.height
        page_width = page_rect.width
        
        # Define position areas
        if position_type == "footer":
            # Bottom 10% of page
            search_rect = fitz.Rect(0, page_height * 0.9, page_width, page_height)
        elif position_type == "header":
            # Top 10% of page
            search_rect = fitz.Rect(0, 0, page_width, page_height * 0.1)
        else:
            # Full page
            search_rect = page_rect
        
        # Get text blocks in the specified area
        text_blocks = page.get_text("dict", clip=search_rect)["blocks"]
        
        position_texts = []
        for block in text_blocks:
            if "lines" in block:
                for line in block["lines"]:
                    line_text = ""
                    for span in line["spans"]:
                        line_text += span["text"]
                    if line_text.strip():
                        # Calculate relative position (left, center, right)
                        bbox = line["bbox"]
                        center_x = (bbox[0] + bbox[2]) / 2
                        if center_x < page_width * 0.33:
                            position = "left"
                        elif center_x > page_width * 0.67:
                            position = "right"
                        else:
                            position = "center"
                        
                        position_texts.append({
                            "text": line_text.strip(),
                            "position": position,
                            "bbox": bbox
                        })
        
        return position_texts
    
    def find_continuous_page_sequence_with_range(self, pdf_path: str, page_range: str, total_pdf_pages: int) -> Dict[int, int]:
        """
        Find continuous page number sequences respecting the page_range structure.
        
        Args:
            pdf_path: Path to the PDF file
            page_range: Page range string (e.g., "1-5, -3")
            total_pdf_pages: Total number of pages in the PDF
            
        Returns:
            Dict mapping PDF page indices to actual page numbers
        """
        from .utils import parse_page_range
        
        doc = fitz.open(pdf_path)
        
        # Parse the page range into actual page indices
        pages_to_analyze = parse_page_range(page_range, total_pdf_pages)
        if not pages_to_analyze:
            self.logger.warning(f"Invalid page range: {page_range}")
            doc.close()
            return {}
        
        # Separate first part and last part based on the original page range
        first_part_pages = []
        last_part_pages = []
        
        # Parse the page_range string to identify first and last parts
        parts = [part.strip() for part in page_range.split(',') if part.strip()]
        
        for part in parts:
            if part.startswith('-'):
                # Last N pages
                try:
                    last_n = abs(int(part))
                    last_part_indices = list(range(max(0, total_pdf_pages - last_n), total_pdf_pages))
                    # Convert to 1-based and filter with pages_to_analyze
                    last_part_pages.extend([i for i in last_part_indices if (i + 1) in pages_to_analyze])
                except ValueError:
                    continue
            elif '-' in part and not part.startswith('-'):
                # Range like "1-5"
                try:
                    start, end = map(int, part.split('-', 1))
                    range_indices = list(range(start - 1, min(end, total_pdf_pages)))  # Convert to 0-based
                    first_part_pages.extend([i for i in range_indices if (i + 1) in pages_to_analyze])
                except ValueError:
                    continue
            else:
                # Single page
                try:
                    page_idx = int(part) - 1  # Convert to 0-based
                    if 0 <= page_idx < total_pdf_pages and (page_idx + 1) in pages_to_analyze:
                        first_part_pages.append(page_idx)
                except ValueError:
                    continue
        
        # Remove duplicates and sort
        first_part_pages = sorted(set(first_part_pages))
        last_part_pages = sorted(set(last_part_pages))
        
        self.logger.info(f"First part pages (0-based): {first_part_pages}")
        self.logger.info(f"Last part pages (0-based): {last_part_pages}")
        
        # Extract page number candidates for each part
        first_part_sequence = self._extract_sequence_from_pages(doc, first_part_pages)
        last_part_sequence = self._extract_sequence_from_pages(doc, last_part_pages)
        
        doc.close()
        
        # Combine sequences using smart logic
        final_sequence = self._smart_combine_sequences(
            first_part_sequence, last_part_sequence, 
            first_part_pages, last_part_pages, total_pdf_pages
        )
        
        return final_sequence
    
    def _extract_sequence_from_pages(self, doc, page_indices: List[int]) -> Dict[int, int]:
        """Extract continuous page number sequence from specific PDF pages"""
        if not page_indices:
            return {}
        
        # Collect all potential page numbers from the specified pages
        page_candidates = {}
        
        for page_idx in page_indices:
            if page_idx >= doc.page_count:
                continue
                
            page = doc[page_idx]
            candidates = []
            
            # Check both header and footer
            for position_type in ["footer", "header"]:
                position_texts = self.extract_text_by_position(page, position_type)
                
                for text_info in position_texts:
                    page_num = self.extract_number_from_text(text_info["text"])
                    if page_num is not None:
                        candidates.append({
                            "page_num": page_num,
                            "position_type": position_type,
                            "position": text_info["position"],
                            "text": text_info["text"],
                            "bbox": text_info["bbox"]
                        })
            
            page_candidates[page_idx] = candidates
        
        # Find the best continuous sequence with consistent positioning
        return self._find_best_sequence_for_part(page_candidates)
    
    def _find_best_sequence_for_part(self, page_candidates: Dict[int, List[Dict]]) -> Dict[int, int]:
        """Find the best continuous sequence for a part (first or last)"""
        if len(page_candidates) < 2:
            return {}
        
        import itertools
        
        # Generate all possible combinations
        page_indices = sorted(page_candidates.keys())
        candidate_lists = [page_candidates[page_idx] for page_idx in page_indices]
        
        # Filter out empty candidate lists
        non_empty_indices = []
        non_empty_lists = []
        for i, candidates in enumerate(candidate_lists):
            if candidates:
                non_empty_indices.append(page_indices[i])
                non_empty_lists.append(candidates)
        
        if len(non_empty_lists) < 2:
            return {}
        
        best_sequence = None
        best_score = -1
        
        for combination in itertools.product(*non_empty_lists):
            # Check position consistency
            if not self._check_position_consistency(combination):
                continue
            
            # Check numerical continuity
            sequence = {}
            for i, candidate in enumerate(combination):
                sequence[non_empty_indices[i]] = candidate["page_num"]
            
            # Score based on continuity within this part
            continuity_score = self._score_part_continuity(sequence)
            position_score = self._score_position_consistency(combination)
            
            total_score = continuity_score + position_score
            
            if total_score > best_score:
                best_score = total_score
                best_sequence = sequence
        
        return best_sequence if best_sequence else {}
    
    def _score_part_continuity(self, sequence: Dict[int, int]) -> float:
        """Score continuity within a part (requires minimum 2 continuous pages)"""
        if len(sequence) < 2:
            return 0
        
        sorted_items = sorted(sequence.items())
        continuity_score = 0
        continuous_pairs = 0
        
        for i in range(len(sorted_items) - 1):
            page_idx1, page_num1 = sorted_items[i]
            page_idx2, page_num2 = sorted_items[i + 1]
            
            expected_diff = page_idx2 - page_idx1
            actual_diff = page_num2 - page_num1
            
            # Perfect continuity: numbers increase by 1 for consecutive pages
            if actual_diff == expected_diff and actual_diff > 0:
                continuity_score += 100
                continuous_pairs += 1
            elif actual_diff == 1:  # Sequential pages
                continuity_score += 90
                continuous_pairs += 1
            elif abs(actual_diff - expected_diff) <= 1:  # Close match
                continuity_score += 50
            else:
                continuity_score -= 30  # Penalty for poor continuity
        
        # Require at least 2 continuous pages to be valid
        if continuous_pairs == 0:
            return -100  # Invalid sequence
        
        return continuity_score / (len(sorted_items) - 1) if len(sorted_items) > 1 else 0
    
    def _smart_combine_sequences(
        self, 
        first_sequence: Dict[int, int], 
        last_sequence: Dict[int, int], 
        first_pages: List[int], 
        last_pages: List[int], 
        total_pdf_pages: int
    ) -> Dict[int, int]:
        """Smart combination of first and last part sequences with deduction logic"""
        
        # Case 1: Both sequences found
        if first_sequence and last_sequence:
            return self._combine_both_sequences(first_sequence, last_sequence, first_pages, last_pages, total_pdf_pages)
        
        # Case 2: Only first sequence found
        elif first_sequence and not last_sequence:
            return self._extend_first_sequence(first_sequence, first_pages, total_pdf_pages)
        
        # Case 3: Only last sequence found  
        elif not first_sequence and last_sequence:
            return self._extend_last_sequence(last_sequence, last_pages, total_pdf_pages)
        
        # Case 4: No sequences found
        else:
            self.logger.warning("No continuous sequences found in either part")
            return {}
    
    def _combine_both_sequences(
        self, 
        first_sequence: Dict[int, int], 
        last_sequence: Dict[int, int], 
        first_pages: List[int], 
        last_pages: List[int], 
        total_pdf_pages: int
    ) -> Dict[int, int]:
        """Combine both sequences with smart gap validation"""
        
        # Get the ranges from each sequence
        first_page_nums = sorted(first_sequence.values())
        last_page_nums = sorted(last_sequence.values())
        
        first_end = max(first_page_nums)
        last_start = min(last_page_nums)
        
        # Calculate expected gap based on PDF page positions
        first_last_pdf_page = max(first_pages)  # Last PDF page of first part
        last_first_pdf_page = min(last_pages)   # First PDF page of last part
        
        pdf_gap = last_first_pdf_page - first_last_pdf_page
        actual_gap = last_start - first_end
        
        self.logger.info(f"Gap analysis - PDF gap: {pdf_gap}, Actual gap: {actual_gap}")
        
        # Smart combine if gap makes sense (within reasonable range)
        if abs(actual_gap - pdf_gap) <= 2:  # Allow some tolerance
            # Create combined sequence
            combined = first_sequence.copy()
            combined.update(last_sequence)
            self.logger.info(f"Smart combined sequences: {first_end} to {max(last_page_nums)}")
            return combined
        else:
            # Gap too large, return first sequence as it's usually more reliable
            self.logger.warning(f"Gap too large ({actual_gap} vs expected {pdf_gap}), using first sequence")
            return self._extend_first_sequence(first_sequence, first_pages, total_pdf_pages)
    
    def _extend_first_sequence(self, first_sequence: Dict[int, int], first_pages: List[int], total_pdf_pages: int) -> Dict[int, int]:
        """Extend first sequence to estimate full range"""
        if not first_sequence:
            return {}
        
        page_nums = sorted(first_sequence.values())
        start_page = min(page_nums)
        end_page = max(page_nums)
        
        # Deduce the actual first page of the document
        first_pdf_page = min(first_pages)  # 0-based
        if first_pdf_page > 0:
            # Estimate what the actual first page number should be
            estimated_first_page = start_page - first_pdf_page
            self.logger.info(f"Deduced document starts at page: {estimated_first_page}")
            
            # Estimate the last page based on total PDF pages
            estimated_last_page = estimated_first_page + total_pdf_pages - 1
            self.logger.info(f"Deduced document ends at page: {estimated_last_page}")
            
            # Create extended sequence (but return the detected range)
            return first_sequence
        else:
            return first_sequence
    
    def _extend_last_sequence(self, last_sequence: Dict[int, int], last_pages: List[int], total_pdf_pages: int) -> Dict[int, int]:
        """Extend last sequence to estimate full range"""
        if not last_sequence:
            return {}
        
        page_nums = sorted(last_sequence.values())
        start_page = min(page_nums)
        end_page = max(page_nums)
        
        # Deduce the actual last page of the document
        last_pdf_page = max(last_pages)  # 0-based
        remaining_pages = total_pdf_pages - 1 - last_pdf_page
        
        if remaining_pages > 0:
            # Estimate what the actual last page number should be
            estimated_last_page = end_page + remaining_pages
            self.logger.info(f"Deduced document ends at page: {estimated_last_page}")
            
            # Estimate the first page
            estimated_first_page = start_page - (last_pdf_page - (total_pdf_pages - len(last_pages)))
            self.logger.info(f"Deduced document starts at page: {estimated_first_page}")
            
            return last_sequence
        else:
            return last_sequence

    def _find_best_sequence(self, page_candidates: Dict[int, List[Dict]]) -> Dict[int, int]:
        """Find the best continuous sequence with consistent positioning"""
        if len(page_candidates) < 2:
            return {}
        
        import itertools
        
        # Generate all possible combinations
        page_indices = sorted(page_candidates.keys())
        candidate_lists = [page_candidates[page_idx] for page_idx in page_indices]
        
        # Filter out empty candidate lists
        non_empty_indices = []
        non_empty_lists = []
        for i, candidates in enumerate(candidate_lists):
            if candidates:
                non_empty_indices.append(page_indices[i])
                non_empty_lists.append(candidates)
        
        if len(non_empty_lists) < 2:
            return {}
        
        best_sequence = None
        best_score = -1
        
        for combination in itertools.product(*non_empty_lists):
            # Check position consistency
            if not self._check_position_consistency(combination):
                continue
            
            # Check numerical continuity
            sequence = {}
            for i, candidate in enumerate(combination):
                sequence[non_empty_indices[i]] = candidate["page_num"]
            
            continuity_score = self._score_continuity(sequence)
            position_score = self._score_position_consistency(combination)
            
            total_score = continuity_score + position_score
            
            if total_score > best_score:
                best_score = total_score
                best_sequence = sequence
        
        return best_sequence if best_sequence else {}
    
    def _check_position_consistency(self, combination: List[Dict]) -> bool:
        """Check if page numbers appear in consistent positions"""
        if len(combination) < 2:
            return True
        
        first_candidate = combination[0]
        reference_position_type = first_candidate["position_type"]
        reference_position = first_candidate["position"]
        
        # Allow some tolerance for position consistency
        consistent_position_type = 0
        consistent_position = 0
        
        for candidate in combination:
            if candidate["position_type"] == reference_position_type:
                consistent_position_type += 1
            if candidate["position"] == reference_position:
                consistent_position += 1
        
        # At least 70% should have consistent positioning
        threshold = len(combination) * 0.7
        return (consistent_position_type >= threshold or 
                consistent_position >= threshold)
    
    def _score_continuity(self, sequence: Dict[int, int]) -> float:
        """Score sequence based on numerical continuity (Rule 1)"""
        if len(sequence) < 2:
            return 0
        
        sorted_items = sorted(sequence.items())
        continuity_score = 0
        
        for i in range(len(sorted_items) - 1):
            page_idx1, page_num1 = sorted_items[i]
            page_idx2, page_num2 = sorted_items[i + 1]
            
            expected_diff = page_idx2 - page_idx1
            actual_diff = page_num2 - page_num1
            
            # Perfect continuity: numbers increase by 1 for consecutive pages
            if actual_diff == expected_diff and actual_diff > 0:
                continuity_score += 100
            elif actual_diff == 1:  # Sequential pages
                continuity_score += 90
            elif abs(actual_diff - expected_diff) <= 1:  # Close match
                continuity_score += 50
            else:
                continuity_score -= 30  # Penalty for poor continuity
        
        return continuity_score / (len(sorted_items) - 1) if len(sorted_items) > 1 else 0
    
    def _score_position_consistency(self, combination: List[Dict]) -> float:
        """Score position consistency (Rule 2)"""
        if len(combination) < 2:
            return 0
        
        # Count position types and positions
        position_types = {}
        positions = {}
        
        for candidate in combination:
            pos_type = candidate["position_type"]
            pos = candidate["position"]
            
            position_types[pos_type] = position_types.get(pos_type, 0) + 1
            positions[pos] = positions.get(pos, 0) + 1
        
        # Score based on consistency
        total_candidates = len(combination)
        max_position_type_count = max(position_types.values())
        max_position_count = max(positions.values())
        
        position_type_score = (max_position_type_count / total_candidates) * 50
        position_score = (max_position_count / total_candidates) * 50
        
        return position_type_score + position_score


class CitationLLM:
    """LLM handler for citation extraction using DSPy."""

    def __init__(self, llm_model="ollama/qwen3"):
        """Initialize the LLM."""
        self.llm = get_llm_model(llm_model, temperature=0.1)
        dspy.settings.configure(lm=self.llm)

    def _truncate_text(self, text: str, max_tokens: int = 2048) -> str:
        """Truncate text to a maximum number of tokens."""
        tokens = text.split()
        if len(tokens) > max_tokens:
            return " ".join(tokens[:max_tokens])
        return text

    def extract_book_citation(self, pdf_text: str) -> Dict:
        """Extract citation from book PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, publisher, year, location, editor, translator, volume, series, isbn, doi",
                "Extract citation information from book PDF text. Focus on cover and copyright pages (usually in first 5 pages). "
                "Look for title in the middle and upper part with biggest font size, author usually right under the title. "
                "In copyright page, find publish year and publisher. For Chinese text, extract information similarly. "
                "Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"Book LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with book LLM extraction: {e}")
            return {}

    def extract_thesis_citation(self, pdf_text: str) -> Dict:
        """Extract citation from thesis PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, thesis_type, year, publisher, location, doi",
                "Extract citation information from thesis PDF text. Focus on cover and title pages (usually in first 5 pages). "
                "Look for title in the middle and upper part with biggest font size, author usually right under the title. "
                "Identify if it's a PhD thesis or Master thesis. Publisher should be a university or college. "
                "For Chinese text, extract information similarly. Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"Thesis LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with thesis LLM extraction: {e}")
            return {}

    def extract_journal_citation(self, pdf_text: str) -> Dict:
        """Extract citation from journal PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, container_title, year, volume, issue, page_numbers, isbn, doi",
                "Extract citation information from journal PDF text. Focus on first page header and footer. "
                "Look for title in first line with biggest font size, author usually right under the title. "
                "Find journal name (as container_title), year, volume, and issue number in header or footer of first page. "
                "Page numbers format should be 'start-end' (e.g., '20-41'). "
                "For Chinese text, extract information similarly. Return 'Unknown' for missing fields.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Convert container_title back to container-title for compatibility
                    if key == "container_title":
                        citation_info["container-title"] = value.strip()
                    else:
                        citation_info[key] = value.strip()

            logging.info(f"Journal LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with journal LLM extraction: {e}")
            return {}

    def extract_bookchapter_citation(self, pdf_text: str) -> Dict:
        """Extract citation from book chapter PDF text."""
        try:
            signature = dspy.Signature(
                "pdf_text -> title, author, container_title, editor, publisher, year, location, page_numbers, isbn, doi",
                "Analyze the text from a book chapter and extract its citation metadata. "
                "Identify the following fields: "
                "- title: The title of the chapter itself. "
                "- author: The author(s) of the chapter. "
                "- container-title: The title of the book that contains the chapter. "
                "- editor: The editor(s) of the book, often found near 'edited by'. "
                "- publisher: The publisher of the book. "
                "- year: The publication year of the book. "
                "- page_numbers: The page range of the chapter (e.g., '20-41'). "
                "- location, isbn, doi: If available. "
                "For Chinese text, extract the information similarly. If a field is not found, return 'Unknown'.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(pdf_text=pdf_text)

            # Convert result to dictionary
            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Convert container_title back to container-title for compatibility
                    if key == "container_title":
                        citation_info["container-title"] = value.strip()
                    else:
                        citation_info[key] = value.strip()

            logging.info(f"Book chapter LLM extraction result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with book chapter LLM extraction: {e}")
            return {}

    def extract_page_numbers_for_journal_chapter(
        self,
        pdf_path: str,  # Changed from text parameters to PDF path
        max_pages: int = 5
    ) -> Dict:
        """
        Enhanced page number extraction using pattern recognition and position consistency.
        
        Implements two key rules:
        1. Continuous number patterns (e.g., 186, 187, 188, 189, 190)
        2. Consistent positioning (same location in header/footer)
        """
        try:
            extractor = ImprovedPageNumberExtractor()
            
            # Get total PDF pages
            doc = fitz.open(pdf_path)
            total_pdf_pages = doc.page_count
            doc.close()
            
            if total_pdf_pages == 0:
                return {}
            
            # Find continuous sequence with page range awareness
            page_sequence = extractor.find_continuous_page_sequence_with_range(
                pdf_path, page_range, total_pdf_pages
            )
            
            if page_sequence:
                # Convert to page range format
                page_numbers = list(page_sequence.values())
                if len(page_numbers) >= 2:
                    start_page = min(page_numbers)
                    end_page = max(page_numbers)
                    page_range_result = f"{start_page}-{end_page}"
                    
                    logging.info(f"Enhanced page extraction found range: {page_range_result}")
                    return {"page_numbers": page_range_result}
                elif len(page_numbers) == 1:
                    # Single page
                    page_range_result = str(page_numbers[0])
                    logging.info(f"Enhanced page extraction found single page: {page_range_result}")
                    return {"page_numbers": page_range_result}
            
            logging.warning("Enhanced page extraction found no continuous sequence")
            
            # Fallback to original LLM-based method if pattern-based fails
            return self._fallback_llm_page_extraction(pdf_path)
            
        except Exception as e:
            logging.error(f"Error in enhanced page number extraction: {e}")
            # Fallback to original method
            return self._fallback_llm_page_extraction(pdf_path)
            
            # Extract text from strategic pages for LLM analysis
            first_page_text = doc[0].get_text() if doc.page_count > 0 else ""
            second_page_text = doc[1].get_text() if doc.page_count > 1 else ""
            last_page_text = doc[doc.page_count - 1].get_text() if doc.page_count > 0 else ""
            second_to_last_page_text = doc[doc.page_count - 2].get_text() if doc.page_count > 1 else ""
            
            doc.close()
            
            signature = dspy.Signature(
                "first_page_text, second_page_text, last_page_text, second_to_last_page_text -> page_numbers",
                "Determine the page range (e.g., '20-41') for a document. "
                "1. Look for a number in the header or footer of the 'first_page_text'. This is the starting page. "
                "2. If not found, look for a number in the header or footer of the 'second_page_text'. If found, the starting page is that number minus 1. "
                "3. Look for a number in the header or footer of the 'last_page_text'. This is the ending page. "
                "4. If not found, look for a number in the header or footer of the 'second_to_last_page_text'. If found, the ending page is that number plus 1. "
                "If you can determine both a start and end page, return them as 'start-end'. Otherwise, return 'Unknown'.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(
                first_page_text=first_page_text,
                second_page_text=second_page_text,
                last_page_text=last_page_text,
                second_to_last_page_text=second_to_last_page_text,
            )

            citation_info = {}
            if result.page_numbers and result.page_numbers.lower() != "unknown":
                citation_info["page_numbers"] = result.page_numbers.strip()

            logging.info(f"Fallback LLM page extraction result: {citation_info}")
            return citation_info
            
        except Exception as e:
            logging.error(f"Error with fallback LLM page extraction: {e}")
            return {}

    def extract_citation_from_text(self, text: str, doc_type: str) -> Dict:
        """Extract citation based on document type after truncating long text."""
        truncated_text = self._truncate_text(text)

        if doc_type == "book":
            return self.extract_book_citation(truncated_text)
        elif doc_type == "thesis":
            return self.extract_thesis_citation(truncated_text)
        elif doc_type == "journal":
            return self.extract_journal_citation(truncated_text)
        elif doc_type == "bookchapter":
            return self.extract_bookchapter_citation(truncated_text)
        else:
            # Default fallback
            logging.warning(f"Unknown document type: {doc_type}, using book extraction")
            return self.extract_book_citation(truncated_text)

    def extract_citation_from_web_markdown(self, markdown_text: str) -> Dict:
        """Extracts citation fields from the markdown content of a webpage."""
        try:
            signature = dspy.Signature(
                "markdown_content -> title, author, date, container_title",
                "Analyze the markdown content of a webpage to extract citation information. "
                "Pay close attention to the main title, author by lines, and publication dates in first 600 words. "
                "The 'container-title' is the name of the overall website. "
                "Also, look for explicit citation hints like 'how to cite', '引用', '引用格式', '格式', '格式如下', '凡例'. "
                "Return 'Unknown' for any fields that cannot be found.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(markdown_content=self._truncate_text(markdown_text))

            citation_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    citation_info[key] = value.strip()

            logging.info(f"LLM extraction from web markdown result: {citation_info}")
            return citation_info

        except Exception as e:
            logging.error(f"Error with web markdown LLM extraction: {e}")
            return {}

    def parse_search_results(self, search_response: str) -> Dict:
        """Parse the response from a search API to extract citation fields."""
        try:
            signature = dspy.Signature(
                "search_results -> container_title, editor, publisher, year, volume, issue, page_numbers, doi",
                "Parse the provided search engine results to find missing citation information for a book chapter or journal article. "
                "Extract fields like the book/journal title (as container-title), editor, publisher, year, etc. "
                "Return 'Unknown' for any fields that cannot be found.",
            )

            predictor = dspy.Predict(signature)
            result = predictor(search_results=search_response)

            # Convert result to dictionary
            parsed_info = {}
            for key, value in result.items():
                if value and value.strip() and value.strip().lower() != "unknown":
                    # Handle key conversion for CSL compatibility
                    if key == "container_title":
                        parsed_info["container-title"] = value.strip()
                    else:
                        parsed_info[key] = value.strip()

            logging.info(f"Parsed search results: {parsed_info}")
            return parsed_info
        except Exception as e:
            logging.error(f"Error parsing search results with LLM: {e}")
            return {}
