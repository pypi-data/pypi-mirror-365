import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix
)
from IPython.display import display, Markdown


class ChurnEvaluator:
    """
    ChurnEvaluator ‚Äî Evaluates churn model variants with metrics and visualizations.

    This class provides methods to:
    - Evaluate traditional ML models and ensemble models.
    - Compute core classification metrics (Accuracy, Precision, Recall, F1-score).
    - Compute business cost using weighted False Positives (FP) and False Negatives (FN).
    - Identify the best-performing variant based on Recall > Cost > F1-score strategy.
    - Optionally visualize performance using confusion matrix, ROC, and radar charts.

    Returns:
        best_variant_name (str): Name of the top-performing variant.
        best_model (object): Trained model object.
        result_df (pd.DataFrame): Metrics and annotations for each variant.
    """

    @staticmethod
    def _compute_metrics(y_true, y_pred):
        """
        Internal method to compute classification metrics and error components.

        Parameters:
            y_true (array-like): Ground truth labels.
            y_pred (array-like): Predicted labels.

        Returns:
            Tuple of:
                accuracy (float)
                precision (float)
                recall (float)
                f1 (float)
                fp (int): False positives count
                fn (int): False negatives count
        """
        # Compute basic classification metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)

        # Extract false positives and false negatives from confusion matrix
        confusion = confusion_matrix(y_true, y_pred)
        fp = confusion[0][1]  # False positives: Predicted churn when not churn
        fn = confusion[1][0]  # False negatives: Missed churns

        return accuracy, precision, recall, f1, fp, fn


    @staticmethod
    def evaluate_model(model_name, builder_method, X_train, X_test, y_train, y_test,
                       plot_variant_level_charts=True, fp_cost=100, fn_cost=500):
        """
        Evaluates a group of model variants generated by a builder method.

        Parameters:
            model_name (str): Display name of the model family (e.g., "Logistic Regression").
            builder_method (callable): Function returning a list of (params, model) tuples.
            X_train, y_train (array-like): Training feature set and labels.
            X_test, y_test (array-like): Test feature set and labels.
            plot_variant_level_charts (bool): Whether to show variant-level plots.
            fp_cost (int): Business cost of a false positive.
            fn_cost (int): Business cost of a false negative.

        Returns:
            Tuple:
                best_variant_name (str): Name of the selected best variant.
                best_model (object): Fitted model object of the best variant.
                result_df (DataFrame): All variant evaluation results.
        """
        # Generate list of model variants with parameters
        variants = builder_method()
        results = []            # Store evaluation metrics
        model_objects = {}      # Map variant name to fitted model

        # Evaluate each variant
        for i, (params, model) in enumerate(variants):
            y_pred = model.predict(X_test)  # Generate predictions

            # Compute performance metrics and cost
            accuracy, precision, recall, f1, fp, fn = ChurnEvaluator._compute_metrics(y_test, y_pred)
            cost = (fp * fp_cost) + (fn * fn_cost)

            variant_name = f"Variant {i+1}"
            model_objects[variant_name] = model

            # Collect all results
            results.append({
                "Model": model_name,
                "Variant": variant_name,
                "Accuracy": accuracy,
                "Precision": precision,
                "Recall": recall,
                "F1-Score": f1,
                "Cost ($)": cost,
                "Params": params
            })

        # Convert results into a DataFrame for structured analysis
        result_df = pd.DataFrame(results)

        # Mark variant with lowest cost
        best_cost = result_df["Cost ($)"].min()
        result_df["is_best_cost"] = result_df["Cost ($)"].apply(
            lambda x: "üü© True" if x == best_cost else "üü• False"
        )

        # Identify best variant based on prioritized metric sorting
        best_idx = result_df.sort_values(
            by=["Recall", "Cost ($)", "F1-Score"],
            ascending=[False, True, False]
        ).index[0]

        # Mark the top-performing variant
        result_df["is_best_variant"] = "‚ùå"
        result_df.at[best_idx, "is_best_variant"] = "‚úÖ Best"

        best_variant_name = result_df.loc[best_idx, "Variant"]
        best_model = model_objects[best_variant_name]

        # Display evaluation and visualizations
        if plot_variant_level_charts:
            display(Markdown(f"### Evaluation Summary ‚Äì **{model_name}**"))
            display(result_df)

            try:
                from churn_modeling_pipelines import ChurnPlotter

                # Confusion Matrix
                print(f"\nConfusion Matrix ‚Äì {model_name}: {best_variant_name}")
                ChurnPlotter.plot_confusion_matrix(y_test, best_model.predict(X_test), model_name)

                # ROC Curve for all variants
                print(f"\nROC Curve ‚Äì All Variants: {model_name}")
                ChurnPlotter.plot_all_roc_curves(variants, X_test, y_test, model_name)

                # Composite Radial Chart
                print(f"\nRadial Chart ‚Äì All Variants: {model_name}")
                ChurnPlotter.plot_composite_radial_chart(result_df, model_name)

            except ImportError:
                print("‚ö†Ô∏è ChurnPlotter module not found. Skipping visualizations.")

        return best_variant_name, best_model, result_df

    @staticmethod
    def evaluate_ensemble_models(model_list, model_name, X_test, y_test,
                                 plot_variant_level_charts=True, fp_cost=100, fn_cost=500):
        """
        Evaluates a list of ensemble models with cost and classification metrics.

        Parameters:
            model_list (list): List of (params, model) tuples.
            model_name (str): Name of the ensemble model type.
            X_test, y_test (array-like): Test dataset for predictions and evaluation.
            plot_variant_level_charts (bool): Whether to visualize the top model.
            fp_cost (int): Business penalty for false positives.
            fn_cost (int): Business penalty for false negatives.

        Returns:
            Tuple:
                best_variant_name (str): Name of the top-performing ensemble variant.
                best_model (object): Best ensemble model object.
                result_df (DataFrame): Evaluation summary with annotations.
        """
        results = []            # Evaluation results for each ensemble variant
        model_objects = {}      # Dictionary to track models by variant name

        for i, (params, model) in enumerate(model_list):
            y_pred = model.predict(X_test)
            y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else y_pred

            # Compute classification metrics and business cost
            accuracy, precision, recall, f1, fp, fn = ChurnEvaluator._compute_metrics(y_test, y_pred)
            cost = (fp * fp_cost) + (fn * fn_cost)

            # Use provided name or default to Variant i
            variant_name = params.get("name", f"Variant {i+1}")
            model_objects[variant_name] = model

            results.append({
                "Model": model_name,
                "Variant": variant_name,
                "Accuracy": accuracy,
                "Precision": precision,
                "Recall": recall,
                "F1-Score": f1,
                "Cost ($)": cost,
                "Params": params
            })

        # Assemble results into DataFrame
        result_df = pd.DataFrame(results)

        # Highlight variant with best (lowest) cost
        best_cost = result_df["Cost ($)"].min()
        result_df["is_best_cost"] = result_df["Cost ($)"].apply(
            lambda x: "üü© True" if x == best_cost else "üü• False"
        )

        # Determine best-performing ensemble variant
        best_idx = result_df.sort_values(
            by=["Recall", "Cost ($)", "F1-Score"],
            ascending=[False, True, False]
        ).index[0]

        result_df["is_best_variant"] = "‚ùå"
        result_df.at[best_idx, "is_best_variant"] = "‚úÖ Best"

        best_variant_name = result_df.loc[best_idx, "Variant"]
        best_model = model_objects[best_variant_name]

        # Plot results if requested
        if plot_variant_level_charts:
            display(Markdown(f"### Evaluation Summary ‚Äì **{model_name}**"))
            display(result_df)

            try:
                from churn_modeling_pipelines import ChurnPlotter

                # Confusion matrix
                print(f"\nConfusion Matrix ‚Äì {model_name}: {best_variant_name}")
                ChurnPlotter.plot_ensemble_confusion_matrix(best_model, X_test, y_test)

                # ROC curve
                print(f"\nROC Curve ‚Äì {model_name}: {best_variant_name}")
                ChurnPlotter.plot_roc_curve(best_model, X_test, y_test, f"{model_name} ‚Äì {best_variant_name}")

                # Radial chart for single best variant
                print(f"\nRadial Chart ‚Äì {model_name}: {best_variant_name}")
                ChurnPlotter.plot_radial_chart(result_df.loc[best_idx].to_dict(), f"{model_name} ‚Äì {best_variant_name}")

            except ImportError:
                print("‚ö†Ô∏è ChurnPlotter module not found. Skipping visualizations.")

        return best_variant_name, best_model, result_df
