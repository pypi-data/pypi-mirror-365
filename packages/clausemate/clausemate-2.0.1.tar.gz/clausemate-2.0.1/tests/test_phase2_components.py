"""Test suite for the Phase 2 modular components.

This module provides basic tests to ensure the refactored components
work correctly and maintain compatibility with the original functionality.
"""

import os
import sys
import tempfile
import unittest
from pathlib import Path

# Add the src directory to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.models import SentenceContext, Token
from src.extractors.coreference_extractor import CoreferenceExtractor
from src.main import ClauseMateAnalyzer
from src.parsers.tsv_parser import DefaultTokenProcessor, TSVParser


class TestModularComponents(unittest.TestCase):
    """Test the basic functionality of modular components."""

    def setUp(self):
        """Set up test fixtures."""
        self.token_processor = DefaultTokenProcessor()
        self.parser = TSVParser(self.token_processor)
        self.coreference_extractor = CoreferenceExtractor()

        # Create a simple test TSV content
        self.test_tsv_content = """# sent_id = test-sentence-1
1	_	Er	_	SUBJ	AGENT	_	_	_	_	*->1-1	PersPron[1]	_	_	_
2	_	sieht	_	_	_	_	_	_	_	_	_	_	_	_
3	_	das	_	_	_	_	_	_	_	*->2-1	D-Pron[2]	_	_	_
4	_	Auto	_	_	_	_	_	_	_	_	_	_	_	_

# sent_id = test-sentence-2
1	Es	_	_	_	_	_	_	_	_	PersPron[2]	_	_	_	_
2	fÃ¤hrt	_	_	_	_	_	_	_	_	_	_	_	_	_
3	schnell	_	_	_	_	_	_	_	_	_	_	_	_	_
"""

    def test_token_creation(self):
        """Test basic token creation and validation."""
        token = Token(
            idx=1,
            text="er",
            sentence_num=1,
            grammatical_role="SUBJ",
            thematic_role="AGENT",
            coreference_link="PersPron[1]",
        )

        self.assertEqual(token.text, "er")
        self.assertEqual(token.idx, 1)
        self.assertIsNotNone(token.coreference_link)

    def test_token_validation(self):
        """Test token validation logic."""
        # Valid token
        valid_token = Token(
            idx=1,
            text="er",
            sentence_num=1,
            grammatical_role="SUBJ",
            thematic_role="AGENT",
        )

        self.assertTrue(self.token_processor.validate_token(valid_token))

        # Invalid token (empty text) - should raise ValueError during creation
        with self.assertRaises(ValueError):
            Token(
                idx=1,
                text="",
                sentence_num=1,
                grammatical_role="SUBJ",
                thematic_role="AGENT",
            )

    @unittest.skip("Test expects different boundary detection behavior - needs review")
    def test_sentence_boundary_detection(self):
        """Test sentence boundary detection."""
        # Test various boundary formats
        boundary_lines = [
            "# sent_id = test-1",
            "# sentence boundary",
            "#text: This is a sentence",
        ]

        for line in boundary_lines:
            self.assertTrue(
                self.parser.is_sentence_boundary(line),
                f"Failed to detect boundary in: {line}",
            )

        # Test non-boundary lines
        non_boundary_lines = ["1\ter\t_\t_\t_", "2\tsieht\t_\t_\t_"]

        for line in non_boundary_lines:
            self.assertFalse(
                self.parser.is_sentence_boundary(line),
                f"Incorrectly detected boundary in: {line}",
            )

    def test_coreference_id_extraction(self):
        """Test coreference ID extraction from tokens."""
        # Token with coreference information
        token = Token(
            idx=1,
            text="er",
            sentence_num=1,
            grammatical_role="SUBJ",
            thematic_role="AGENT",
            coreference_link="*->1-2",
            coreference_type="PersPron[1-3]",
        )

        ids = self.coreference_extractor.extract_all_ids_from_token(token)

        # Should extract numeric IDs from both fields
        self.assertGreater(len(ids), 0, "Should extract at least one ID")

        # Check specific extraction patterns
        expected_ids = {"1", "2", "3"}  # From the patterns above
        self.assertTrue(
            ids.intersection(expected_ids),
            f"Should extract some expected IDs. Got: {ids}",
        )

    def test_streaming_parser_with_content(self):
        """Test streaming parser with actual content."""
        # Create temporary file with test content
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".tsv", delete=False, encoding="utf-8"
        ) as f:
            f.write(self.test_tsv_content)
            temp_file_path = f.name

        try:
            # Parse using streaming
            contexts = list(self.parser.parse_sentence_streaming(temp_file_path))

            # Should have parsed sentences
            self.assertGreater(len(contexts), 0, "Should parse at least one sentence")

            # Check first sentence
            first_context = contexts[0]
            self.assertIsInstance(first_context, SentenceContext)
            self.assertGreater(len(first_context.tokens), 0, "Should have tokens")

            # Check tokens have expected properties
            first_token = first_context.tokens[0]
            self.assertEqual(first_token.text, "Er")
            self.assertEqual(first_token.idx, 1)

        finally:
            # Clean up temporary file
            os.unlink(temp_file_path)

    def test_analyzer_initialization(self):
        """Test analyzer initialization."""
        analyzer = ClauseMateAnalyzer()

        # Should have initialized components
        self.assertIsNotNone(analyzer.parser)
        self.assertIsNotNone(analyzer.coreference_extractor)

        # Should have initial statistics
        stats = analyzer.get_statistics()
        self.assertIn("sentences_processed", stats)
        self.assertEqual(stats["sentences_processed"], 0)

    def test_analyzer_with_test_file(self):
        """Test analyzer with a test file."""
        # Create temporary file with test content
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".tsv", delete=False, encoding="utf-8"
        ) as f:
            f.write(self.test_tsv_content)
            temp_file_path = f.name

        try:
            analyzer = ClauseMateAnalyzer()

            # Should be able to analyze the file without errors
            relationships = analyzer.analyze_file(temp_file_path)

            # Should return a list (even if empty for now)
            self.assertIsInstance(relationships, list)

            # Should have processed sentences
            stats = analyzer.get_statistics()
            self.assertGreater(stats["sentences_processed"], 0)

        finally:
            # Clean up temporary file
            os.unlink(temp_file_path)


class TestDataModels(unittest.TestCase):
    """Test the data models."""

    def test_sentence_context_creation(self):
        """Test sentence context creation."""
        tokens = [
            Token(
                idx=1,
                text="Er",
                sentence_num=1,
                grammatical_role="SUBJ",
                thematic_role="AGENT",
            ),
            Token(
                idx=2,
                text="sieht",
                sentence_num=1,
                grammatical_role="PRED",
                thematic_role="ACTION",
            ),
        ]

        context = SentenceContext(
            sentence_id="test-1",
            sentence_num=1,
            tokens=tokens,
            critical_pronouns=[],
            coreference_phrases=[],
        )

        self.assertEqual(context.sentence_id, "test-1")
        self.assertEqual(len(context.tokens), 2)
        self.assertFalse(context.has_critical_pronouns)
        self.assertFalse(context.has_coreference_phrases)


if __name__ == "__main__":
    # Run the tests
    unittest.main(verbosity=2)
