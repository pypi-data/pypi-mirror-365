Metadata-Version: 2.4
Name: exsclaim
Version: 2.4.1
Summary: EXSCLAIM! is a library for the automatic EXtraction, Separation, and Caption-based natural Language Annotation of IMages from scientific figures.
Author-email: "Eric Schwenker, Trevor Spreadbury, Weixin Jiang, Maria Chan, Len Washington III" <developer@materialeyes.org>
License: MIT License
        
        Copyright (c) 2024 MaterialEyes
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://exsclaim.materialeyes.org
Project-URL: Source, https://github.com/MaterialEyes/exsclaim2.0
Project-URL: Documentation, https://github.com/MaterialEyes/exsclaim2.0/wiki
Project-URL: Issues, https://github.com/MaterialEyes/exsclaim2.0/issues
Project-URL: Paper, https://arxiv.org/abs/2103.10631
Classifier: Development Status :: 4 - Beta
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Framework :: Dash
Classifier: Framework :: FastAPI
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: JavaScript
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Image Processing
Classifier: Topic :: Text Processing
Requires-Python: >=3.13.2
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: loguru==0.7.3
Requires-Dist: numpy==2.2.6
Requires-Dist: opencv-contrib-python-headless==4.12.0.88
Requires-Dist: Pillow==11.3.0
Requires-Dist: ultralytics==8.3.170
Requires-Dist: aiohttp==3.12.15
Requires-Dist: httpx==0.28.1
Requires-Dist: brotli==1.1.0
Requires-Dist: selenium==4.34.2
Requires-Dist: selenium-stealth==1.0.6
Requires-Dist: urllib3==2.5.0
Requires-Dist: asyncpg==0.30.0
Requires-Dist: beautifulsoup4==4.13.4
Requires-Dist: soupsieve==2.7
Requires-Dist: deepdiff==8.5.0
Requires-Dist: pycocotools==2.0.10
Requires-Dist: pytz==2025.2
Requires-Dist: typing_extensions==4.14.0
Requires-Dist: PyYAML==6.0.2
Requires-Dist: pymupdf==1.26.3
Requires-Dist: torch==2.7.1
Requires-Dist: torchvision==0.22.1
Requires-Dist: pytorch-model-summary==0.1.2
Requires-Dist: ollama==0.5.1
Requires-Dist: openai==1.97.1
Requires-Dist: asksageclient==1.31
Requires-Dist: dash[async]==3.1.1
Requires-Dist: fastapi[standard]==0.116.1
Requires-Dist: pydantic==2.11.7
Requires-Dist: starlette==0.47.2
Requires-Dist: uvicorn==0.35.0
Requires-Dist: hypercorn==0.17.3
Requires-Dist: sqlmodel==0.0.24
Requires-Dist: sqlalchemy==2.0.42
Requires-Dist: pydantic_settings==2.10.1
Requires-Dist: uuid-utils==0.11.0
Requires-Dist: dash_bootstrap_components==2.0.3
Requires-Dist: gunicorn[gevent]==23.0.0
Requires-Dist: certifi==2025.7.14
Dynamic: license-file

# EXSCLAIM2.0: LLM-powered Automatic **EX**traction, **S**eparation, and **C**aption-based natural **L**anguage **A**nnotation of **IM**ages from scientific figures
[![License](https://img.shields.io/github/license/MaterialEyes/exsclaim2.0.svg?color=blue)](https://github.com/MaterialEyes/exsclaim2.0/blob/main/LICENSE)
[![Website](https://img.shields.io/website?url=https%3A%2F%2Fexsclaim-dev.materialeyes.org%2F&up_message=online&down_message=offline&down_color=red&label=Website)
](https://exsclaim-dev.materialeyes.org)
[![Release](https://img.shields.io/github/release/MaterialEyes/exsclaim2.0.svg)](https://github.com/MaterialEyes/exsclaim2.0/releases)
[![DOI](https://zenodo.org/badge/DOI/10.48550/arXiv.2103.10631.svg)](https://arxiv.org/abs/2103.10631)

## ðŸ¤” Consider Collaboration

If you find this tool or any of its derived capabilities useful, please consider registering as a user of Center for Nanoscale Materials. We will keep you posted of latest developments, as well as opportunities for computational resources, relevant data, and collaboration. Please contact Maria Chan (mchan@anl.gov) for details.

## Introduction to EXSCLAIM2.0

EXSCLAIM2.0 is a Python package combining EXSCLAIM! code with Large Language models (LLMs) that can be used for the automatic generation of datasets of labeled images from published papers.
There are four main steps:
1. [JournalScraper](https://github.com/MaterialEyes/exsclaim2.0/wiki/JournalScraper): scrap journal websites, acquiring figures, captions, and metadata
2. [CaptionDistributor](https://github.com/MaterialEyes/exsclaim2.0/wiki/CaptionDistributor): separate figure captions into the component chunks that refer to the figure's subfigures using LLMs and prompt engineering
3. [FigureSeparator](https://github.com/MaterialEyes/exsclaim2.0/wiki/FigureSeparator): separate figures into subfigures, detect scale information, label, and type of image

## Examples and tutorials
We provide several tutorials demonstrating how to use EXSCLAIM2.0:
1. [Nature_exsclaim_search](/notebooks/1_Nature_exsclaim_search.ipynb): automatically scrapping data from literature and performing Named Entity Recognition (NER) on the extracted captions.
2. [HTMLScraper](/notebooks/2_HTMLScraper.ipynb): automatically scrapping data from user provided HTML files
3. [Microscopy_CLIP_retrieval](/notebooks/3_Microscopy_CLIP_retrieval.ipynb): Using Microscopy_CLIP to perform image-to-image and text-to-image retrieval on our multimodal microscopy dataset.


## Installation
The guides to install EXSCLAIM through Pip, Git and Docker can be found within the [wiki](https://github.com/MaterialEyes/exsclaim2.0/wiki/Installation).
The guides include installing pre-compiled versions as well as building from the source code and then installing.

### Using Exsclaim 2.0
```python
from exsclaim import Pipeline
search_query = {
		...
}
results = Pipeline(search_query_json)
```
where `search_query` is either a dictionary representing a valid JSON object, or a Pathlike string pointing towards a valid JSON file,
or 
```shell
python -m exsclaim query {path to json file holding search query}
```
More extensive guides can be found within the [wiki](https://github.com/MaterialEyes/exsclaim2.0/wiki/Running-the-EXSCLAIM-Pipeline).

### Using Docker Compose
To use Docker Compose to host the service, run the following commands in the base directory:
```shell
docker compose build base
docker compose build {service(s) here}
docker compose up {service(s) here}
```

## Acknowledgements
This material is based upon work supported by Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DE-AC02-06CH11357

This work was performed at the Center for Nanoscale Materials, a U.S. Department of Energy Office of Science User Facility, and supported by the U.S. Department of Energy, Office of Science, under Contract No. DE-AC02-06CH11357.

We gratefully acknowledge the computing resources provided on Bebop, a high-performance computing cluster operated by the Laboratory Computing Resource Center at Argonne National Laboratory.

## Citation
If you find EXSCLAIM! useful, please encourage its development by citing the following [paper](https://arxiv.org/abs/2103.10631) in your research:
```
Schwenker, E., Jiang, W. Spreadbury, T., Ferrier N., Cossairt, O., Chan M.K.Y., EXSCLAIM! - An automated pipeline for the construction and
labeling of materials imaging datasets from scientific literature. arXiv e-prints (2021): arXiv-2103
```

#### Bibtex
```
@article{schwenker2021exsclaim,
  title={EXSCLAIM! - An automated pipeline for the construction of labeled materials imaging datasets from literature},
  author={Schwenker, Eric and Jiang, Weixin and Spreadbury, Trevor and Ferrier, Nicola and Cossairt, Oliver and Chan, Maria KY},
  journal={arXiv e-prints},
  pages={arXiv--2103},
  year={2021}
}
```
