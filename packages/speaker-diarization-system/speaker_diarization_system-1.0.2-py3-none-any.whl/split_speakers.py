"""
This script performs speaker diarization on audio files, identifying and separating
individual speakers into distinct audio tracks. It handles overlapping speech and
maintains the original timing of the audio.
"""

# ===================================================================================
# SPEAKER DIARIZATION & AUDIO SPLITTING SCRIPT
# ===================================================================================
#
# PURPOSE:
# This script processes audio files to identify individual speakers and splits the
# audio into separate tracks for each speaker. It preserves the original timing,
# inserting silence when a speaker is not talking. Overlapping speech is removed
# to ensure each output track contains only one speaker.
#
#
# --- PREREQUISITES ---
#
# 1. Environment Setup:
#    You must first run the `setup.py` script to install all the necessary
#    dependencies in your Python virtual environment.
#
# 2. Hugging Face Token:
#    Create a file named `.env` in this directory and add your Hugging Face
#    access token to it like this:
#    HF_TOKEN=your_token_here
#
# 3. FFmpeg:
#    This script requires FFmpeg to be installed and accessible in your
#    system's PATH.
#
#
# --- USAGE ---
#
# The script is run from the command line. Place audio files you want to process
# into the `audio/pending` directory.
#
# COMMANDS:
#
# 1. Automatic Speaker Detection:
#    python split_speakers.py
#
# 2. Set an Exact Number of Speakers:
#    (e.g., for exactly 2 speakers)
#    python split_speakers.py 2
#
# 3. Set a Range for the Number of Speakers:
#    (e.g., for a minimum of 2 and a maximum of 4 speakers)
#    python split_speakers.py 2 4
#
# 4. Enable Verbose/Debug Mode:
#    Add the `--verbose` or `-v` flag to any command to see detailed logs
#    and library warnings, which is useful for troubleshooting.
#    python split_speakers.py --verbose
#
#
# --- FILE STRUCTURE ---
#
# - audio/pending:   Place your source audio files here.
# - audio/processed: After a file is successfully processed, it's moved here.
# - audio/completed: The separated speaker tracks are saved here.
#
# pylint: disable = broad-except
# ===================================================================================

import argparse
import logging
import os
import shutil
import sys
import webbrowser
import warnings
from pathlib import Path

import dotenv
import torch
from huggingface_hub import hf_hub_download
from huggingface_hub.utils import GatedRepoError
from pydub import AudioSegment
from pyannote.audio import Pipeline
from pyannote.core import Annotation, Timeline

# --- Pre-emptive Warning Control ---
# We check for the verbose flag manually because the full argparse logic runs later in main().
if "-v" not in sys.argv and "--verbose" not in sys.argv:
    # In quiet mode, suppress all warnings generated by the libraries.
    warnings.filterwarnings("ignore")

# --- CONFIGURATION & SETUP FUNCTIONS ---

PIPELINE_MODEL = "pyannote/speaker-diarization-3.1"
REQUIRED_MODELS = [PIPELINE_MODEL, "pyannote/segmentation-3.0"]
SUPPORTED_EXTENSIONS = [".wav", ".mp3", ".m4a", ".flac"]


def check_ffmpeg():
    """
    Checks if the FFmpeg command-line tool is installed and available in the
    system's PATH. Exits the script with an error if it's not found.
    """
    if not shutil.which("ffmpeg"):
        logging.error("ffmpeg is not installed or not in your PATH.")
        logging.info("This script requires ffmpeg to process audio files.")
        logging.info("Download from: https://www.gyan.dev/ffmpeg/builds/")
        sys.exit(1)


def check_hf_model_access(model_list: list, token: str):
    """
    Proactively checks for download access to all required gated models on
    Hugging Face by attempting to download a small config file from each.

    Args:
        model_list (list): A list of model IDs to check (e.g., ['pyannote/model-name']).
        token (str): The Hugging Face authentication token.
    """
    logging.info("Verifying download access to Hugging Face models...")
    for model_id in model_list:
        try:
            # Attempt to download a small, non-essential file to verify access.
            hf_hub_download(
                repo_id=model_id, filename="config.yaml", token=token
            )
            logging.info("   ‚úÖ Access verified for %s", model_id)
        except GatedRepoError:
            # This specific error means the user hasn't accepted the license agreement.
            url = f"https://huggingface.co/{model_id}"
            logging.error(
                "User agreement for '%s' not accepted. Opening page...",
                model_id,
            )
            webbrowser.open(url)
            sys.exit(1)
        except Exception as e:
            # Some models (like the main pipeline) might not have a root config.yaml.
            # A 404 error in this context means the repository is public and accessible.
            if "404" in str(e):
                logging.info(
                    "   ‚úÖ Access verified for %s (no root config.yaml, skipping)",
                    model_id,
                )
                continue
            logging.error(
                "An unexpected error occurred while checking '%s': %s",
                model_id,
                e,
            )
            sys.exit(1)


def remove_crosstalk(diarization: Annotation) -> Annotation:
    """
    Removes overlapping speech from a diarization result to ensure each speaker
    has a clean track.

    This function uses a robust workaround because the simple timeline subtraction
    operator (`-`) is buggy in some library versions. The logic `A - B` is
    mathematically equivalent to `A ‚à© (not B)`, which we can calculate reliably.

    Args:
        diarization (Annotation): The raw diarization result from the pipeline.

    Returns:
        Annotation: A new annotation object with all overlapping segments removed.
    """
    logging.info("Removing crosstalk from speaker timelines...")
    cleaned_diarization = Annotation()

    for speaker_a in diarization.labels():
        # Get the timeline for the current speaker.
        timeline_a = diarization.label_timeline(speaker_a)

        # Create a single timeline containing the speech of all *other* speakers.
        other_speakers_timeline = Timeline()
        for speaker_b in diarization.labels():
            if speaker_a == speaker_b:
                continue
            other_speakers_timeline.update(
                diarization.label_timeline(speaker_b)
            )

        # Merge all segments from other speakers into a single continuous timeline.
        other_support = other_speakers_timeline.support()

        # --- The Workaround Logic ---
        # 1. Find the gaps in other speakers' speech (this is "not B").
        #    We need to provide the full duration of the audio as the "support"
        #    to correctly identify all gaps.
        speech_extent = diarization.get_timeline().extent()
        not_other_support = other_support.gaps(support=speech_extent)

        # 2. Find the intersection of Speaker A's speech and the gaps of others.
        #    This leaves only the segments where Speaker A was talking alone.
        cleaned_timeline = timeline_a.crop(not_other_support)

        # Add the resulting clean segments to our new annotation.
        for segment in cleaned_timeline:
            cleaned_diarization[segment] = speaker_a

    return cleaned_diarization


def main():
    """
    Main function to orchestrate the entire diarization and splitting process.
    It handles argument parsing, environment checks, and file processing.
    """
    # --- Argument Parsing & Logging Setup ---
    parser = argparse.ArgumentParser(
        description="Split audio files by speaker using speaker diarization."
    )
    parser.add_argument(
        "speakers",
        type=int,
        nargs="*",
        help="""Optional: Number of speakers.
                Can be one integer (e.g., 3) or a min and max (e.g., 2 5).""",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Enable verbose (DEBUG) logging and show all warnings.",
    )
    args = parser.parse_args()

    # Configure logging. Use force=True to override any settings applied by
    # imported libraries like speechbrain.
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(levelname)s - %(message)s",
        datefmt="%H:%M:%S",
        force=True,
    )

    if not args.verbose:
        # In quiet mode, specifically target and silence noisy library loggers.
        logging.getLogger("speechbrain").setLevel(logging.WARNING)
        logging.getLogger("pytorch_lightning").setLevel(logging.WARNING)
        logging.getLogger("lightning_fabric").setLevel(logging.WARNING)

    # --- Performance & Environment Setup ---
    # Enable TF32 on CUDA devices for a performance boost, as recommended by PyTorch.
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True

    # Interpret speaker arguments from the command line.
    num_speakers, min_speakers, max_speakers = None, None, None
    if len(args.speakers) == 1:
        num_speakers = min_speakers = max_speakers = args.speakers[0]
        logging.info("Expecting exactly %d speakers.", num_speakers)
    elif len(args.speakers) == 2:
        min_speakers = args.speakers[0]
        max_speakers = args.speakers[1]
        logging.info(
            "Expecting between %d and %d speakers.", min_speakers, max_speakers
        )
    elif len(args.speakers) > 2:
        logging.error(
            "Too many speaker count arguments. Provide 0, 1, or 2 integers."
        )
        sys.exit(1)

    # Load environment variables (specifically the HF_TOKEN).
    dotenv.load_dotenv()
    hf_token = os.getenv("HF_TOKEN")
    if not hf_token:
        raise ValueError("HF_TOKEN not found in .env file.")

    # Perform pre-flight checks.
    check_ffmpeg()
    check_hf_model_access(REQUIRED_MODELS, hf_token)

    # Set the device for computation (GPU if available, otherwise CPU).
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info("‚ö° Using device: %s", device)

    # --- Directory Setup ---
    base_dir = Path.cwd()
    audio_dir = base_dir / "audio"
    pending_dir = audio_dir / "pending"
    processed_dir = audio_dir / "processed"
    completed_dir = audio_dir / "completed"

    for folder in [pending_dir, processed_dir, completed_dir]:
        folder.mkdir(parents=True, exist_ok=True)

    # --- File Discovery ---
    pending_files = [
        p for ext in SUPPORTED_EXTENSIONS for p in pending_dir.glob(f"*{ext}")
    ]
    if not pending_files:
        logging.info("No files in pending directory.")
        sys.exit(0)

    logging.info("Found %d file(s) to process.", len(pending_files))

    # --- Pipeline Loading ---
    logging.info(
        "Loading diarization pipeline '%s' (this may take a minute)...",
        PIPELINE_MODEL,
    )
    try:
        pipeline = Pipeline.from_pretrained(
            PIPELINE_MODEL, use_auth_token=hf_token
        )
        pipeline.to(device)
    except Exception as e:
        logging.error(
            "An unexpected error occurred while loading the pipeline: %s", e
        )
        sys.exit(1)

    # --- Main Processing Loop ---
    errors_encountered = False
    for input_file_path in sorted(pending_files):
        try:
            input_filename = input_file_path.stem
            logging.info(
                "\n%s\nüéôÔ∏è  Processing: %s%s",
                "‚îÄ" * 50,
                input_filename,
                input_file_path.suffix,
            )

            # Run the main diarization pipeline.
            logging.info(
                "Running diarization... analyzing speaker segments..."
            )
            diarization = pipeline(
                str(input_file_path),
                num_speakers=num_speakers,
                min_speakers=min_speakers,
                max_speakers=max_speakers,
            )

            # Remove overlapping speech to get clean tracks.
            cleaned_diarization = remove_crosstalk(diarization)

            # Load the audio file into memory for slicing.
            logging.info("Loading audio file...")
            audio = AudioSegment.from_file(input_file_path)

            # Collect all audio segments for each speaker.
            speaker_segments = {}
            logging.info(
                "Segmenting and assigning audio to speakers (using cleaned timelines)..."
            )
            for turn, _, speaker in cleaned_diarization.itertracks(
                yield_label=True
            ):
                if speaker not in speaker_segments:
                    speaker_segments[speaker] = []

                start_ms = int(turn.start * 1000)
                end_ms = int(turn.end * 1000)
                segment = audio[start_ms:end_ms]
                speaker_segments[speaker].append((start_ms, segment))

            # Export a separate audio file for each speaker.
            num_speakers_found = len(speaker_segments)
            total_segments = sum(
                len(segs) for segs in speaker_segments.values()
            )
            logging.info(
                "Exporting %d speaker track(s) from %d segment(s)...",
                num_speakers_found,
                total_segments,
            )

            for speaker, segments in speaker_segments.items():
                # Create a silent track with the same duration as the original audio.
                output_track = AudioSegment.silent(duration=len(audio))
                # Overlay each of the speaker's segments at the correct time.
                for start, segment in segments:
                    output_track = output_track.overlay(
                        segment, position=start
                    )

                # Sanitize the speaker label for the filename.
                safe_speaker_label = speaker.replace(" ", "_")
                output_filename = f"{input_filename}_{safe_speaker_label}.wav"
                output_path = completed_dir / output_filename

                output_track.export(output_path, format="wav")
                logging.info("‚úÖ Saved: %s", output_filename)

            # Move the original file to the processed folder upon success.
            processed_path = processed_dir / input_file_path.name
            shutil.move(str(input_file_path), processed_path)
            logging.info("üì¶ Moved original to: %s", processed_path)

        except Exception as e:
            logging.error(
                "Error processing file %s: %s", input_file_path.name, e
            )
            logging.info("Skipping to next file.")
            errors_encountered = True
            continue

    # --- Final Status Message ---
    logging.info("\n%s", "‚îÄ" * 50)
    if errors_encountered:
        logging.warning("‚ö†Ô∏è Processing finished, but one or more files failed.")
    else:
        logging.info("‚úÖ All done!")


if __name__ == "__main__":
    main()
