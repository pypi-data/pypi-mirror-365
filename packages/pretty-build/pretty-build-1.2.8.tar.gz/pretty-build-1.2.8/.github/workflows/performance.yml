name: Performance Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests weekly on Saturdays at 3 AM UTC
    - cron: '0 3 * * 6'
  workflow_dispatch:

jobs:
  performance-test:
    name: Performance Testing
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.9", "3.11"]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .
        pip install pytest-benchmark memory-profiler psutil

    - name: Create performance test
      run: |
        mkdir -p tests/performance
        cat > tests/performance/test_performance.py << 'EOF'
        #!/usr/bin/env python3
        """Performance tests for Pretty Build"""

        import pytest
        import time
        import psutil
        import os
        from pathlib import Path
        import tempfile
        import subprocess
        import sys

        # Add src to path
        sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

        try:
            import pretty_build
        except ImportError:
            pytest.skip("pretty_build module not available", allow_module_level=True)


        class TestPrettyBuildPerformance:
            """Performance tests for Pretty Build"""

            def test_import_time(self, benchmark):
                """Test import time of pretty_build module"""
                def import_module():
                    import importlib
                    if 'pretty_build' in sys.modules:
                        importlib.reload(sys.modules['pretty_build'])
                    else:
                        import pretty_build
                    return pretty_build

                result = benchmark(import_module)
                assert result is not None

            def test_config_loading_performance(self, benchmark):
                """Test configuration loading performance"""
                def load_config():
                    from pretty_build import BuildConfig
                    config = BuildConfig()
                    return config

                result = benchmark(load_config)
                assert result is not None

            def test_memory_usage(self):
                """Test memory usage of pretty_build"""
                import pretty_build
                
                process = psutil.Process()
                initial_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                # Create multiple instances to test memory usage
                configs = []
                for i in range(100):
                    config = pretty_build.BuildConfig()
                    configs.append(config)
                
                final_memory = process.memory_info().rss / 1024 / 1024  # MB
                memory_increase = final_memory - initial_memory
                
                # Memory increase should be reasonable (less than 50MB for 100 configs)
                assert memory_increase < 50, f"Memory usage increased by {memory_increase:.2f}MB"

            def test_cli_startup_time(self, benchmark):
                """Test CLI startup time"""
                def run_help_command():
                    result = subprocess.run([
                        sys.executable, "-c", 
                        "import sys; sys.path.insert(0, 'src'); "
                        "from pretty_build import main; "
                        "sys.argv = ['pretty-build', '--help']; "
                        "try: main(); except SystemExit: pass"
                    ], capture_output=True, text=True, timeout=30)
                    return result.returncode

                result = benchmark(run_help_command)
                assert result == 0

            @pytest.mark.skipif(os.name == 'nt', reason="Unix-specific test")
            def test_build_simulation_performance(self, benchmark):
                """Test build simulation performance on Unix systems"""
                def simulate_build():
                    with tempfile.TemporaryDirectory() as tmpdir:
                        # Create a simple Makefile
                        makefile_path = Path(tmpdir) / "Makefile"
                        makefile_path.write_text("""
        all:
        \t@echo "Building..."
        \t@sleep 0.1
        \t@echo "Build complete"

        clean:
        \t@echo "Cleaning..."
        """)
                        
                        # Run make command
                        result = subprocess.run(
                            ["make", "-C", tmpdir],
                            capture_output=True,
                            text=True,
                            timeout=10
                        )
                        return result.returncode == 0

                result = benchmark(simulate_build)
                assert result is True


        class TestTUIPerformance:
            """Performance tests for TUI components"""

            def test_tui_import_time(self, benchmark):
                """Test TUI module import time"""
                def import_tui():
                    import importlib
                    if 'textual_tui' in sys.modules:
                        importlib.reload(sys.modules['textual_tui'])
                    else:
                        import textual_tui
                    return textual_tui

                result = benchmark(import_tui)
                assert result is not None

            def test_tui_initialization(self, benchmark):
                """Test TUI initialization performance"""
                def init_tui():
                    try:
                        import textual_tui
                        
                        class MockConfig:
                            build_command = "echo test"
                            clean_command = "echo clean"
                            output_dir = "build"
                            parallel_jobs = 4
                            enable_notifications = True
                        
                        config = MockConfig()
                        # Just test that we can create the config, not run the full TUI
                        return config
                    except Exception as e:
                        pytest.skip(f"TUI not available: {e}")

                result = benchmark(init_tui)
                assert result is not None


        if __name__ == "__main__":
            pytest.main([__file__, "-v"])
        EOF

    - name: Run performance tests
      run: |
        cd tests/performance
        python -m pytest test_performance.py -v --benchmark-only --benchmark-json=../../benchmark-results.json

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: benchmark-results.json

    - name: Memory profiling
      run: |
        # Create a simple memory profiling script
        cat > memory_profile.py << 'EOF'
        #!/usr/bin/env python3
        import sys
        from pathlib import Path
        sys.path.insert(0, str(Path(__file__).parent / "src"))

        @profile
        def test_memory_usage():
            import pretty_build
            
            # Test basic functionality
            config = pretty_build.BuildConfig()
            
            # Create multiple instances
            configs = []
            for i in range(50):
                configs.append(pretty_build.BuildConfig())
            
            return configs

        if __name__ == "__main__":
            result = test_memory_usage()
            print(f"Created {len(result)} config instances")
        EOF

        # Run memory profiling
        python -m memory_profiler memory_profile.py > memory_profile.txt || echo "Memory profiling completed"

    - name: Upload memory profile
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: memory-profile-${{ matrix.os }}-py${{ matrix.python-version }}
        path: memory_profile.txt

  benchmark-comparison:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    needs: performance-test
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-ubuntu-latest-py3.11
        path: current/

    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        path: main-branch/

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"

    - name: Install dependencies for main branch
      run: |
        cd main-branch
        python -m pip install --upgrade pip
        pip install -r requirements.txt || echo "No requirements.txt in main"
        pip install -e . || echo "Failed to install main branch"
        pip install pytest-benchmark

    - name: Run benchmark on main branch
      run: |
        cd main-branch
        if [ -f "tests/performance/test_performance.py" ]; then
          python -m pytest tests/performance/test_performance.py --benchmark-only --benchmark-json=../main-benchmark.json || echo "Benchmark failed on main"
        else
          echo "No performance tests in main branch"
          echo '{"benchmarks": []}' > ../main-benchmark.json
        fi

    - name: Compare benchmarks
      run: |
        python << 'EOF'
        import json
        import os

        def load_benchmark(file_path):
            try:
                with open(file_path, 'r') as f:
                    return json.load(f)
            except:
                return {"benchmarks": []}

        current = load_benchmark('current/benchmark-results.json')
        main = load_benchmark('main-benchmark.json')

        print("## 📊 Performance Comparison")
        print()

        if not current.get('benchmarks') and not main.get('benchmarks'):
            print("No benchmark data available for comparison.")
            exit(0)

        current_benchmarks = {b['name']: b for b in current.get('benchmarks', [])}
        main_benchmarks = {b['name']: b for b in main.get('benchmarks', [])}

        print("| Test | Current (s) | Main (s) | Change | Status |")
        print("|------|-------------|----------|---------|--------|")

        for name in set(current_benchmarks.keys()) | set(main_benchmarks.keys()):
            current_time = current_benchmarks.get(name, {}).get('stats', {}).get('mean', 0)
            main_time = main_benchmarks.get(name, {}).get('stats', {}).get('mean', 0)
            
            if main_time > 0 and current_time > 0:
                change = ((current_time - main_time) / main_time) * 100
                status = "🔴" if change > 10 else "🟡" if change > 5 else "🟢"
                print(f"| {name} | {current_time:.4f} | {main_time:.4f} | {change:+.1f}% | {status} |")
            elif current_time > 0:
                print(f"| {name} | {current_time:.4f} | N/A | New | 🆕 |")
            elif main_time > 0:
                print(f"| {name} | N/A | {main_time:.4f} | Removed | ❌ |")

        print()
        print("Legend: 🟢 Good (<5% change), 🟡 Warning (5-10% slower), 🔴 Regression (>10% slower)")
        EOF

  performance-report:
    name: Performance Report
    runs-on: ubuntu-latest
    needs: performance-test
    if: always()

    steps:
    - name: Download all benchmark results
      uses: actions/download-artifact@v3

    - name: Generate performance report
      run: |
        python << 'EOF'
        import json
        import os
        from pathlib import Path

        print("# 📊 Performance Test Report")
        print()
        print(f"**Date:** {os.environ.get('GITHUB_RUN_ID', 'Unknown')}")
        print(f"**Commit:** {os.environ.get('GITHUB_SHA', 'Unknown')[:8]}")
        print()

        # Find all benchmark result files
        benchmark_files = list(Path('.').glob('**/benchmark-results.json'))
        
        if not benchmark_files:
            print("No benchmark results found.")
            exit(0)

        print("## Results by Platform")
        print()

        for file_path in benchmark_files:
            platform_info = file_path.parent.name
            print(f"### {platform_info}")
            print()
            
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                
                benchmarks = data.get('benchmarks', [])
                if benchmarks:
                    print("| Test | Mean (s) | Min (s) | Max (s) | Std Dev |")
                    print("|------|----------|---------|---------|---------|")
                    
                    for benchmark in benchmarks:
                        name = benchmark['name']
                        stats = benchmark.get('stats', {})
                        mean = stats.get('mean', 0)
                        min_time = stats.get('min', 0)
                        max_time = stats.get('max', 0)
                        stddev = stats.get('stddev', 0)
                        
                        print(f"| {name} | {mean:.4f} | {min_time:.4f} | {max_time:.4f} | {stddev:.4f} |")
                else:
                    print("No benchmark data available.")
                    
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
            
            print()

        print("## Memory Usage")
        print()
        
        # Find memory profile files
        memory_files = list(Path('.').glob('**/memory_profile.txt'))
        for file_path in memory_files:
            platform_info = file_path.parent.name
            print(f"### {platform_info}")
            print()
            
            try:
                with open(file_path, 'r') as f:
                    content = f.read()
                    if content.strip():
                        print("```")
                        print(content)
                        print("```")
                    else:
                        print("No memory profile data available.")
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
            
            print()
        EOF > performance-report.md

    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance-report.md