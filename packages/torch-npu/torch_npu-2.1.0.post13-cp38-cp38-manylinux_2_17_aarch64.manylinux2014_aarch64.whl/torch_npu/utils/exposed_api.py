public_npu_functions = ['npu_rotated_overlaps', 'npu_gmm_alltoallv', 'npu_transpose', 'npu_bounding_box_encode', 'npu_rotated_iou', 'npu_dynamic_quant', 'npu_quant_matmul', 'npu_transpose_batchmatmul', 'npu_lstm', 'npu_nms_with_mask', 'npu_anchor_response_flags', 'npu_one_hot', 'npu_sign_bits_pack', 'npu_fused_infer_attention_score', 'npu_mla_prolog_v2', 'fast_gelu', 'npu_convert_weight_to_int4pack', 'npu_ciou', 'npu_moe_finalize_routing', 'npu_moe_gating_top_k_softmax', 'empty_with_swapped_memory', 'npu_get_float_status', 'npu_scatter_nd_update', 'npu_random_choice_with_mask', 'npu_roi_align', 'npu_ps_roi_pooling', 'npu_weight_quant_batchmatmul', 'npu_conv3d', 'scatter_update', 'npu_indexing', 'npu_mla_prolog', 'npu_slice', 'npu_scaled_masked_softmax', 'npu_dropout_with_add_softmax', 'npu_alloc_float_status', 'npu_group_norm_silu', 'npu_yolo_boxes_encode', 'npu_grid_assign_positive', 'scatter_update_', 'npu_alltoallv_gmm', 'npu_group_norm_swish', 'npu_softmax_cross_entropy_with_logits', 'npu_quant_scatter', 'npu_confusion_transpose', '_npu_dropout', 'npu_quantize', 'npu_rms_norm', 'npu_bmmV2', 'npu_swiglu', 'npu_scatter_nd_update_', 'npu_top_k_top_p', 'npu_rotary_mul', 'npu_clear_float_status', 'npu_iou', 'npu_prefetch', 'npu_trans_quant_param', 'npu_mm_reduce_scatter_base', 'npu_prompt_flash_attention', 'npu_grouped_matmul', 'npu_ffn', 'npu_gather_sparse_index', 'npu_bert_apply_adam', 'npu_mm_all_reduce_base', 'npu_dynamic_quant_asymmetric', 'npu_anti_quant', 'npu_batch_nms', 'npu_all_gather_base_mm', 'npu_fast_gelu', 'npu_linear', 'npu_cross_entropy_loss', 'npu_format_cast', 'npu_diou', 'npu_incre_flash_attention', 'npu_advance_step_flashattn', 'npu_bounding_box_decode', 'npu_moe_compute_expert_tokens', 'npu_giou', 'npu_deformable_conv2d', 'npu_multi_head_attention', 'npu_fusion_attention', 'npu_sign_bits_unpack', 'npu_moe_init_routing', 'npu_pad', 'npu_max', 'npu_nms_v4', 'npu_quant_scatter_', 'npu_fused_attention_score', 'npu_grouped_matmul_finalize_routing', 'npu_format_cast_']