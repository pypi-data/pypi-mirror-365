# LLM Configuration
# Provider: openai, ollama, litellm (auto-detected if not set)
LLM_PROVIDER=openai

# OpenAI Configuration (production recommended)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1
# OPENAI_ORGANIZATION=your_org_id_here

# Ollama Configuration (local/development)
# LLM_PROVIDER=ollama
# OLLAMA_MODEL=llama3.2:3b
# OLLAMA_BASE_URL=http://localhost:11434/v1
# For backward compatibility:
# LLM_BASE_URL=http://localhost:11434/v1

# LiteLLM Configuration (custom endpoints with bearer tokens)
# LLM_PROVIDER=litellm
# LITELLM_BEARER_TOKEN=your_bearer_token_here
# LITELLM_BASE_URL=https://your-custom-endpoint.com/v1
# LITELLM_MODEL=gpt-4o
# LITELLM_ADDITIONAL_HEADERS={"X-Custom-Header": "value"}

# Neo4j Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here
NEO4J_DATABASE=neo4j

# Performance Settings
KG_CACHE_TTL=300
KG_MAX_BATCH_SIZE=50

# Application Settings
LOG_LEVEL=INFO
PYTHONPATH=./src
PYTHONUNBUFFERED=1