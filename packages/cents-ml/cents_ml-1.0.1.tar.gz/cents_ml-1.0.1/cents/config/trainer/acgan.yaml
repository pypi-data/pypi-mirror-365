precision: "16-mixed"
accelerator: auto
devices: auto
strategy: ddp_find_unused_parameters_true
max_epochs: 5000
batch_size: 1024
sampling_batch_size: 4096
gradient_accumulate_every: 1
log_every_n_steps: 1
eval_after_training: False

checkpoint:
  save_last: False
  save_top_k: 0
  every_n_train_steps: null
  every_n_epochs: null

optimizer:
  generator:
    _target_: torch.optim.Adam
    lr: 3e-4
    betas: [0.5, 0.999]

  discriminator:
    _target_: torch.optim.Adam
    lr: 1e-4
    betas: [0.5, 0.999]

lr_scheduler_params:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  factor: 0.5
  patience: 200
  min_lr: 1e-5
  threshold: 0.1
  threshold_mode: rel
