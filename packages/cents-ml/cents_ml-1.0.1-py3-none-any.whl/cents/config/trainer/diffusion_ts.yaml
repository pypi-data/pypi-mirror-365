precision: "16-mixed"
accelerator: auto
devices: auto
strategy: ddp_find_unused_parameters_true
gradient_accumulate_every: 2
log_every_n_steps: 1
batch_size: 1024
max_epochs: 5000
base_lr: 1e-4
eval_after_training: False

checkpoint:
  save_last: False
  save_top_k: 0
  every_n_train_steps: null
  every_n_epochs: null

lr_scheduler_params:
  factor: 0.5
  patience: 200
  min_lr: 1.0e-5
  threshold: 1.0e-1
  threshold_mode: rel
  verbose: false
