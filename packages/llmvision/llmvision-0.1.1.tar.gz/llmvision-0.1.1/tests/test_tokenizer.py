import pytest
from hypothesis import given, strategies as st, example, settings, HealthCheck
from llmvision import (
    SimpleTokenizer,
    WordTokenizer,
    CharTokenizer,
    visualize_tokens,
    tokenize_and_visualize
)


class TestTokenizers:
    @pytest.fixture
    def tokenizers(self):
        return {
            'simple': SimpleTokenizer(),
            'word': WordTokenizer(),
            'char': CharTokenizer()
        }
    
    @given(text=st.text())
    @settings(suppress_health_check=[HealthCheck.function_scoped_fixture])
    @example(text="")  # Empty string
    @example(text="Hello, world!")  # Basic ASCII
    @example(text="ðŸ˜€ðŸŽ‰ðŸŒ")  # Emojis
    @example(text="ä½ å¥½ä¸–ç•Œ")  # Chinese
    @example(text="Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…")  # Arabic
    @example(text="Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹ Ð¼Ð¸Ñ€")  # Cyrillic
    @example(text="ðŸ‡ºðŸ‡¸ðŸ‡¬ðŸ‡§ðŸ‡¯ðŸ‡µ")  # Flag emojis
    @example(text="\n\t\r")  # Whitespace
    @example(text="\x00\x01\x02")  # Control characters
    @example(text="\u200b\u200c\u200d")  # Zero-width characters
    @example(text="cafÃ©")  # Accented characters
    @example(text="ðŸ§‘â€ðŸ’»ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦")  # Complex emoji sequences
    def test_tokenizer_returns_list(self, text, tokenizers):
        for name, tokenizer in tokenizers.items():
            result = tokenizer.tokenize(text)
            assert isinstance(result, list), f"{name} tokenizer should return a list"
            assert all(isinstance(token, str) for token in result), f"{name} tokenizer should return list of strings"
    
    @given(st.text())
    def test_char_tokenizer_preserves_length(self, text):
        tokenizer = CharTokenizer()
        tokens = tokenizer.tokenize(text)
        assert len(tokens) == len(text)
        assert ''.join(tokens) == text
    
    @given(st.text())
    def test_word_tokenizer_handles_whitespace(self, text):
        tokenizer = WordTokenizer()
        tokens = tokenizer.tokenize(text)
        # Word tokenizer splits on whitespace
        if text.strip():
            assert len(tokens) >= 1
        else:
            assert len(tokens) == 0
    
    @given(st.text(min_size=1))
    def test_visualization_preserves_information(self, text):
        # Test that we can identify token boundaries
        tokenizer = SimpleTokenizer()
        tokens = tokenizer.tokenize(text)
        visualization = visualize_tokens(tokens)
        
        # Check that visualization contains separators
        if len(tokens) > 1:
            assert "â”‚" in visualization
        
        # Ensure no information is completely lost
        assert len(visualization) > 0
    
    @given(st.text())
    def test_tokenize_and_visualize_integration(self, text):
        result = tokenize_and_visualize(text)
        assert isinstance(result, str)
        # Should not raise any exceptions
    
    def test_control_characters(self):
        text = "Hello\x00World\x01Test\x1f"
        result = tokenize_and_visualize(text)
        # Control characters should be escaped
        assert "\\u0000" in result
        assert "\\u0001" in result
        assert "\\u001f" in result
    
    def test_various_unicode_spaces(self):
        # Different Unicode space characters
        spaces = [
            "\u0020",  # Normal space
            "\u00A0",  # Non-breaking space
            "\u2000",  # En quad
            "\u2001",  # Em quad
            "\u2002",  # En space
            "\u2003",  # Em space
            "\u2009",  # Thin space
            "\u200A",  # Hair space
        ]
        
        for space in spaces:
            text = f"Hello{space}World"
            tokens = SimpleTokenizer().tokenize(text)
            assert len(tokens) == 3  # Hello, space, World
            
            # Check visualization
            viz = visualize_tokens(tokens)
            assert "â”‚" in viz
    
    def test_emoji_handling(self):
        emojis = [
            "ðŸ˜€",  # Simple emoji
            "ðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦",  # Family emoji (ZWJ sequence)
            "ðŸ³ï¸â€ðŸŒˆ",  # Rainbow flag (combining sequence)
            "ðŸ‘‹ðŸ¾",  # Waving hand with skin tone
        ]
        
        for emoji in emojis:
            # Character tokenizer should handle grapheme clusters
            char_tokens = CharTokenizer().tokenize(emoji)
            simple_tokens = SimpleTokenizer().tokenize(emoji)
            
            # Visualization should handle these without crashing
            viz = tokenize_and_visualize(emoji)
            assert len(viz) > 0
    
    @given(st.lists(st.text(), min_size=1))
    def test_custom_separator(self, tokens):
        # Test with different separators
        separators = ["â”‚", "|", "â†’", "Â·", "â€¢"]
        
        for sep in separators:
            result = visualize_tokens(tokens, separator=sep)
            if len(tokens) > 1:
                assert sep in result


class TestRealWorldExamples:
    def test_code_snippet(self):
        code = """def hello_world():
    print("Hello, ä¸–ç•Œ!")
    return 42"""
        
        result = tokenize_and_visualize(code)
        assert "def" in result
        assert "\n" in result or "\\n" in result  # Newlines should be visible
        assert "ä¸–ç•Œ" in result  # Unicode should be preserved
    
    def test_mixed_scripts(self):
        text = "English Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ä¸­æ–‡ æ—¥æœ¬èªž í•œêµ­ì–´ ×¢×‘×¨×™×ª"
        result = tokenize_and_visualize(text)
        
        # All scripts should be present
        assert "English" in result
        assert "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" in result
        assert "ä¸­æ–‡" in result
        assert "æ—¥æœ¬èªž" in result
        assert "í•œêµ­ì–´" in result
        assert "×¢×‘×¨×™×ª" in result
    
    def test_url_tokenization(self):
        url = "https://example.com/path/to/resource?param=value&other=123"
        
        simple = SimpleTokenizer().tokenize(url)
        word = WordTokenizer().tokenize(url)
        
        # Simple tokenizer should split on punctuation
        assert len(simple) > len(word)
        
        # Word tokenizer treats URL as one token
        assert len(word) == 1
    
    def test_mathematical_expressions(self):
        expr = "âˆ‘(i=1 to n) xÂ²+2Ï€r = âˆž"
        
        result = tokenize_and_visualize(expr)
        # Mathematical symbols should be preserved
        assert "âˆ‘" in result
        assert "Ï€" in result
        assert "âˆž" in result
        assert "Â²" in result