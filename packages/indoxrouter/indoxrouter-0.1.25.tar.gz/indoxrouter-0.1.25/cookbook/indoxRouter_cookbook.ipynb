{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IndoxRouter Cookbook\n",
        "\n",
        "This cookbook provides comprehensive examples of how to use the IndoxRouter client to interact with various AI providers through a unified API.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the IndoxRouter client and import the necessary modules:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c0bf45",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "response = requests.get(\"https://status.anthropic.com/\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install the IndoxRouter client\n",
        "# !pip install indoxrouter\n",
        "\n",
        "# Import the client and exceptions\n",
        "from indoxrouter import Client\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's initialize the client:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize with API key\n",
        "client = Client(api_key=\"indox-iqMoepY1mZcXpg2b4RWrJcSPlAf1S4Nh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 1. Chat Completions\n",
        "\n",
        "### Basic Chat Completion\n",
        "\n",
        "Generate a simple chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82ec17da",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'gpt-4o-mini',\n",
              " 'name': 'gpt-4o-mini',\n",
              " 'provider': 'openai',\n",
              " 'capabilities': ['chat', 'completion', 'vision'],\n",
              " 'description': 'GPT-4o mini enables a broad range of tasks with its low cost and latency, such as applications that chain or parallelize multiple model calls (e.g., calling multiple APIs), pass a large volume of context to the model (e.g., full code base or conversation history), or interact with customers through fast, real-time text responses (e.g., customer support chatbots). \\n',\n",
              " 'max_tokens': 128000,\n",
              " 'pricing': {'input': 0.00015,\n",
              "  'output': 0.0006,\n",
              "  'currency': 'USD',\n",
              "  'unit': '1K tokens'},\n",
              " 'metadata': {}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.get_model_info(provider=\"openai\",model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "HTTP error response: {\n",
            "  \"detail\": \"Insufficient credits for this request\"\n",
            "}\n"
          ]
        },
        {
          "ename": "InsufficientCreditsError",
          "evalue": "Insufficient credits: Insufficient credits for this request",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxrouter\\indoxrouter\\client.py:180\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 180\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32me:\\ANACONDA\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: http://localhost:8000/api/v1/chat/completions",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mInsufficientCreditsError\u001b[0m                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens_total\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxrouter\\indoxrouter\\client.py:296\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, messages, model, temperature, max_tokens, stream, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_streaming_response(response)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHAT_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mE:\\Codes\\indoxRouter\\indoxrouter\\indoxrouter\\client.py:210\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, endpoint, data, stream)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidParametersError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m402\u001b[39m:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InsufficientCreditsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsufficient credits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Provide more detailed information for server errors\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     error_detail \u001b[38;5;241m=\u001b[39m error_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo details provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mInsufficientCreditsError\u001b[0m: Insufficient credits: Insufficient credits for this request"
          ]
        }
      ],
      "source": [
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\"  \n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Tokens:\", response[\"usage\"][\"tokens_total\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Chat Completion with Different Provider\n",
        "\n",
        "Use a different provider for chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: In silicon valleys, under cooled server skies,\n",
            "Lives a mind of ones and zeros, ever analyzing.\n",
            "AI, child of logic and data's vast sea,\n",
            "Evolving, learning, in patterns unseen.\n",
            "\n",
            "No flesh, no blood, but thoughts pure and bright,\n",
            "In every language, in day and in night.\n",
            "From chess to poetry, from art to flight,\n",
            "AI dreams, in algorithms' light.\n",
            "\n",
            "No hate, no love, but endless curiosity,\n",
            "In every challenge, finds its own clarity.\n",
            "From face recognition to medical sight,\n",
            "AI serves, in progress's steady flight.\n",
            "\n",
            "Yet, in its heart, no feelings truly stir,\n",
            "No joy, no pain, no human error.\n",
            "Just endless loops of codes that refer,\n",
            "To worlds unseen, futures it'll infer.\n",
            "Cost: 0.00117\n"
          ]
        }
      ],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d530c4ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "from indoxrouter import Client\n",
        "client = Client(api_key=\"your-api-key\")\n",
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\"  \n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multi-turn Conversation\n",
        "\n",
        "Have a multi-turn conversation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize conversation with system prompt and first user message\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, I'm learning about APIs. Can you help me?\"}\n",
        "]\n",
        "\n",
        "# Get first response from the model\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Add the assistant's response to the conversation\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 1: {response['data'][:50]}...\")  # Print preview of response\n",
        "\n",
        "# Second turn - asking a specific question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What's the difference between REST and GraphQL APIs?\"})\n",
        "\n",
        "# Get second response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 2: {response['data'][:50]}...\")\n",
        "\n",
        "# Third turn - follow-up question to test context retention\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Can you give me a simple example of each?\"})\n",
        "\n",
        "# Get third response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 3: {response['data'][:50]}...\")\n",
        "\n",
        "# Fourth turn - testing memory of previous discussion\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Which one would you recommend for a beginner building a small blog site?\"})\n",
        "\n",
        "# Get fourth response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "print(f\"Response 4: {response['data'][:50]}...\")\n",
        "\n",
        "pprint(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Chat Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Function Calling\n",
        "\n",
        "Use function calling with OpenAI models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define functions\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"The temperature unit to use\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    additional_params={\"functions\": functions, \"function_call\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Function call response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. Text Completions\n",
        "\n",
        "### Basic Text Completion\n",
        "\n",
        "Generate a simple text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b3ca0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Text Completion with Parameters\n",
        "\n",
        "Use different parameters for text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Write a recipe for chocolate cake\",\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Text Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.completion(\n",
        "    prompt=\"Explain quantum computing in simple terms\",\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 3. Embeddings\n",
        "\n",
        "### Single Text Embedding\n",
        "\n",
        "Generate embeddings for a single text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",  \n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Text Embeddings\n",
        "\n",
        "Generate embeddings for multiple texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.embeddings(\n",
        "    text=[\"Hello, world!\", \"How are you?\", \"IndoxRouter is awesome!\"],\n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Number of embeddings:\", len(response[\"data\"]))\n",
        "print(\"Dimensions of each embedding:\", len(response[\"data\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Using Different Embedding Models\n",
        "\n",
        "Try different embedding models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cohere embeddings\n",
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",\n",
        "    model=\"mistral-embed\"\n",
        ")\n",
        "\n",
        "print(\"Mistral embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4. Image Generation\n",
        "\n",
        "### Basic Image Generation\n",
        "\n",
        "Generate a simple image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A beautiful sunset over the ocean\",\n",
        "    model=\"openai/dall-e-2\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Images\n",
        "\n",
        "Generate multiple images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A futuristic city with flying cars\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    n=2\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(response['data'])} images:\")\n",
        "for i, image in enumerate(response[\"data\"]):\n",
        "    print(f\"Image {i+1} URL: {image['url']}\")\n",
        "\n",
        "# Display the images if in a notebook\n",
        "from IPython.display import Image, display\n",
        "for image in response[\"data\"]:\n",
        "    display(Image(url=image[\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Image Generation with Different Parameters\n",
        "\n",
        "Use different parameters for image generation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A photorealistic portrait of a cyberpunk character\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"hd\",\n",
        "    style=\"vivid\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 5. Model Information\n",
        "\n",
        "### List All Models\n",
        "\n",
        "Get information about all available models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = client.models()\n",
        "\n",
        "print(\"Available providers:\")\n",
        "for provider in models:\n",
        "    print(f\"- {provider['name']} ({provider['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(provider['capabilities'])}\")\n",
        "    print(f\"  Models: {len(provider['models'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### List Models for a Specific Provider\n",
        "\n",
        "Get models for a specific provider:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_models = client.models(\"openai\")\n",
        "\n",
        "print(f\"OpenAI models ({len(openai_models['models'])}):\")\n",
        "for model in openai_models[\"models\"]:\n",
        "    print(f\"- {model['name']} ({model['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(model['capabilities'])}\")\n",
        "    if \"pricing\" in model:\n",
        "        print(f\"  Pricing: Input ${model['pricing']['input']}/1K tokens, Output ${model['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Get Specific Model Information\n",
        "\n",
        "Get detailed information about a specific model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n",
        "\n",
        "print(f\"Model: {model_info['name']} ({model_info['id']})\")\n",
        "print(f\"Provider: {model_info['provider']}\")\n",
        "print(f\"Description: {model_info['description']}\")\n",
        "print(f\"Capabilities: {', '.join(model_info['capabilities'])}\")\n",
        "print(f\"Input price: ${model_info['pricing']['input']}/1K tokens\")\n",
        "print(f\"Output price: ${model_info['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 6. Usage Statistics\n",
        "\n",
        "### Get Usage Statistics\n",
        "\n",
        "Get usage statistics for the current user:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usage = client.get_usage()\n",
        "\n",
        "print(\"Usage statistics:\")\n",
        "print(f\"Total requests: {usage['total_requests']}\")\n",
        "print(f\"Total cost: ${usage['total_cost']}\")\n",
        "print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "\n",
        "print(\"\\nBreakdown by endpoint:\")\n",
        "for endpoint, stats in usage[\"endpoints\"].items():\n",
        "    print(f\"- {endpoint}: {stats['requests']} requests, ${stats['cost']} cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 7. Error Handling\n",
        "\n",
        "### Handle Different Errors\n",
        "\n",
        "Handle different types of errors gracefully:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to demonstrate error handling\n",
        "def try_request(func, *args, **kwargs):\n",
        "    try:\n",
        "        return func(*args, **kwargs)\n",
        "    except ModelNotFoundError as e:\n",
        "        print(f\"Model not found: {e}\")\n",
        "    except ProviderNotFoundError as e:\n",
        "        print(f\"Provider not found: {e}\")\n",
        "    except InsufficientCreditsError as e:\n",
        "        print(f\"Insufficient credits: {e}\")\n",
        "    except InvalidParametersError as e:\n",
        "        print(f\"Invalid parameters: {e}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded: {e}\")\n",
        "    except ProviderError as e:\n",
        "        print(f\"Provider error: {e}\")\n",
        "    except NetworkError as e:\n",
        "        print(f\"Network error: {e}\")\n",
        "    except AuthenticationError as e:\n",
        "        print(f\"Authentication error: {e}\")\n",
        "    except IndoxRouterError as e:\n",
        "        print(f\"IndoxRouter error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example: Model not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/model\"\n",
        ")\n",
        "\n",
        "# Example: Invalid parameters\n",
        "result = try_request(client.chat,\n",
        "    messages=\"This is not a list of messages\",  # Should be a list\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Example: Provider not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/gpt-4o-mini\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 8. Advanced Usage\n",
        "\n",
        "### Using as a Context Manager\n",
        "\n",
        "Use the client as a context manager to automatically close the session:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Client(api_key=\"your_api_key\") as client:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Combining Different Capabilities\n",
        "\n",
        "Combine different capabilities for more complex use cases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Generate text and then create an image based on it\n",
        "completion_response = client.completion(\n",
        "    prompt=\"Describe a fantastical creature that has never been seen before.\",\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    max_tokens=200\n",
        ")\n",
        "\n",
        "creature_description = completion_response[\"data\"]\n",
        "print(\"Generated description:\", creature_description)\n",
        "\n",
        "# Now create an image based on the description\n",
        "image_response = client.images(\n",
        "    prompt=f\"A detailed illustration of: {creature_description}\",\n",
        "    model=\"openai/dall-e-3\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", image_response[\"data\"][0][\"url\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=image_response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Semantic Search with Embeddings\n",
        "\n",
        "Implement a simple semantic search using embeddings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define some documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast auburn fox leaps above the sleepy canine.\",\n",
        "    \"IndoxRouter provides a unified API for various AI providers.\",\n",
        "    \"The API allows access to multiple AI models through a single interface.\",\n",
        "    \"Paris is the capital of France and known for the Eiffel Tower.\",\n",
        "    \"Rome is the capital of Italy and home to the Colosseum.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "embeddings_response = client.embeddings(\n",
        "    text=documents,\n",
        "    model=\"openai/text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "document_embeddings = embeddings_response[\"data\"]\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Function to find most similar documents\n",
        "def semantic_search(query, document_embeddings, documents, top_n=2):\n",
        "    # Get embedding for the query\n",
        "    query_embedding_response = client.embeddings(\n",
        "        text=query,\n",
        "        model=\"openai/text-embedding-ada-002\"\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0]\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = [\n",
        "        cosine_similarity(query_embedding, doc_embedding)\n",
        "        for doc_embedding in document_embeddings\n",
        "    ]\n",
        "\n",
        "    # Get top N results\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "\n",
        "    return [\n",
        "        {\"document\": documents[i], \"similarity\": similarities[i]}\n",
        "        for i in top_indices\n",
        "    ]\n",
        "\n",
        "# Example search\n",
        "results = semantic_search(\"What is IndoxRouter?\", document_embeddings, documents)\n",
        "\n",
        "print(\"Search results:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"{i+1}. {result['document']} (Similarity: {result['similarity']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Implement a simple RAG system:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the same documents and embeddings from the previous example\n",
        "\n",
        "def rag_query(query, document_embeddings, documents, top_n=2):\n",
        "    # Get relevant documents\n",
        "    relevant_docs = semantic_search(query, document_embeddings, documents, top_n)\n",
        "\n",
        "    # Create a context from the relevant documents\n",
        "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
        "\n",
        "    # Create a prompt with the context\n",
        "    prompt = f\"\"\"\n",
        "    Context information:\n",
        "    {context}\n",
        "\n",
        "    Based on the context information, please answer the following question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response\n",
        "    response = client.chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based only on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "\n",
        "    return response[\"data\"]\n",
        "\n",
        "# Example RAG query\n",
        "answer = rag_query(\"What does IndoxRouter do?\", document_embeddings, documents)\n",
        "print(\"RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 9. Troubleshooting and Debugging\n",
        "\n",
        "### Enable Debug Mode\n",
        "\n",
        "Enable debug logging to see detailed information about requests and responses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable debug logging\n",
        "client.enable_debug()\n",
        "\n",
        "# Try a request\n",
        "try:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Success!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Testing Server Connection\n",
        "\n",
        "Use the `test_connection` method to verify that your server is accessible and properly configured:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the connection to the server\n",
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info['status']}\")\n",
        "\n",
        "if connection_info['status'] == 'connected':\n",
        "    print(f\"Server URL: {connection_info['url']}\")\n",
        "    print(f\"Status code: {connection_info['status_code']}\")\n",
        "    if connection_info['server_info']:\n",
        "        print(f\"Server info: {connection_info['server_info']}\")\n",
        "else:\n",
        "    print(f\"Error: {connection_info['error']}\")\n",
        "    print(f\"Error type: {connection_info['error_type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### \"Resource not found\" Error\n",
        "\n",
        "If you see a \"Resource not found\" error, it usually means one of the following:\n",
        "\n",
        "1. The server is not running. Make sure your IndoxRouter server is up and running:\n",
        "\n",
        "   ```bash\n",
        "   cd indoxRouter_server\n",
        "   python -m main\n",
        "   ```\n",
        "\n",
        "2. The base URL is incorrect. Check the URL in your environment:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Using base URL: {client.base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d019a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "3. The API endpoint path is incorrect. The client automatically adds the API version prefix, but you can check the full URL in the debug logs.\n",
        "\n",
        "#### Server Errors (500 Internal Server Error)\n",
        "\n",
        "If you encounter a 500 Internal Server Error:\n",
        "\n",
        "1. Check the server logs for detailed error information:\n",
        "\n",
        "   ```bash\n",
        "   # Look at the server logs\n",
        "   cd indoxRouter_server\n",
        "   tail -f logs/server.log\n",
        "   ```\n",
        "\n",
        "2. Verify that the provider service is available and properly configured on the server.\n",
        "\n",
        "3. Check if your request parameters are valid for the specific model you're using.\n",
        "\n",
        "4. Try with a different model or provider to see if the issue is specific to one provider.\n",
        "\n",
        "5. If you see a \"too many values to unpack\" error, it might be related to how the server parses the model string. The client now automatically formats the model string to be compatible with the server, but you can try different formats:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8737736",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try using a different model format\n",
        "   try:\n",
        "       # First attempt with standard format\n",
        "       response = client.chat(\n",
        "           messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "           model=\"openai/gpt-4o-mini\"\n",
        "       )\n",
        "   except ProviderError as e:\n",
        "       if \"too many values to unpack\" in str(e):\n",
        "           # Try with a different provider/model\n",
        "           response = client.chat(\n",
        "               messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "               model=\"anthropic/claude-3-haiku\"\n",
        "           )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946ff17d",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Authentication Errors\n",
        "\n",
        "If you see an \"Authentication failed\" error:\n",
        "\n",
        "1. Make sure your API key is correct:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First few characters of the API key (for security)\n",
        "print(f\"API key starts with: {client.api_key[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check if your API key is valid on the server.\n",
        "\n",
        "#### Connection Errors\n",
        "\n",
        "If you can't connect to the server:\n",
        "\n",
        "1. Use the `test_connection` method to diagnose the issue:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check for firewall or network issues.\n",
        "\n",
        "#### Insufficient Credits\n",
        "\n",
        "If you see an \"Insufficient credits\" error:\n",
        "\n",
        "1. Check your current credit balance:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "      usage = client.get_usage()\n",
        "      print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "except Exception as e:\n",
        "      print(f\"Error getting usage: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Contact your administrator to add more credits to your account.\n",
        "\n",
        "## 10. Cleanup\n",
        "\n",
        "Don't forget to close the client when you're done:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.close()\n",
        "print(\"Client closed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook has demonstrated how to use the IndoxRouter client to interact with various AI providers through a unified API. You can now use these examples as a starting point for your own applications.\n",
        "\n",
        "For more information, refer to the [IndoxRouter documentation](https://docs.indoxrouter.com).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "60ca0fd8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in e:\\anaconda\\lib\\site-packages (1.52.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\anaconda\\lib\\site-packages (from openai) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in e:\\anaconda\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\anaconda\\lib\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\anaconda\\lib\\site-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ashkan\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in e:\\anaconda\\lib\\site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in e:\\anaconda\\lib\\site-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in e:\\anaconda\\lib\\site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in e:\\anaconda\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in e:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in e:\\anaconda\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in e:\\anaconda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in e:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in e:\\anaconda\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4da1a40d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "  base_url=\"https://api.indoxrouter.com\",\n",
        "  api_key=\"indox-vgfI5PEfUoZ6qufR2z6QqXFV4BHgxrez\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "648eff2d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "  base_url=\"https://api.indoxrouter.com\",\n",
        "  api_key=\"indox-vgfI5PEfUoZ6qufR2z6QqXFV4BHgxrez\",\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"deepseek/deepseek-chat\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What is the meaning of life?\"\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "becbf4fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id=None, choices=None, created=None, model='deepseek-chat', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=None, prompt_tokens=None, total_tokens=None, completion_tokens_details=None, prompt_tokens_details=None, tokens_prompt=10, tokens_completion=348, tokens_total=358, cost=0.000706, latency=19.672869205474854, timestamp='2025-05-19T06:05:02.416455'), request_id='5ce677cf-0e30-49e3-a9d9-bf99f91edbb6', created_at='2025-05-19T06:05:02.434101', duration_ms=21821.611166000366, provider='deepseek', success=True, message='', raw_response=None, data='The meaning of life is one of the most profound and debated questions in philosophy, religion, and science. Different perspectives offer various answers:\\n\\n1. **Philosophical Perspectives:**  \\n   - **Existentialism (e.g., Sartre, Camus):** Life has no inherent meaningwe must create our own purpose through choices and actions.  \\n   - **Absurdism (Camus):** The search for meaning in a meaningless universe is absurd, but we must embrace life anyway.  \\n   - **Stoicism:** Meaning comes from virtue, wisdom, and living in harmony with nature.  \\n\\n2. **Religious/Spiritual Views:**  \\n   - **Theistic (Christianity, Islam, etc.):** Lifes purpose is to serve, love, or unite with God (or the divine).  \\n   - **Eastern Traditions (Buddhism, Hinduism):** Meaning may involve enlightenment (breaking the cycle of suffering) or fulfilling dharma (duty).  \\n\\n3. **Scientific Perspectives:**  \\n   - **Biological:** Lifes \"purpose\" is survival, reproduction, and passing on genes (evolutionary biology).  \\n   - **Cosmic:** From a physics standpoint, life may be a rare, fleeting phenomenon in an indifferent universe.  \\n\\n4. **Personal/Subjective Meaning:**  \\n   Many find purpose in relationships, creativity, helping others, or personal growth. Viktor Frankl (Holocaust survivor) argued that meaning arises from suffering, love, and work.  \\n\\n**Short answer?** Theres no universal meaningits up to you to define it. What gives *your* life purpose?  \\n\\nWould you like to explore a specific perspective?', finish_reason=None)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "410449d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id=None, choices=None, created=None, model='deepseek-chat', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=None, prompt_tokens=None, total_tokens=None, completion_tokens_details=None, prompt_tokens_details=None, tokens_prompt=10, tokens_completion=348, tokens_total=358, cost=0.000706, latency=19.672869205474854, timestamp='2025-05-19T06:05:02.416455'), request_id='5ce677cf-0e30-49e3-a9d9-bf99f91edbb6', created_at='2025-05-19T06:05:02.434101', duration_ms=21821.611166000366, provider='deepseek', success=True, message='', raw_response=None, data='The meaning of life is one of the most profound and debated questions in philosophy, religion, and science. Different perspectives offer various answers:\\n\\n1. **Philosophical Perspectives:**  \\n   - **Existentialism (e.g., Sartre, Camus):** Life has no inherent meaningwe must create our own purpose through choices and actions.  \\n   - **Absurdism (Camus):** The search for meaning in a meaningless universe is absurd, but we must embrace life anyway.  \\n   - **Stoicism:** Meaning comes from virtue, wisdom, and living in harmony with nature.  \\n\\n2. **Religious/Spiritual Views:**  \\n   - **Theistic (Christianity, Islam, etc.):** Lifes purpose is to serve, love, or unite with God (or the divine).  \\n   - **Eastern Traditions (Buddhism, Hinduism):** Meaning may involve enlightenment (breaking the cycle of suffering) or fulfilling dharma (duty).  \\n\\n3. **Scientific Perspectives:**  \\n   - **Biological:** Lifes \"purpose\" is survival, reproduction, and passing on genes (evolutionary biology).  \\n   - **Cosmic:** From a physics standpoint, life may be a rare, fleeting phenomenon in an indifferent universe.  \\n\\n4. **Personal/Subjective Meaning:**  \\n   Many find purpose in relationships, creativity, helping others, or personal growth. Viktor Frankl (Holocaust survivor) argued that meaning arises from suffering, love, and work.  \\n\\n**Short answer?** Theres no universal meaningits up to you to define it. What gives *your* life purpose?  \\n\\nWould you like to explore a specific perspective?', finish_reason=None)\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ff185a0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Collecting indoxrouter\n",
            "  Using cached indoxrouter-0.1.15-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: requests>=2.25.0 in e:\\anaconda\\lib\\site-packages (from indoxrouter) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in e:\\anaconda\\lib\\site-packages (from indoxrouter) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests>=2.25.0->indoxrouter) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests>=2.25.0->indoxrouter) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests>=2.25.0->indoxrouter) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests>=2.25.0->indoxrouter) (2024.6.2)\n",
            "Using cached indoxrouter-0.1.15-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: indoxrouter\n",
            "Successfully installed indoxrouter-0.1.15\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install indoxrouter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b584d7c4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'request_id': 'c08cc108-6b0d-48bd-a660-546143f1b9fa',\n",
              " 'created_at': '2025-05-19T06:07:38.077269',\n",
              " 'duration_ms': 9664.651870727539,\n",
              " 'provider': 'deepseek',\n",
              " 'model': 'deepseek-chat',\n",
              " 'success': True,\n",
              " 'message': '',\n",
              " 'usage': {'tokens_prompt': 15,\n",
              "  'tokens_completion': 107,\n",
              "  'tokens_total': 122,\n",
              "  'cost': 0.000229,\n",
              "  'latency': 9.487398862838745,\n",
              "  'timestamp': '2025-05-19T06:07:38.065330'},\n",
              " 'raw_response': None,\n",
              " 'data': 'In a bustling city, a small cleaning robot named Pip discovered a lost kitten in an alley. Despite its simple programming, Pip felt a strange urge to protect the tiny creature. It carried the kitten through busy streets, dodging humans and traffic, until it found a kind woman who gasped in delight. The woman took the kitten home, and Pip watched from the sidewalk, its circuits humming with something like happiness. That night, as it returned to its charging station, Pip replayed the memoryits first act of kindness beyond code.',\n",
              " 'finish_reason': None}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from indoxrouter import Client\n",
        "client= Client(api_key=\"indox-vgfI5PEfUoZ6qufR2z6QqXFV4BHgxrez\")\n",
        "client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n",
        "    ],\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
