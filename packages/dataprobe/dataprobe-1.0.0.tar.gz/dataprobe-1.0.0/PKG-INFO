Metadata-Version: 2.4
Name: dataprobe
Version: 1.0.0
Summary: Advanced data pipeline debugging and profiling tools for Python
Home-page: https://github.com/santhoshkrishnan30/dataprobe
Author: SANTHOSH KRISHNAN R
Author-email: santhoshkrishnan3006@gmail.com
Project-URL: Bug Tracker, https://github.com/santhoshkrishnan30/dataprobe/issues
Project-URL: Documentation, https://dataprobe.readthedocs.io
Project-URL: Source Code, https://github.com/santhoshkrishnan30/dataprobe
Keywords: data-pipeline,debugging,profiling,data-engineering,etl,data-lineage,memory-profiling,pandas,polars
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Information Analysis
Classifier: Topic :: Software Development :: Debuggers
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=1.21.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: polars>=0.19.0
Requires-Dist: scikit-learn>=1.0.0
Requires-Dist: matplotlib>=3.5.0
Requires-Dist: seaborn>=0.12.0
Requires-Dist: rich>=13.0.0
Requires-Dist: click>=8.0.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: memory-profiler>=0.60.0
Requires-Dist: graphviz>=0.20.0
Requires-Dist: networkx>=2.8.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=4.0.0; extra == "dev"
Requires-Dist: mypy>=0.990; extra == "dev"
Requires-Dist: sphinx>=4.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "dev"
Provides-Extra: all
Requires-Dist: sqlalchemy>=2.0.0; extra == "all"
Requires-Dist: pyarrow>=10.0.0; extra == "all"
Requires-Dist: jupyter>=1.0.0; extra == "all"
Requires-Dist: ipython>=8.0.0; extra == "all"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

ï»¿## Project description

# DataProbe

**DataProbe** is a comprehensive Python toolkit for debugging, profiling, and optimizing data pipelines. It provides powerful tools to track data lineage, identify bottlenecks, monitor memory usage, and visualize pipeline execution flow with **enterprise-grade visualizations**.

## ğŸ¨ **NEW: Enterprise-Grade Visualizations**

DataProbe v1.0.0 introduces comprehensive pipeline debugging capabilities with professional-quality visualizations, intelligent optimization recommendations, advanced memory profiling, data lineage tracking, and enterprise-grade reporting.

### **Dashboard Features**

#### ğŸ¢ **Enterprise Dashboard**

- **KPI Panels**: Real-time success rates, duration, memory usage
- **Pipeline Flowchart**: Interactive operation flow with status indicators
- **Performance Analytics**: Memory usage timelines with peak detection
- **Data Insights**: Comprehensive lineage and transformation tracking

```python
# Generate enterprise dashboard
debugger.visualize_pipeline()
```

#### ğŸŒ **3D Pipeline Network**

- **3D Visualization**: Interactive network showing operation relationships
- **Performance Mapping**: Z-axis represents operation duration
- **Status Color-coding**: Visual error and bottleneck identification

```python
# Create 3D network visualization
debugger.create_3d_pipeline_visualization()
```

#### ğŸ“Š **Executive Reports**

- **Multi-page Reports**: Professional stakeholder-ready documentation
- **Performance Trends**: Dual-axis charts showing duration and memory patterns
- **Optimization Recommendations**: AI-powered suggestions for improvements
- **Data Quality Metrics**: Comprehensive pipeline health scoring

```python
# Generate executive report
debugger.generate_executive_report()
```

### **Color-Coded Status System**

- ğŸŸ¢ **Success**: Operations completed without issues
- ğŸŸ¡ **Warning**: Performance bottlenecks detected
- ğŸ”´ **Error**: Failed operations requiring attention
- ğŸŸ¦ **Info**: Data flow and transformation indicators

## ğŸš€ Features

### PipelineDebugger

* **ğŸ” Operation Tracking** : Automatically track execution time, memory usage, and data shapes for each operation
* **ğŸ“Š Enterprise-Grade Visualizations** : Professional dashboards, 3D networks, and executive reports
* **ğŸ’¾ Memory Profiling** : Monitor memory usage and identify memory-intensive operations
* **ğŸ”— Data Lineage** : Track data transformations and column changes throughout the pipeline
* **âš ï¸ Bottleneck Detection** : Automatically identify slow operations and memory peaks
* **ğŸ“ˆ Performance Reports** : Generate comprehensive debugging reports with optimization suggestions
* **ğŸ¯ Error Tracking** : Capture and track errors with full traceback information
* **ğŸŒ³ Nested Operations** : Support for tracking nested function calls and their relationships

## ğŸ“¦ Installation

```bash
pip install dataprobe
```

For development installation:

```bash
git clone https://github.com/santhoshkrishnan30/dataprobe.git
cd dataprobe
pip install -e ".[dev]"
```

## ğŸ¯ Quick Start

### Basic Usage with Enhanced Visualizations

```python
from dataprobe import PipelineDebugger
import pandas as pd

# Initialize the debugger with enhanced features
debugger = PipelineDebugger(
    name="My_ETL_Pipeline",
    track_memory=True,
    track_lineage=True
)

# Use decorators to track operations
@debugger.track_operation("Load Data")
def load_data(file_path):
    return pd.read_csv(file_path)

@debugger.track_operation("Transform Data")
def transform_data(df):
    df['new_column'] = df['value'] * 2
    return df

# Run your pipeline
df = load_data("data.csv")
df = transform_data(df)

# Generate enterprise-grade visualizations
debugger.visualize_pipeline()              # Enterprise dashboard
debugger.create_3d_pipeline_visualization() # 3D network view  
debugger.generate_executive_report()       # Executive report

# Get AI-powered optimization suggestions
suggestions = debugger.suggest_optimizations()
for suggestion in suggestions:
    print(f"ğŸ’¡ {suggestion['suggestion']}")

# Print summary and reports
debugger.print_summary()
report = debugger.generate_report()
```

### Memory Profiling

```python
@debugger.profile_memory
def memory_intensive_operation():
    large_df = pd.DataFrame(np.random.randn(1000000, 50))
    result = large_df.groupby(large_df.index % 1000).mean()
    return result
```

### DataFrame Analysis

```python
# Analyze DataFrames for potential issues
debugger.analyze_dataframe(df, name="Sales Data")
```

## ğŸ“Š Example Output

### Enterprise Dashboard

Professional KPI dashboard with real-time metrics, pipeline flowchart, memory analytics, and performance insights.

### Pipeline Summary

```
Pipeline Summary: My_ETL_Pipeline
â”œâ”€â”€ Execution Statistics
â”‚   â”œâ”€â”€ Total Operations: 5
â”‚   â”œâ”€â”€ Total Duration: 2.34s
â”‚   â””â”€â”€ Total Memory Used: 125.6MB
â”œâ”€â”€ Bottlenecks (1)
â”‚   â””â”€â”€ Transform Data: 1.52s
â””â”€â”€ Memory Peaks (1)
    â””â”€â”€ Load Large Dataset: +85.3MB
```

### Optimization Suggestions

```
ğŸ’¡ OPTIMIZATION RECOMMENDATIONS:

1. [PERFORMANCE] Transform Data
   Issue: Operation took 1.52s
   ğŸ’¡ Consider optimizing this operation or parallelizing if possible

2. [MEMORY] Load Large Dataset  
   Issue: High memory usage: +85.3MB
   ğŸ’¡ Consider processing data in chunks or optimizing memory usage
```

## ğŸ”§ Advanced Features

### Multiple Visualization Options

```python
# Enterprise dashboard - Professional KPI dashboard
debugger.visualize_pipeline()

# 3D network visualization - Interactive operation relationships  
debugger.create_3d_pipeline_visualization()

# Executive report - Multi-page stakeholder documentation
debugger.generate_executive_report()
```

### Data Lineage Tracking

```python
# Export data lineage information
lineage_json = debugger.export_lineage(format="json")

# Track column changes automatically
@debugger.track_operation("Add Features")
def add_features(df):
    df['feature_1'] = df['value'].rolling(7).mean()
    df['feature_2'] = df['value'].shift(1)
    return df
```

### Custom Metadata

```python
@debugger.track_operation("Process Batch", batch_id=123, source="api")
def process_batch(data):
    # Operation metadata is stored and included in reports
    return processed_data
```

### Checkpoint Saving

```python
# Auto-save is enabled by default
debugger = PipelineDebugger(name="Pipeline", auto_save=True)

# Manual checkpoint
debugger.save_checkpoint()
```

## ğŸ“ˆ Performance Tips

1. **Use with Context** : The debugger adds minimal overhead, but for production pipelines, you can disable tracking:

```python
   debugger = PipelineDebugger(name="Pipeline", track_memory=False, track_lineage=False)
```

2. **Batch Operations** : Group small operations together to reduce tracking overhead
3. **Memory Monitoring** : Set appropriate memory thresholds to catch issues early:

```python
   debugger = PipelineDebugger(name="Pipeline", memory_threshold_mb=500)
```

## ğŸ’¼ **Enterprise Features**

âœ… **Professional Styling**: Modern design matching enterprise standards
âœ… **Executive Ready**: Suitable for stakeholder presentations
âœ… **Performance Insights**: AI-powered optimization recommendations
âœ… **Export Options**: High-resolution PNG outputs
âœ… **Responsive Design**: Scales from detailed debugging to executive overview
âœ… **Real-time Metrics**: Live performance and memory tracking

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](https://github.com/santhoshkrishnan30/dataprobe/blob/main/LICENSE) file for details.

## ğŸ™ Acknowledgments

* Built with [Rich](https://github.com/Textualize/rich) for beautiful terminal output
* Uses [NetworkX](https://networkx.org/) for pipeline visualization
* Enhanced with [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) for enterprise-grade visualizations
* Inspired by the need for better data pipeline debugging tools

## ğŸ“ Support

* ğŸ“§ Email: [santhoshkrishnan3006@gmail.com](mailto:santhoshkrishnan3006@gmail.com)
* ğŸ› Issues: [GitHub Issues](https://github.com/santhoshkrishnan30/dataprobe/issues)
* ğŸ“– Documentation: [Read the Docs](https://dataprobe.readthedocs.io/)

## ğŸ—ºï¸ Roadmap

* [X] Enterprise-grade dashboard visualizations
* [X] 3D pipeline network views
* [X] Executive-level reporting capabilities
* [ ] Support for distributed pipeline debugging
* [ ] Integration with popular orchestration tools (Airflow, Prefect, Dagster)
* [ ] Real-time pipeline monitoring dashboard
* [ ] Advanced anomaly detection in data flow
* [ ] Support for streaming data pipelines

---

Made with â¤ï¸ by Santhosh Krishnan R
