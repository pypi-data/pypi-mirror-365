Metadata-Version: 2.3
Name: ragbee-fw
Version: 0.1.3
Summary: Lightweight RAG framework
License: MIT
Keywords: rag,llm,retrieval,framework
Author: shevchenko v.a.
Author-email: droogg@mail.ru
Requires-Python: >=3.10
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: charset-normalizer (>=3.4.2,<4.0.0)
Requires-Dist: huggingface-hub (>=0.31.1,<0.32.0)
Requires-Dist: openai (>=1.97.0,<2.0.0)
Requires-Dist: pydantic (>=2.11.7,<3.0.0)
Requires-Dist: rank-bm25 (>=0.2.2,<0.3.0)
Requires-Dist: typer (>=0.15.4,<0.16.0)
Project-URL: Homepage, https://github.com/droogg/ragbee_fw
Project-URL: Repository, https://github.com/droogg/ragbee_fw
Description-Content-Type: text/markdown

# RAGBee FW

*Lightweight Retrieval-Augmented-Generation framework for SMEs.*

RAGBee helps you **load documents â†’ split â†’ retrieve â†’ generate**  
with any LLM (OpenAI, HF Inference API, local vLLM/Triton).  
Ports & Adapters + DI give you clean extensibility; CLI lets you run the pipeline in three commands.

---

## âœ¨ Features

- **Clean architecture** â€” `core.ports` (contracts) + `infrastructure.*` (plug-ins)  
- **Dependency Injection** â€” YAML config â‡’ container wires loader / splitter / retriever / LLM  
- **CLI (`ragbee_cli`)** â€” `ingest`, `ask`, `shell` out of the box  
- **LLM agnostic** â€” OpenAI, HuggingFace Hub, vLLM, Triton â€¦ or your own adapter  
- **Composable** â€” embed in FastAPI, Telegram-bot, Streamlit, Airflow DAG  
- **MIT license** â€” free for commercial use

---

## ðŸš€ Quick start

```bash
pip install ragbee-fw           # 1. install

ragbee_cli ingest config.yml    # 2. build index
ragbee_cli ask config.yml "Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ RAG?"   # 3. ask
ragbee_cli shell config.yml     # â€¦or interactive REPL
```

#### config.yml (minimal):

```yaml
data_loader:
  type: file_loader
  path: ./data

text_chunker:
  type: recursive_splitter
  chunk_size: 500
  chunk_overlap: 100

retriever:
  type: bm25
  top_k: 3

llm:
  type: hf
  model_name: gpt-3.5-turbo
  token: ${env:HF_TOKEN}
```

## ðŸ§‘â€ðŸ’» Python API

```python

from ragbee_fw import DIContainer, load_config

cfg = load_config("config.yml")
container = DIContainer(cfg)

# 1) build / update index
ingestion = container.build_ingestion_service()
ingestion.build_index()                # or .update_index()

# 2) answer questions
answer = container.build_answer_service()
print(answer.generate_answer("What is RAG?", top_k=3))
```

## ðŸ—º Architecture (Clean / Hexagonal)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      CLI / UI     â”‚  â†  FastAPI, Streamlit, Telegram Bot â€¦
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ adapter
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Application    â”‚  â†  DI container, services
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ ports
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Core         â”‚  â†  pure dataclasses, protocols
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ adapters
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Infrastructure    â”‚  â†  FS-loader, Splitter, BM25, HF LLM â€¦
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“š Documentation

Docs & API â€” [README](https://github.com/droogg/ragbee_fw/blob/main/README.md)

Examples â€” [example/](https://github.com/droogg/ragbee_fw/tree/main/example)


## ðŸ¤ Contributing

1. Fork â†’ clone â†’ poetry install

2. Format code with black . && isort .

3. Submit PR â†’ CI will run lint & tests

See ```CONTRIBUTING.md``` for full guide.

### ðŸ“œ License
MIT Â© V.A. Shevchenko â€” free for any purpose, commercial or private.

