Metadata-Version: 2.4
Name: llmabba
Version: 0.0.5
Summary: LLM-ABBA: mining time series via symbolic approximation and large language models
Home-page: https://github.com/inEXASCALE/llmabba
Author: Erin Carson, Xinye Chen, Cheng Kang
Author-email: xinyechenai@email.com
License: BSD 3-Clause
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python
Classifier: Topic :: Software Development
Classifier: Topic :: Scientific/Engineering
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: Unix
Classifier: Programming Language :: Python :: 3
Description-Content-Type: text/x-rst
License-File: LICENSE
Requires-Dist: tf_keras
Requires-Dist: wandb
Requires-Dist: numpy>=1.3.0
Requires-Dist: scipy>=0.7.0
Requires-Dist: requests
Requires-Dist: pandas
Requires-Dist: scikit-learn
Requires-Dist: cython>=0.27
Requires-Dist: joblib>=1.1.1
Requires-Dist: transformers
Requires-Dist: peft
Requires-Dist: trl
Requires-Dist: datasets
Requires-Dist: accelerate
Requires-Dist: matplotlib
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: summary

LLM-ABBA
========

.. image:: https://img.shields.io/pypi/v/llmabba?color=lightsalmon
   :alt: PyPI Version
   :target: https://pypi.org/project/llmabba/

.. image:: https://img.shields.io/pypi/dm/llmabba.svg?label=PyPI%20downloads
   :alt: PyPI Downloads
   :target: https://pypi.org/project/llmabba/

.. image:: https://img.shields.io/badge/Cython_Support-Accelerated-blue?style=flat&logoColor=cyan&labelColor=cyan&color=black
   :alt: Cython Support
   :target: https://github.com/inEXASCALE/llm-abba

.. image:: https://readthedocs.org/projects/llm-abba/badge/?version=latest
   :alt: Documentation Status
   :target: https://llm-abba.readthedocs.io/en/latest/

.. image:: https://img.shields.io/github/license/inEXASCALE/llm-abba
   :alt: License
   :target: https://github.com/inEXASCALE/llm-abba/blob/main/LICENSE

`llmabba` is a software framework for time series analysis using Large Language Models (LLMs) based on symbolic representation, as introduced in the paper `LLM-ABBA: Symbolic Time Series Approximation using Large Language Models <https://arxiv.org/abs/2411.18506>`_.

Time series analysis involves identifying patterns, trends, and structures within data sequences. Traditional methods like discrete wavelet transforms or symbolic aggregate approximation (SAX) convert continuous time series into symbolic representations for better analysis and compression. However, these methods often struggle with complex patterns.

`llmabba` enhances these techniques by leveraging LLMs, which excel in pattern recognition and sequence prediction. By applying LLMs to symbolic time series, `llmabba` discovers rich, meaningful representations, offering:

- **Higher accuracy and compression**: Better symbolic representations via LLMs, improving data compression and pattern accuracy.
- **Adaptability**: Robust performance across domains like finance, healthcare, and environmental science.
- **Scalability**: Efficient handling of large-scale time series datasets.
- **Automatic feature discovery**: Uncovers novel patterns that traditional methods may miss.

Key Features
------------
- Symbolic Time Series Approximation: Converts time series into symbolic representations.
- LLM-Powered Encoding: Enhances compression and pattern discovery.
- Efficient and Scalable: Suitable for large-scale datasets.
- Flexible Integration: Compatible with machine learning and statistical workflows.
